{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For this I am using Shakespeare's text\n",
    "# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this we will use this data to find the vocab size:\n",
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay now get the vocabulary\n",
    "vocab = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the vocabulary has 65 different types of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# we would like vocab to be a sorted list\n",
    "vocab = sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to map a number to each token(letter) so as to input it to the model\n",
    "# and also have the reverse mapping so that we can convert the number back to letter on receiving output\n",
    "stoi = { ch : i for i, ch in enumerate(vocab)}\n",
    "itos = { i : ch for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ':',\n",
       " 11: ';',\n",
       " 12: '?',\n",
       " 13: 'A',\n",
       " 14: 'B',\n",
       " 15: 'C',\n",
       " 16: 'D',\n",
       " 17: 'E',\n",
       " 18: 'F',\n",
       " 19: 'G',\n",
       " 20: 'H',\n",
       " 21: 'I',\n",
       " 22: 'J',\n",
       " 23: 'K',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'W',\n",
       " 36: 'X',\n",
       " 37: 'Y',\n",
       " 38: 'Z',\n",
       " 39: 'a',\n",
       " 40: 'b',\n",
       " 41: 'c',\n",
       " 42: 'd',\n",
       " 43: 'e',\n",
       " 44: 'f',\n",
       " 45: 'g',\n",
       " 46: 'h',\n",
       " 47: 'i',\n",
       " 48: 'j',\n",
       " 49: 'k',\n",
       " 50: 'l',\n",
       " 51: 'm',\n",
       " 52: 'n',\n",
       " 53: 'o',\n",
       " 54: 'p',\n",
       " 55: 'q',\n",
       " 56: 'r',\n",
       " 57: 's',\n",
       " 58: 't',\n",
       " 59: 'u',\n",
       " 60: 'v',\n",
       " 61: 'w',\n",
       " 62: 'x',\n",
       " 63: 'y',\n",
       " 64: 'z'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i.item()] for i in l])  # Convert tensor to Python int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to use torch now because it will help in backpropagation\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "data = torch.tensor(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using our block size of 8, meaning the maximum context length that the LLM can receive upon is 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider using a batch size of 4, i.e the model can complete 4 requests per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 58, 61, 53,  1, 42, 39, 63],\n",
      "        [ 0, 25, 13, 30, 15, 21, 33, 31],\n",
      "        [ 1, 57, 51, 39, 50, 50,  1, 47],\n",
      "        [56,  1, 47, 58,  1, 46, 47, 51]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 61, 53,  1, 42, 39, 63, 57],\n",
      "        [25, 13, 30, 15, 21, 33, 31, 10],\n",
      "        [57, 51, 39, 50, 50,  1, 47, 52],\n",
      "        [ 1, 47, 58,  1, 46, 47, 51, 11]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [1] the target: 58\n",
      "when input is [1, 58] the target: 61\n",
      "when input is [1, 58, 61] the target: 53\n",
      "when input is [1, 58, 61, 53] the target: 1\n",
      "when input is [1, 58, 61, 53, 1] the target: 42\n",
      "when input is [1, 58, 61, 53, 1, 42] the target: 39\n",
      "when input is [1, 58, 61, 53, 1, 42, 39] the target: 63\n",
      "when input is [1, 58, 61, 53, 1, 42, 39, 63] the target: 57\n",
      "when input is [0] the target: 25\n",
      "when input is [0, 25] the target: 13\n",
      "when input is [0, 25, 13] the target: 30\n",
      "when input is [0, 25, 13, 30] the target: 15\n",
      "when input is [0, 25, 13, 30, 15] the target: 21\n",
      "when input is [0, 25, 13, 30, 15, 21] the target: 33\n",
      "when input is [0, 25, 13, 30, 15, 21, 33] the target: 31\n",
      "when input is [0, 25, 13, 30, 15, 21, 33, 31] the target: 10\n",
      "when input is [1] the target: 57\n",
      "when input is [1, 57] the target: 51\n",
      "when input is [1, 57, 51] the target: 39\n",
      "when input is [1, 57, 51, 39] the target: 50\n",
      "when input is [1, 57, 51, 39, 50] the target: 50\n",
      "when input is [1, 57, 51, 39, 50, 50] the target: 1\n",
      "when input is [1, 57, 51, 39, 50, 50, 1] the target: 47\n",
      "when input is [1, 57, 51, 39, 50, 50, 1, 47] the target: 52\n",
      "when input is [56] the target: 1\n",
      "when input is [56, 1] the target: 47\n",
      "when input is [56, 1, 47] the target: 58\n",
      "when input is [56, 1, 47, 58] the target: 1\n",
      "when input is [56, 1, 47, 58, 1] the target: 46\n",
      "when input is [56, 1, 47, 58, 1, 46] the target: 47\n",
      "when input is [56, 1, 47, 58, 1, 46, 47] the target: 51\n",
      "when input is [56, 1, 47, 58, 1, 46, 47, 51] the target: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use channels here meaning as the dimensions of the token-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_table = nn.Embedding(len(vocab), n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time(or block), channels\n",
    "x_out =token_embedding_table(xb)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 32\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x_out)   # (B, T, 32)\n",
    "q = query(x_out) # (B, T, 32)\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5# (B, T, 32) @ (B, 32, T) ---> (B, T, T) \n",
    "# Attention weights calculated\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower traingular matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # lower triangular matrix untouched as upper one stores neg.inf\n",
    "wei = F.softmax(wei, dim=-1) #perform softmax along the rows\n",
    "\n",
    "v = value(x_out)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "dropout = 0.3\n",
    "\n",
    "ffn_c1 = nn.Linear(n_embd, 4 * n_embd)  # projection up\n",
    "ffn_c2 = nn.Linear(4 * n_embd, n_embd)   # projection down\n",
    "ln1 = nn.LayerNorm(n_embd)  # layernorm for attention\n",
    "ln2 = nn.LayerNorm(n_embd)  # layernorm for ffn\n",
    "drop = nn.Dropout(dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out =token_embedding_table(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n",
      "torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "print(x_out.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = x_out + out\n",
    "x_out = ln1(x_out)\n",
    "\n",
    "# feed forward network\n",
    "x_out = ffn_c2(F.relu(ffn_c1(x_out)))\n",
    "x_out = drop(x_out)  # apply dropout\n",
    "x_out = ln2(x_out)   # final layer norm\n",
    "\n",
    "\n",
    "# for sequence prediction, add final projection to vocabulary size\n",
    "vocab_size = 65 \n",
    "lm_head = nn.Linear(n_embd, vocab_size)\n",
    "logits = lm_head(x_out)  # (B, T, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sequence logits shape: torch.Size([4, 8, 65])\n",
      "Next token logits shape: torch.Size([4, 65])\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "# if you want just the next token prediction:\n",
    "next_token_logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "print(f\"Full sequence logits shape: {logits.shape}\")  # [4, 8, 100]\n",
    "print(f\"Next token logits shape: {next_token_logits.shape}\")  # [4, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1443, -0.2766, -0.4747, -0.6146, -0.6413, -0.6616,  0.1625,  0.1650,\n",
      "        -0.7168,  0.0272, -0.1567,  0.3417, -0.2788, -0.8119, -0.4043, -0.1428,\n",
      "        -0.9130,  1.0195,  0.4471, -0.3599, -0.6686,  0.7302,  0.1412,  0.6330,\n",
      "        -0.2456, -0.8446, -0.4725, -0.9241, -0.9292, -0.0419, -0.3271,  0.8514,\n",
      "        -0.6371, -1.0336,  0.6583, -0.5417,  0.0699, -0.7249,  0.6746,  0.8653,\n",
      "        -0.0172, -0.2517,  0.4788, -0.5700, -0.3379, -0.0820, -0.5902,  0.5371,\n",
      "        -0.2949,  1.2138, -0.3999, -0.4621, -0.9018,  0.4939,  0.6795, -0.5234,\n",
      "         0.6740,  0.1504,  0.3867,  0.0429,  0.6505, -0.2266, -0.1173,  0.1074,\n",
      "         0.3538], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(next_token_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(next_token_logits, dim=-1)  # Normalize across the token dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0125, 0.0109, 0.0090, 0.0078, 0.0076, 0.0074, 0.0170, 0.0170, 0.0070,\n",
       "         0.0148, 0.0123, 0.0203, 0.0109, 0.0064, 0.0096, 0.0125, 0.0058, 0.0400,\n",
       "         0.0226, 0.0101, 0.0074, 0.0299, 0.0166, 0.0272, 0.0113, 0.0062, 0.0090,\n",
       "         0.0057, 0.0057, 0.0138, 0.0104, 0.0338, 0.0076, 0.0051, 0.0279, 0.0084,\n",
       "         0.0155, 0.0070, 0.0283, 0.0343, 0.0142, 0.0112, 0.0233, 0.0082, 0.0103,\n",
       "         0.0133, 0.0080, 0.0247, 0.0107, 0.0485, 0.0097, 0.0091, 0.0059, 0.0236,\n",
       "         0.0285, 0.0085, 0.0283, 0.0168, 0.0212, 0.0151, 0.0276, 0.0115, 0.0128,\n",
       "         0.0161, 0.0205],\n",
       "        [0.0146, 0.0162, 0.0083, 0.0355, 0.0263, 0.0106, 0.0047, 0.0078, 0.0190,\n",
       "         0.0243, 0.0210, 0.0074, 0.0146, 0.0127, 0.0071, 0.0064, 0.0153, 0.0242,\n",
       "         0.0374, 0.0092, 0.0116, 0.0363, 0.0094, 0.0063, 0.0122, 0.0072, 0.0059,\n",
       "         0.0206, 0.0076, 0.0077, 0.0119, 0.0463, 0.0187, 0.0115, 0.0116, 0.0246,\n",
       "         0.0055, 0.0119, 0.0259, 0.0134, 0.0081, 0.0062, 0.0197, 0.0324, 0.0212,\n",
       "         0.0081, 0.0066, 0.0039, 0.0117, 0.0233, 0.0097, 0.0256, 0.0071, 0.0107,\n",
       "         0.0074, 0.0160, 0.0266, 0.0276, 0.0137, 0.0299, 0.0125, 0.0091, 0.0152,\n",
       "         0.0053, 0.0138],\n",
       "        [0.0129, 0.0127, 0.0059, 0.0350, 0.0155, 0.0072, 0.0057, 0.0251, 0.0129,\n",
       "         0.0083, 0.0160, 0.0268, 0.0103, 0.0073, 0.0121, 0.0118, 0.0066, 0.0169,\n",
       "         0.0335, 0.0161, 0.0095, 0.0199, 0.0126, 0.0261, 0.0276, 0.0040, 0.0134,\n",
       "         0.0180, 0.0094, 0.0089, 0.0248, 0.0376, 0.0167, 0.0089, 0.0231, 0.0189,\n",
       "         0.0108, 0.0164, 0.0140, 0.0257, 0.0179, 0.0132, 0.0140, 0.0135, 0.0285,\n",
       "         0.0165, 0.0162, 0.0062, 0.0036, 0.0196, 0.0078, 0.0095, 0.0114, 0.0179,\n",
       "         0.0169, 0.0110, 0.0233, 0.0112, 0.0070, 0.0065, 0.0140, 0.0071, 0.0090,\n",
       "         0.0345, 0.0187],\n",
       "        [0.0069, 0.0047, 0.0076, 0.0139, 0.0063, 0.0106, 0.0095, 0.0086, 0.0185,\n",
       "         0.0114, 0.0130, 0.0183, 0.0093, 0.0079, 0.0110, 0.0060, 0.0180, 0.0134,\n",
       "         0.0252, 0.0110, 0.0142, 0.0142, 0.0137, 0.0177, 0.0225, 0.0064, 0.0173,\n",
       "         0.0120, 0.0114, 0.0032, 0.0212, 0.0359, 0.0185, 0.0163, 0.0167, 0.0122,\n",
       "         0.0046, 0.0133, 0.0107, 0.0732, 0.0123, 0.0166, 0.0279, 0.0206, 0.0104,\n",
       "         0.0083, 0.0036, 0.0299, 0.0041, 0.0086, 0.0186, 0.0206, 0.0138, 0.0218,\n",
       "         0.0138, 0.0072, 0.0102, 0.0166, 0.0157, 0.0206, 0.0653, 0.0029, 0.0124,\n",
       "         0.0111, 0.0207]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token = torch.argmax(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49, 31, 31, 39])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kSSa'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 58, 61, 53,  1, 42, 39, 63],\n",
       "        [ 0, 25, 13, 30, 15, 21, 33, 31],\n",
       "        [ 1, 57, 51, 39, 50, 50,  1, 47],\n",
       "        [56,  1, 47, 58,  1, 46, 47, 51]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMARCIUS'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(xb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MARCIUS:'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(yb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(result, expected_value):\n",
    "    loss = F.cross_entropy(result, expected_value)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(result, expected_value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimate_loss\u001b[39m(result, expected_value):\n\u001b[0;32m----> 3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "estimate_loss(xb[1][-1], yb[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

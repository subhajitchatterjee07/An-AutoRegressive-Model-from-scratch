{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "torch.Size([1115394]) torch.int64\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 46, 39, 52,  1, 44, 43, 43],\n",
      "        [ 1, 44, 53, 53, 58,  0, 20, 43],\n",
      "        [19, 47, 60, 43,  1, 51, 43,  1],\n",
      "        [53, 59,  1, 51, 59, 57, 58,  6]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[46, 39, 52,  1, 44, 43, 43, 58],\n",
      "        [44, 53, 53, 58,  0, 20, 43,  1],\n",
      "        [47, 60, 43,  1, 51, 43,  1, 58],\n",
      "        [59,  1, 51, 59, 57, 58,  6,  1]])\n",
      "----\n",
      "when input is [58] the target: 46\n",
      "when input is [58, 46] the target: 39\n",
      "when input is [58, 46, 39] the target: 52\n",
      "when input is [58, 46, 39, 52] the target: 1\n",
      "when input is [58, 46, 39, 52, 1] the target: 44\n",
      "when input is [58, 46, 39, 52, 1, 44] the target: 43\n",
      "when input is [58, 46, 39, 52, 1, 44, 43] the target: 43\n",
      "when input is [58, 46, 39, 52, 1, 44, 43, 43] the target: 58\n",
      "when input is [1] the target: 44\n",
      "when input is [1, 44] the target: 53\n",
      "when input is [1, 44, 53] the target: 53\n",
      "when input is [1, 44, 53, 53] the target: 58\n",
      "when input is [1, 44, 53, 53, 58] the target: 0\n",
      "when input is [1, 44, 53, 53, 58, 0] the target: 20\n",
      "when input is [1, 44, 53, 53, 58, 0, 20] the target: 43\n",
      "when input is [1, 44, 53, 53, 58, 0, 20, 43] the target: 1\n",
      "when input is [19] the target: 47\n",
      "when input is [19, 47] the target: 60\n",
      "when input is [19, 47, 60] the target: 43\n",
      "when input is [19, 47, 60, 43] the target: 1\n",
      "when input is [19, 47, 60, 43, 1] the target: 51\n",
      "when input is [19, 47, 60, 43, 1, 51] the target: 43\n",
      "when input is [19, 47, 60, 43, 1, 51, 43] the target: 1\n",
      "when input is [19, 47, 60, 43, 1, 51, 43, 1] the target: 58\n",
      "when input is [53] the target: 59\n",
      "when input is [53, 59] the target: 1\n",
      "when input is [53, 59, 1] the target: 51\n",
      "when input is [53, 59, 1, 51] the target: 59\n",
      "when input is [53, 59, 1, 51, 59] the target: 57\n",
      "when input is [53, 59, 1, 51, 59, 57] the target: 58\n",
      "when input is [53, 59, 1, 51, 59, 57, 58] the target: 6\n",
      "when input is [53, 59, 1, 51, 59, 57, 58, 6] the target: 1\n",
      "torch.Size([4, 8, 32])\n",
      "Full sequence logits shape: torch.Size([4, 8, 65])\n",
      "Next token logits shape: torch.Size([4, 65])\n"
     ]
    }
   ],
   "source": [
    "#dataset \n",
    "# After this we will use this data to find the vocab size:\n",
    "# read it in to inspect it\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# okay now get the vocabulary\n",
    "vocab = set(text)\n",
    "\n",
    "len(vocab)\n",
    "\n",
    "# we would like vocab to be a sorted list\n",
    "vocab = sorted(list(vocab))\n",
    "print(''.join(vocab))\n",
    "\n",
    "# Now we need to map a number to each token(letter) so as to input it to the model\n",
    "# and also have the reverse mapping so that we can convert the number back to letter on receiving output\n",
    "stoi = { ch : i for i, ch in enumerate(vocab)}\n",
    "itos = { i : ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i.item()] for i in l])  # Convert tensor to Python int\n",
    "\n",
    "enc_text = encode(text)\n",
    "\n",
    "data = torch.tensor(enc_text)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "\n",
    "\n",
    "n_embd = 32\n",
    "dropout = 0.3\n",
    "\n",
    "token_embedding_table = nn.Embedding(len(vocab), n_embd)\n",
    "\n",
    "\n",
    "# self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time(or block), channels\n",
    "\n",
    "x_out =token_embedding_table(xb)\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 32\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x_out)   # (B, T, 32)\n",
    "q = query(x_out) # (B, T, 32)\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5# (B, T, 32) @ (B, 32, T) ---> (B, T, T) \n",
    "# Attention weights calculated\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower traingular matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # lower triangular matrix untouched as upper one stores neg.inf\n",
    "wei = F.softmax(wei, dim=-1) #perform softmax along the rows\n",
    "\n",
    "v = value(x_out)\n",
    "out = wei @ v\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "ffn_c1 = nn.Linear(n_embd, 4 * n_embd)  # projection up\n",
    "ffn_c2 = nn.Linear(4 * n_embd, n_embd)   # projection down\n",
    "ln1 = nn.LayerNorm(n_embd)  # layernorm for attention\n",
    "ln2 = nn.LayerNorm(n_embd)  # layernorm for ffn\n",
    "drop = nn.Dropout(dropout)\n",
    "\n",
    "x_out = x_out + out\n",
    "x_out = ln1(x_out)\n",
    "\n",
    "# feed forward network\n",
    "x_out = ffn_c2(F.relu(ffn_c1(x_out)))\n",
    "x_out = drop(x_out)  # apply dropout\n",
    "x_out = ln2(x_out)   # final layer norm\n",
    "\n",
    "\n",
    "# for sequence prediction, add final projection to vocabulary size\n",
    "vocab_size = 65 \n",
    "lm_head = nn.Linear(n_embd, vocab_size)\n",
    "logits = lm_head(x_out)  # (B, T, vocab_size)\n",
    "\n",
    "# get predictions\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "# if you want just the next token prediction:\n",
    "next_token_logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "print(f\"Full sequence logits shape: {logits.shape}\")  # [4, 8, 100]\n",
    "print(f\"Next token logits shape: {next_token_logits.shape}\")  # [4, 100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss: 4.3902, Validation Loss: 4.2025\n",
      "Iteration 100, Train Loss: 4.3152, Validation Loss: 4.1620\n",
      "Iteration 200, Train Loss: 4.1820, Validation Loss: 3.9703\n",
      "Iteration 300, Train Loss: 3.9463, Validation Loss: 4.2240\n",
      "Iteration 400, Train Loss: 4.0025, Validation Loss: 4.1944\n",
      "Iteration 500, Train Loss: 4.0149, Validation Loss: 4.1657\n",
      "Iteration 600, Train Loss: 3.8793, Validation Loss: 3.7647\n",
      "Iteration 700, Train Loss: 3.7156, Validation Loss: 3.9885\n",
      "Iteration 800, Train Loss: 3.8511, Validation Loss: 3.8184\n",
      "Iteration 900, Train Loss: 3.7978, Validation Loss: 3.6808\n",
      "Iteration 1000, Train Loss: 3.7492, Validation Loss: 3.8188\n",
      "Iteration 1100, Train Loss: 3.4992, Validation Loss: 3.7903\n",
      "Iteration 1200, Train Loss: 3.6853, Validation Loss: 3.7850\n",
      "Iteration 1300, Train Loss: 3.7897, Validation Loss: 3.4083\n",
      "Iteration 1400, Train Loss: 3.4041, Validation Loss: 3.7673\n",
      "Iteration 1500, Train Loss: 3.2916, Validation Loss: 3.4662\n",
      "Iteration 1600, Train Loss: 3.6403, Validation Loss: 3.8176\n",
      "Iteration 1700, Train Loss: 3.2954, Validation Loss: 3.3921\n",
      "Iteration 1800, Train Loss: 3.6542, Validation Loss: 3.4722\n",
      "Iteration 1900, Train Loss: 3.4166, Validation Loss: 3.3902\n",
      "Iteration 2000, Train Loss: 3.0459, Validation Loss: 3.2516\n",
      "Iteration 2100, Train Loss: 3.2501, Validation Loss: 2.9013\n",
      "Iteration 2200, Train Loss: 3.3153, Validation Loss: 3.0866\n",
      "Iteration 2300, Train Loss: 3.0314, Validation Loss: 3.3534\n",
      "Iteration 2400, Train Loss: 2.9767, Validation Loss: 3.3845\n",
      "Iteration 2500, Train Loss: 3.5309, Validation Loss: 3.1481\n",
      "Iteration 2600, Train Loss: 3.2975, Validation Loss: 3.0023\n",
      "Iteration 2700, Train Loss: 3.1690, Validation Loss: 3.1164\n",
      "Iteration 2800, Train Loss: 3.0325, Validation Loss: 3.0490\n",
      "Iteration 2900, Train Loss: 3.6498, Validation Loss: 3.6050\n",
      "Iteration 3000, Train Loss: 3.2276, Validation Loss: 3.1468\n",
      "Iteration 3100, Train Loss: 3.1224, Validation Loss: 2.9649\n",
      "Iteration 3200, Train Loss: 2.7306, Validation Loss: 3.3251\n",
      "Iteration 3300, Train Loss: 2.9796, Validation Loss: 2.9623\n",
      "Iteration 3400, Train Loss: 3.3005, Validation Loss: 3.8828\n",
      "Iteration 3500, Train Loss: 3.0783, Validation Loss: 3.0824\n",
      "Iteration 3600, Train Loss: 3.3485, Validation Loss: 2.7328\n",
      "Iteration 3700, Train Loss: 3.3058, Validation Loss: 3.2328\n",
      "Iteration 3800, Train Loss: 3.0467, Validation Loss: 2.9741\n",
      "Iteration 3900, Train Loss: 3.1808, Validation Loss: 3.1236\n",
      "Iteration 4000, Train Loss: 3.1525, Validation Loss: 3.4137\n",
      "Iteration 4100, Train Loss: 3.1674, Validation Loss: 3.3024\n",
      "Iteration 4200, Train Loss: 3.2253, Validation Loss: 3.7962\n",
      "Iteration 4300, Train Loss: 3.0876, Validation Loss: 3.0939\n",
      "Iteration 4400, Train Loss: 3.4337, Validation Loss: 2.9682\n",
      "Iteration 4500, Train Loss: 3.5193, Validation Loss: 3.2860\n",
      "Iteration 4600, Train Loss: 2.7705, Validation Loss: 2.9318\n",
      "Iteration 4700, Train Loss: 3.0624, Validation Loss: 3.0202\n",
      "Iteration 4800, Train Loss: 2.8074, Validation Loss: 2.9557\n",
      "Iteration 4900, Train Loss: 2.7760, Validation Loss: 3.0670\n",
      "Iteration 5000, Train Loss: 2.7945, Validation Loss: 2.9934\n",
      "Iteration 5100, Train Loss: 3.0561, Validation Loss: 3.0293\n",
      "Iteration 5200, Train Loss: 3.1008, Validation Loss: 2.7488\n",
      "Iteration 5300, Train Loss: 2.8060, Validation Loss: 3.4829\n",
      "Iteration 5400, Train Loss: 2.7468, Validation Loss: 3.1984\n",
      "Iteration 5500, Train Loss: 2.9117, Validation Loss: 3.1905\n",
      "Iteration 5600, Train Loss: 3.2819, Validation Loss: 2.9961\n",
      "Iteration 5700, Train Loss: 2.6499, Validation Loss: 3.5776\n",
      "Iteration 5800, Train Loss: 2.9945, Validation Loss: 3.2847\n",
      "Iteration 5900, Train Loss: 3.6667, Validation Loss: 3.0323\n",
      "Iteration 6000, Train Loss: 2.5466, Validation Loss: 2.8315\n",
      "Iteration 6100, Train Loss: 2.6827, Validation Loss: 3.0090\n",
      "Iteration 6200, Train Loss: 2.8474, Validation Loss: 3.2463\n",
      "Iteration 6300, Train Loss: 2.7269, Validation Loss: 2.9681\n",
      "Iteration 6400, Train Loss: 2.8608, Validation Loss: 2.7712\n",
      "Iteration 6500, Train Loss: 2.6302, Validation Loss: 2.9404\n",
      "Iteration 6600, Train Loss: 2.6906, Validation Loss: 2.9683\n",
      "Iteration 6700, Train Loss: 3.0192, Validation Loss: 2.4881\n",
      "Iteration 6800, Train Loss: 2.9050, Validation Loss: 2.9964\n",
      "Iteration 6900, Train Loss: 3.0105, Validation Loss: 3.1623\n",
      "Iteration 7000, Train Loss: 2.5832, Validation Loss: 2.5436\n",
      "Iteration 7100, Train Loss: 3.0951, Validation Loss: 3.0120\n",
      "Iteration 7200, Train Loss: 3.0455, Validation Loss: 2.9900\n",
      "Iteration 7300, Train Loss: 3.1120, Validation Loss: 2.9065\n",
      "Iteration 7400, Train Loss: 2.3282, Validation Loss: 3.0660\n",
      "Iteration 7500, Train Loss: 2.8106, Validation Loss: 2.8159\n",
      "Iteration 7600, Train Loss: 2.9631, Validation Loss: 2.6567\n",
      "Iteration 7700, Train Loss: 3.0656, Validation Loss: 2.9417\n",
      "Iteration 7800, Train Loss: 2.8815, Validation Loss: 2.7135\n",
      "Iteration 7900, Train Loss: 2.9135, Validation Loss: 3.0893\n",
      "Iteration 8000, Train Loss: 2.7534, Validation Loss: 2.7374\n",
      "Iteration 8100, Train Loss: 2.5722, Validation Loss: 2.8966\n",
      "Iteration 8200, Train Loss: 3.1773, Validation Loss: 2.8349\n",
      "Iteration 8300, Train Loss: 3.1227, Validation Loss: 2.5321\n",
      "Iteration 8400, Train Loss: 3.2356, Validation Loss: 3.0626\n",
      "Iteration 8500, Train Loss: 3.2468, Validation Loss: 2.7287\n",
      "Iteration 8600, Train Loss: 2.8161, Validation Loss: 2.4462\n",
      "Iteration 8700, Train Loss: 2.7265, Validation Loss: 2.9367\n",
      "Iteration 8800, Train Loss: 2.6449, Validation Loss: 2.8578\n",
      "Iteration 8900, Train Loss: 2.8132, Validation Loss: 3.1935\n",
      "Iteration 9000, Train Loss: 2.9985, Validation Loss: 2.7189\n",
      "Iteration 9100, Train Loss: 2.5768, Validation Loss: 2.6313\n",
      "Iteration 9200, Train Loss: 2.7827, Validation Loss: 2.5519\n",
      "Iteration 9300, Train Loss: 2.7711, Validation Loss: 2.7734\n",
      "Iteration 9400, Train Loss: 2.8942, Validation Loss: 2.7445\n",
      "Iteration 9500, Train Loss: 2.5811, Validation Loss: 2.9578\n",
      "Iteration 9600, Train Loss: 2.3676, Validation Loss: 2.8299\n",
      "Iteration 9700, Train Loss: 2.6494, Validation Loss: 3.1761\n",
      "Iteration 9800, Train Loss: 2.9525, Validation Loss: 2.4794\n",
      "Iteration 9900, Train Loss: 2.7305, Validation Loss: 3.1426\n",
      "Iteration 10000, Train Loss: 2.5425, Validation Loss: 3.0513\n",
      "Iteration 10100, Train Loss: 2.3971, Validation Loss: 2.6575\n",
      "Iteration 10200, Train Loss: 2.6845, Validation Loss: 2.5730\n",
      "Iteration 10300, Train Loss: 2.8881, Validation Loss: 2.9921\n",
      "Iteration 10400, Train Loss: 2.8789, Validation Loss: 2.6205\n",
      "Iteration 10500, Train Loss: 2.7054, Validation Loss: 2.7763\n",
      "Iteration 10600, Train Loss: 2.4859, Validation Loss: 2.6364\n",
      "Iteration 10700, Train Loss: 2.7262, Validation Loss: 2.9951\n",
      "Iteration 10800, Train Loss: 2.8228, Validation Loss: 2.7658\n",
      "Iteration 10900, Train Loss: 2.8340, Validation Loss: 2.6934\n",
      "Iteration 11000, Train Loss: 2.6550, Validation Loss: 2.4704\n",
      "Iteration 11100, Train Loss: 2.5093, Validation Loss: 2.5947\n",
      "Iteration 11200, Train Loss: 2.4872, Validation Loss: 2.7320\n",
      "Iteration 11300, Train Loss: 2.8549, Validation Loss: 2.6018\n",
      "Iteration 11400, Train Loss: 3.0054, Validation Loss: 2.9504\n",
      "Iteration 11500, Train Loss: 2.8280, Validation Loss: 2.3134\n",
      "Iteration 11600, Train Loss: 2.8146, Validation Loss: 3.1344\n",
      "Iteration 11700, Train Loss: 2.5438, Validation Loss: 2.4814\n",
      "Iteration 11800, Train Loss: 2.6613, Validation Loss: 2.4559\n",
      "Iteration 11900, Train Loss: 2.5399, Validation Loss: 2.8743\n",
      "Iteration 12000, Train Loss: 2.8726, Validation Loss: 2.5443\n",
      "Iteration 12100, Train Loss: 3.1743, Validation Loss: 2.7616\n",
      "Iteration 12200, Train Loss: 2.9108, Validation Loss: 2.6548\n",
      "Iteration 12300, Train Loss: 2.6747, Validation Loss: 2.6630\n",
      "Iteration 12400, Train Loss: 2.3224, Validation Loss: 2.3882\n",
      "Iteration 12500, Train Loss: 2.4720, Validation Loss: 2.4810\n",
      "Iteration 12600, Train Loss: 2.6527, Validation Loss: 2.6064\n",
      "Iteration 12700, Train Loss: 3.0373, Validation Loss: 2.7367\n",
      "Iteration 12800, Train Loss: 3.0816, Validation Loss: 2.5794\n",
      "Iteration 12900, Train Loss: 2.5345, Validation Loss: 2.8323\n",
      "Iteration 13000, Train Loss: 2.5189, Validation Loss: 2.9557\n",
      "Iteration 13100, Train Loss: 3.1500, Validation Loss: 2.5325\n",
      "Iteration 13200, Train Loss: 2.4966, Validation Loss: 2.7641\n",
      "Iteration 13300, Train Loss: 2.5760, Validation Loss: 2.4398\n",
      "Iteration 13400, Train Loss: 2.8192, Validation Loss: 3.0215\n",
      "Iteration 13500, Train Loss: 2.9976, Validation Loss: 3.1247\n",
      "Iteration 13600, Train Loss: 2.7057, Validation Loss: 3.0543\n",
      "Iteration 13700, Train Loss: 2.8606, Validation Loss: 2.7179\n",
      "Iteration 13800, Train Loss: 2.6588, Validation Loss: 2.2356\n",
      "Iteration 13900, Train Loss: 2.9073, Validation Loss: 3.0401\n",
      "Iteration 14000, Train Loss: 2.6629, Validation Loss: 2.4712\n",
      "Iteration 14100, Train Loss: 3.3188, Validation Loss: 2.5571\n",
      "Iteration 14200, Train Loss: 2.6870, Validation Loss: 2.4647\n",
      "Iteration 14300, Train Loss: 2.6802, Validation Loss: 2.7648\n",
      "Iteration 14400, Train Loss: 2.2738, Validation Loss: 3.1500\n",
      "Iteration 14500, Train Loss: 2.2665, Validation Loss: 3.4268\n",
      "Iteration 14600, Train Loss: 2.5129, Validation Loss: 2.6517\n",
      "Iteration 14700, Train Loss: 2.7390, Validation Loss: 2.4152\n",
      "Iteration 14800, Train Loss: 3.1214, Validation Loss: 2.8997\n",
      "Iteration 14900, Train Loss: 2.5595, Validation Loss: 2.5846\n",
      "Iteration 15000, Train Loss: 3.1288, Validation Loss: 2.5139\n",
      "Iteration 15100, Train Loss: 2.3513, Validation Loss: 2.4476\n",
      "Iteration 15200, Train Loss: 3.0091, Validation Loss: 2.6643\n",
      "Iteration 15300, Train Loss: 2.3508, Validation Loss: 2.4846\n",
      "Iteration 15400, Train Loss: 2.8667, Validation Loss: 2.6817\n",
      "Iteration 15500, Train Loss: 2.5419, Validation Loss: 3.0187\n",
      "Iteration 15600, Train Loss: 3.0480, Validation Loss: 2.4327\n",
      "Iteration 15700, Train Loss: 2.5076, Validation Loss: 2.4110\n",
      "Iteration 15800, Train Loss: 3.2553, Validation Loss: 2.6392\n",
      "Iteration 15900, Train Loss: 2.4333, Validation Loss: 2.7908\n",
      "Iteration 16000, Train Loss: 2.5922, Validation Loss: 2.4892\n",
      "Iteration 16100, Train Loss: 2.7105, Validation Loss: 2.7548\n",
      "Iteration 16200, Train Loss: 3.1448, Validation Loss: 3.2152\n",
      "Iteration 16300, Train Loss: 2.9074, Validation Loss: 2.1685\n",
      "Iteration 16400, Train Loss: 2.6205, Validation Loss: 2.1530\n",
      "Iteration 16500, Train Loss: 2.8094, Validation Loss: 2.9676\n",
      "Iteration 16600, Train Loss: 2.5493, Validation Loss: 2.9617\n",
      "Iteration 16700, Train Loss: 2.5845, Validation Loss: 2.5582\n",
      "Iteration 16800, Train Loss: 2.4056, Validation Loss: 2.6308\n",
      "Iteration 16900, Train Loss: 3.2063, Validation Loss: 2.9256\n",
      "Iteration 17000, Train Loss: 2.6457, Validation Loss: 2.3676\n",
      "Iteration 17100, Train Loss: 2.2462, Validation Loss: 2.4529\n",
      "Iteration 17200, Train Loss: 2.7864, Validation Loss: 2.6273\n",
      "Iteration 17300, Train Loss: 2.4817, Validation Loss: 3.0205\n",
      "Iteration 17400, Train Loss: 2.1152, Validation Loss: 2.5399\n",
      "Iteration 17500, Train Loss: 2.5172, Validation Loss: 2.7077\n",
      "Iteration 17600, Train Loss: 2.9640, Validation Loss: 2.7898\n",
      "Iteration 17700, Train Loss: 2.7004, Validation Loss: 2.7628\n",
      "Iteration 17800, Train Loss: 2.2932, Validation Loss: 2.3545\n",
      "Iteration 17900, Train Loss: 2.8022, Validation Loss: 2.4660\n",
      "Iteration 18000, Train Loss: 2.3008, Validation Loss: 2.7380\n",
      "Iteration 18100, Train Loss: 3.7318, Validation Loss: 2.4111\n",
      "Iteration 18200, Train Loss: 2.5080, Validation Loss: 2.4827\n",
      "Iteration 18300, Train Loss: 2.8385, Validation Loss: 2.8960\n",
      "Iteration 18400, Train Loss: 3.2123, Validation Loss: 2.7611\n",
      "Iteration 18500, Train Loss: 2.6562, Validation Loss: 3.0492\n",
      "Iteration 18600, Train Loss: 2.9331, Validation Loss: 2.8994\n",
      "Iteration 18700, Train Loss: 2.9313, Validation Loss: 2.5864\n",
      "Iteration 18800, Train Loss: 2.5550, Validation Loss: 2.7453\n",
      "Iteration 18900, Train Loss: 2.7901, Validation Loss: 3.0104\n",
      "Iteration 19000, Train Loss: 2.4289, Validation Loss: 2.4706\n",
      "Iteration 19100, Train Loss: 2.5686, Validation Loss: 3.0919\n",
      "Iteration 19200, Train Loss: 2.2695, Validation Loss: 2.7357\n",
      "Iteration 19300, Train Loss: 2.3832, Validation Loss: 3.1080\n",
      "Iteration 19400, Train Loss: 2.9646, Validation Loss: 2.5414\n",
      "Iteration 19500, Train Loss: 2.2752, Validation Loss: 2.7252\n",
      "Iteration 19600, Train Loss: 1.9897, Validation Loss: 2.5060\n",
      "Iteration 19700, Train Loss: 3.3462, Validation Loss: 2.8559\n",
      "Iteration 19800, Train Loss: 2.6594, Validation Loss: 2.4548\n",
      "Iteration 19900, Train Loss: 2.6603, Validation Loss: 2.9842\n",
      "Iteration 20000, Train Loss: 1.9952, Validation Loss: 3.1552\n",
      "Iteration 20100, Train Loss: 2.8011, Validation Loss: 2.1317\n",
      "Iteration 20200, Train Loss: 2.4627, Validation Loss: 2.5404\n",
      "Iteration 20300, Train Loss: 2.5452, Validation Loss: 2.4444\n",
      "Iteration 20400, Train Loss: 2.8030, Validation Loss: 2.6990\n",
      "Iteration 20500, Train Loss: 2.5572, Validation Loss: 2.6347\n",
      "Iteration 20600, Train Loss: 2.4148, Validation Loss: 2.4865\n",
      "Iteration 20700, Train Loss: 2.2321, Validation Loss: 2.6568\n",
      "Iteration 20800, Train Loss: 2.3206, Validation Loss: 2.6566\n",
      "Iteration 20900, Train Loss: 3.2061, Validation Loss: 2.6000\n",
      "Iteration 21000, Train Loss: 2.5417, Validation Loss: 2.1126\n",
      "Iteration 21100, Train Loss: 3.0727, Validation Loss: 2.7808\n",
      "Iteration 21200, Train Loss: 2.2794, Validation Loss: 2.4937\n",
      "Iteration 21300, Train Loss: 2.6603, Validation Loss: 2.3788\n",
      "Iteration 21400, Train Loss: 2.4830, Validation Loss: 2.2500\n",
      "Iteration 21500, Train Loss: 2.8403, Validation Loss: 2.7176\n",
      "Iteration 21600, Train Loss: 2.5044, Validation Loss: 2.5550\n",
      "Iteration 21700, Train Loss: 2.5153, Validation Loss: 2.9239\n",
      "Iteration 21800, Train Loss: 2.4632, Validation Loss: 2.5225\n",
      "Iteration 21900, Train Loss: 2.1591, Validation Loss: 2.5673\n",
      "Iteration 22000, Train Loss: 2.7540, Validation Loss: 2.5729\n",
      "Iteration 22100, Train Loss: 2.5503, Validation Loss: 2.9652\n",
      "Iteration 22200, Train Loss: 2.6073, Validation Loss: 2.7322\n",
      "Iteration 22300, Train Loss: 2.5691, Validation Loss: 2.6771\n",
      "Iteration 22400, Train Loss: 2.6769, Validation Loss: 2.5471\n",
      "Iteration 22500, Train Loss: 2.6691, Validation Loss: 2.5097\n",
      "Iteration 22600, Train Loss: 2.9732, Validation Loss: 2.9563\n",
      "Iteration 22700, Train Loss: 2.3277, Validation Loss: 2.5462\n",
      "Iteration 22800, Train Loss: 2.4467, Validation Loss: 3.0229\n",
      "Iteration 22900, Train Loss: 2.3957, Validation Loss: 2.7307\n",
      "Iteration 23000, Train Loss: 2.8784, Validation Loss: 2.3987\n",
      "Iteration 23100, Train Loss: 2.2510, Validation Loss: 2.3381\n",
      "Iteration 23200, Train Loss: 2.3871, Validation Loss: 2.6204\n",
      "Iteration 23300, Train Loss: 2.7132, Validation Loss: 2.4740\n",
      "Iteration 23400, Train Loss: 2.6728, Validation Loss: 2.6296\n",
      "Iteration 23500, Train Loss: 2.7385, Validation Loss: 2.6202\n",
      "Iteration 23600, Train Loss: 2.6284, Validation Loss: 2.7077\n",
      "Iteration 23700, Train Loss: 2.4676, Validation Loss: 3.1490\n",
      "Iteration 23800, Train Loss: 2.4907, Validation Loss: 2.8956\n",
      "Iteration 23900, Train Loss: 2.2243, Validation Loss: 3.1669\n",
      "Iteration 24000, Train Loss: 2.7083, Validation Loss: 2.3157\n",
      "Iteration 24100, Train Loss: 2.2202, Validation Loss: 2.0756\n",
      "Iteration 24200, Train Loss: 2.5777, Validation Loss: 2.7363\n",
      "Iteration 24300, Train Loss: 2.5802, Validation Loss: 2.4346\n",
      "Iteration 24400, Train Loss: 2.6365, Validation Loss: 2.4702\n",
      "Iteration 24500, Train Loss: 2.5479, Validation Loss: 2.7998\n",
      "Iteration 24600, Train Loss: 2.5478, Validation Loss: 2.7727\n",
      "Iteration 24700, Train Loss: 2.2412, Validation Loss: 2.2615\n",
      "Iteration 24800, Train Loss: 2.3433, Validation Loss: 2.4019\n",
      "Iteration 24900, Train Loss: 2.7683, Validation Loss: 3.0401\n",
      "Iteration 25000, Train Loss: 2.7611, Validation Loss: 2.6098\n",
      "Iteration 25100, Train Loss: 2.8896, Validation Loss: 2.7901\n",
      "Iteration 25200, Train Loss: 2.6836, Validation Loss: 2.1693\n",
      "Iteration 25300, Train Loss: 2.5914, Validation Loss: 2.4407\n",
      "Iteration 25400, Train Loss: 2.7766, Validation Loss: 2.5140\n",
      "Iteration 25500, Train Loss: 2.3165, Validation Loss: 2.5997\n",
      "Iteration 25600, Train Loss: 2.3877, Validation Loss: 2.9902\n",
      "Iteration 25700, Train Loss: 2.5098, Validation Loss: 2.4267\n",
      "Iteration 25800, Train Loss: 2.4212, Validation Loss: 2.3493\n",
      "Iteration 25900, Train Loss: 2.8585, Validation Loss: 2.6719\n",
      "Iteration 26000, Train Loss: 2.3646, Validation Loss: 2.2492\n",
      "Iteration 26100, Train Loss: 2.4241, Validation Loss: 2.5692\n",
      "Iteration 26200, Train Loss: 2.5545, Validation Loss: 2.6737\n",
      "Iteration 26300, Train Loss: 2.3191, Validation Loss: 2.2360\n",
      "Iteration 26400, Train Loss: 2.3149, Validation Loss: 2.5053\n",
      "Iteration 26500, Train Loss: 2.8144, Validation Loss: 3.0409\n",
      "Iteration 26600, Train Loss: 2.3641, Validation Loss: 2.2927\n",
      "Iteration 26700, Train Loss: 2.6635, Validation Loss: 2.5602\n",
      "Iteration 26800, Train Loss: 2.4099, Validation Loss: 2.3369\n",
      "Iteration 26900, Train Loss: 2.7702, Validation Loss: 2.6823\n",
      "Iteration 27000, Train Loss: 2.5083, Validation Loss: 2.2610\n",
      "Iteration 27100, Train Loss: 2.5370, Validation Loss: 2.7188\n",
      "Iteration 27200, Train Loss: 2.9244, Validation Loss: 2.5875\n",
      "Iteration 27300, Train Loss: 3.3698, Validation Loss: 2.2993\n",
      "Iteration 27400, Train Loss: 2.3177, Validation Loss: 2.5626\n",
      "Iteration 27500, Train Loss: 2.7391, Validation Loss: 2.5037\n",
      "Iteration 27600, Train Loss: 2.4102, Validation Loss: 3.1812\n",
      "Iteration 27700, Train Loss: 2.2662, Validation Loss: 2.3779\n",
      "Iteration 27800, Train Loss: 2.4184, Validation Loss: 2.4452\n",
      "Iteration 27900, Train Loss: 2.8050, Validation Loss: 2.6847\n",
      "Iteration 28000, Train Loss: 2.7210, Validation Loss: 2.6014\n",
      "Iteration 28100, Train Loss: 2.8527, Validation Loss: 2.6666\n",
      "Iteration 28200, Train Loss: 2.2535, Validation Loss: 2.8389\n",
      "Iteration 28300, Train Loss: 2.3600, Validation Loss: 2.6405\n",
      "Iteration 28400, Train Loss: 2.3475, Validation Loss: 2.4599\n",
      "Iteration 28500, Train Loss: 2.2866, Validation Loss: 2.1847\n",
      "Iteration 28600, Train Loss: 2.5242, Validation Loss: 2.2929\n",
      "Iteration 28700, Train Loss: 2.4918, Validation Loss: 2.7457\n",
      "Iteration 28800, Train Loss: 2.6341, Validation Loss: 3.1645\n",
      "Iteration 28900, Train Loss: 2.6834, Validation Loss: 2.6418\n",
      "Iteration 29000, Train Loss: 2.4284, Validation Loss: 2.3850\n",
      "Iteration 29100, Train Loss: 2.6439, Validation Loss: 2.5215\n",
      "Iteration 29200, Train Loss: 2.0703, Validation Loss: 2.2893\n",
      "Iteration 29300, Train Loss: 2.5369, Validation Loss: 1.9168\n",
      "Iteration 29400, Train Loss: 2.1800, Validation Loss: 2.7948\n",
      "Iteration 29500, Train Loss: 2.4464, Validation Loss: 2.5237\n",
      "Iteration 29600, Train Loss: 2.4902, Validation Loss: 2.5290\n",
      "Iteration 29700, Train Loss: 2.4900, Validation Loss: 3.0390\n",
      "Iteration 29800, Train Loss: 2.3520, Validation Loss: 2.5671\n",
      "Iteration 29900, Train Loss: 2.3269, Validation Loss: 2.4615\n",
      "Iteration 30000, Train Loss: 2.4122, Validation Loss: 2.2966\n",
      "Iteration 30100, Train Loss: 2.5592, Validation Loss: 2.5316\n",
      "Iteration 30200, Train Loss: 2.7475, Validation Loss: 2.8471\n",
      "Iteration 30300, Train Loss: 2.3204, Validation Loss: 2.2376\n",
      "Iteration 30400, Train Loss: 2.7431, Validation Loss: 2.7702\n",
      "Iteration 30500, Train Loss: 2.6067, Validation Loss: 2.3108\n",
      "Iteration 30600, Train Loss: 2.7241, Validation Loss: 2.7669\n",
      "Iteration 30700, Train Loss: 2.6209, Validation Loss: 2.4643\n",
      "Iteration 30800, Train Loss: 2.0554, Validation Loss: 2.5245\n",
      "Iteration 30900, Train Loss: 2.3010, Validation Loss: 2.7804\n",
      "Iteration 31000, Train Loss: 2.3094, Validation Loss: 2.7612\n",
      "Iteration 31100, Train Loss: 2.7256, Validation Loss: 2.6104\n",
      "Iteration 31200, Train Loss: 2.5966, Validation Loss: 2.5029\n",
      "Iteration 31300, Train Loss: 2.4635, Validation Loss: 2.5796\n",
      "Iteration 31400, Train Loss: 2.6017, Validation Loss: 2.3142\n",
      "Iteration 31500, Train Loss: 2.6438, Validation Loss: 2.4342\n",
      "Iteration 31600, Train Loss: 2.3321, Validation Loss: 2.8202\n",
      "Iteration 31700, Train Loss: 2.6086, Validation Loss: 3.3077\n",
      "Iteration 31800, Train Loss: 2.8951, Validation Loss: 2.3795\n",
      "Iteration 31900, Train Loss: 2.5962, Validation Loss: 2.7510\n",
      "Iteration 32000, Train Loss: 2.2255, Validation Loss: 2.5440\n",
      "Iteration 32100, Train Loss: 2.3941, Validation Loss: 2.2119\n",
      "Iteration 32200, Train Loss: 2.4148, Validation Loss: 2.4808\n",
      "Iteration 32300, Train Loss: 2.6359, Validation Loss: 2.4700\n",
      "Iteration 32400, Train Loss: 2.7905, Validation Loss: 2.6114\n",
      "Iteration 32500, Train Loss: 2.6307, Validation Loss: 2.6096\n",
      "Iteration 32600, Train Loss: 2.4878, Validation Loss: 2.3418\n",
      "Iteration 32700, Train Loss: 2.5718, Validation Loss: 2.6323\n",
      "Iteration 32800, Train Loss: 2.3170, Validation Loss: 3.1034\n",
      "Iteration 32900, Train Loss: 3.1546, Validation Loss: 2.3677\n",
      "Iteration 33000, Train Loss: 2.8569, Validation Loss: 2.4099\n",
      "Iteration 33100, Train Loss: 2.4588, Validation Loss: 2.4083\n",
      "Iteration 33200, Train Loss: 2.0684, Validation Loss: 2.6791\n",
      "Iteration 33300, Train Loss: 2.5645, Validation Loss: 2.8109\n",
      "Iteration 33400, Train Loss: 2.5074, Validation Loss: 2.7863\n",
      "Iteration 33500, Train Loss: 2.0770, Validation Loss: 2.9255\n",
      "Iteration 33600, Train Loss: 2.4551, Validation Loss: 2.6209\n",
      "Iteration 33700, Train Loss: 2.2261, Validation Loss: 2.6279\n",
      "Iteration 33800, Train Loss: 2.4661, Validation Loss: 2.6154\n",
      "Iteration 33900, Train Loss: 2.5769, Validation Loss: 2.9087\n",
      "Iteration 34000, Train Loss: 2.6184, Validation Loss: 2.5639\n",
      "Iteration 34100, Train Loss: 2.2871, Validation Loss: 2.7995\n",
      "Iteration 34200, Train Loss: 2.4927, Validation Loss: 2.0578\n",
      "Iteration 34300, Train Loss: 2.3909, Validation Loss: 3.1111\n",
      "Iteration 34400, Train Loss: 2.1786, Validation Loss: 2.6496\n",
      "Iteration 34500, Train Loss: 2.3687, Validation Loss: 2.8139\n",
      "Iteration 34600, Train Loss: 2.8750, Validation Loss: 2.7791\n",
      "Iteration 34700, Train Loss: 2.1051, Validation Loss: 2.5323\n",
      "Iteration 34800, Train Loss: 3.0212, Validation Loss: 2.5894\n",
      "Iteration 34900, Train Loss: 2.7638, Validation Loss: 2.1260\n",
      "Iteration 35000, Train Loss: 2.6885, Validation Loss: 2.1512\n",
      "Iteration 35100, Train Loss: 2.3909, Validation Loss: 2.3591\n",
      "Iteration 35200, Train Loss: 2.4216, Validation Loss: 2.6070\n",
      "Iteration 35300, Train Loss: 2.5058, Validation Loss: 2.6993\n",
      "Iteration 35400, Train Loss: 2.6171, Validation Loss: 3.0773\n",
      "Iteration 35500, Train Loss: 3.0423, Validation Loss: 2.2092\n",
      "Iteration 35600, Train Loss: 2.4538, Validation Loss: 2.6020\n",
      "Iteration 35700, Train Loss: 2.4979, Validation Loss: 2.8391\n",
      "Iteration 35800, Train Loss: 2.8397, Validation Loss: 2.3663\n",
      "Iteration 35900, Train Loss: 2.4191, Validation Loss: 2.4108\n",
      "Iteration 36000, Train Loss: 2.3260, Validation Loss: 2.1695\n",
      "Iteration 36100, Train Loss: 2.6160, Validation Loss: 2.3015\n",
      "Iteration 36200, Train Loss: 2.3256, Validation Loss: 2.6588\n",
      "Iteration 36300, Train Loss: 2.5167, Validation Loss: 2.8036\n",
      "Iteration 36400, Train Loss: 2.0294, Validation Loss: 2.4945\n",
      "Iteration 36500, Train Loss: 2.4004, Validation Loss: 2.7238\n",
      "Iteration 36600, Train Loss: 2.5378, Validation Loss: 2.4105\n",
      "Iteration 36700, Train Loss: 2.3139, Validation Loss: 2.7748\n",
      "Iteration 36800, Train Loss: 3.1061, Validation Loss: 2.4457\n",
      "Iteration 36900, Train Loss: 2.7456, Validation Loss: 2.0840\n",
      "Iteration 37000, Train Loss: 2.4974, Validation Loss: 2.1556\n",
      "Iteration 37100, Train Loss: 2.4680, Validation Loss: 2.6069\n",
      "Iteration 37200, Train Loss: 2.1179, Validation Loss: 2.3440\n",
      "Iteration 37300, Train Loss: 2.4287, Validation Loss: 2.7018\n",
      "Iteration 37400, Train Loss: 2.3427, Validation Loss: 2.6539\n",
      "Iteration 37500, Train Loss: 2.5692, Validation Loss: 2.8048\n",
      "Iteration 37600, Train Loss: 2.3152, Validation Loss: 2.6381\n",
      "Iteration 37700, Train Loss: 2.5889, Validation Loss: 2.2157\n",
      "Iteration 37800, Train Loss: 2.4421, Validation Loss: 2.3733\n",
      "Iteration 37900, Train Loss: 2.7307, Validation Loss: 2.4827\n",
      "Iteration 38000, Train Loss: 2.4314, Validation Loss: 2.2996\n",
      "Iteration 38100, Train Loss: 2.8196, Validation Loss: 2.7943\n",
      "Iteration 38200, Train Loss: 2.9474, Validation Loss: 2.9721\n",
      "Iteration 38300, Train Loss: 2.6166, Validation Loss: 2.2311\n",
      "Iteration 38400, Train Loss: 2.1759, Validation Loss: 2.4296\n",
      "Iteration 38500, Train Loss: 2.8661, Validation Loss: 2.9242\n",
      "Iteration 38600, Train Loss: 2.5338, Validation Loss: 2.2179\n",
      "Iteration 38700, Train Loss: 2.5649, Validation Loss: 2.7452\n",
      "Iteration 38800, Train Loss: 2.7375, Validation Loss: 2.7512\n",
      "Iteration 38900, Train Loss: 2.6382, Validation Loss: 2.5601\n",
      "Iteration 39000, Train Loss: 2.4735, Validation Loss: 1.9749\n",
      "Iteration 39100, Train Loss: 2.6164, Validation Loss: 2.3657\n",
      "Iteration 39200, Train Loss: 2.9806, Validation Loss: 2.8647\n",
      "Iteration 39300, Train Loss: 2.0695, Validation Loss: 2.5774\n",
      "Iteration 39400, Train Loss: 3.1123, Validation Loss: 2.6792\n",
      "Iteration 39500, Train Loss: 2.7839, Validation Loss: 2.5155\n",
      "Iteration 39600, Train Loss: 2.4170, Validation Loss: 2.3169\n",
      "Iteration 39700, Train Loss: 2.3799, Validation Loss: 2.4309\n",
      "Iteration 39800, Train Loss: 2.5214, Validation Loss: 2.5493\n",
      "Iteration 39900, Train Loss: 2.2332, Validation Loss: 2.7920\n",
      "Iteration 40000, Train Loss: 2.5979, Validation Loss: 2.3016\n",
      "Iteration 40100, Train Loss: 2.6694, Validation Loss: 2.1486\n",
      "Iteration 40200, Train Loss: 2.3393, Validation Loss: 2.4014\n",
      "Iteration 40300, Train Loss: 2.5470, Validation Loss: 2.5346\n",
      "Iteration 40400, Train Loss: 2.7280, Validation Loss: 2.3241\n",
      "Iteration 40500, Train Loss: 3.1321, Validation Loss: 2.6181\n",
      "Iteration 40600, Train Loss: 2.3446, Validation Loss: 2.3687\n",
      "Iteration 40700, Train Loss: 2.7677, Validation Loss: 2.8312\n",
      "Iteration 40800, Train Loss: 2.2588, Validation Loss: 2.4405\n",
      "Iteration 40900, Train Loss: 2.3929, Validation Loss: 2.6744\n",
      "Iteration 41000, Train Loss: 2.4567, Validation Loss: 2.6613\n",
      "Iteration 41100, Train Loss: 2.4901, Validation Loss: 2.5339\n",
      "Iteration 41200, Train Loss: 2.7927, Validation Loss: 2.7230\n",
      "Iteration 41300, Train Loss: 2.7325, Validation Loss: 2.3344\n",
      "Iteration 41400, Train Loss: 2.2812, Validation Loss: 2.3483\n",
      "Iteration 41500, Train Loss: 2.6081, Validation Loss: 2.1956\n",
      "Iteration 41600, Train Loss: 2.5251, Validation Loss: 2.4079\n",
      "Iteration 41700, Train Loss: 2.2976, Validation Loss: 2.4069\n",
      "Iteration 41800, Train Loss: 2.6856, Validation Loss: 2.4286\n",
      "Iteration 41900, Train Loss: 2.7611, Validation Loss: 2.5515\n",
      "Iteration 42000, Train Loss: 2.3924, Validation Loss: 2.2715\n",
      "Iteration 42100, Train Loss: 2.8093, Validation Loss: 2.3501\n",
      "Iteration 42200, Train Loss: 2.3019, Validation Loss: 2.6177\n",
      "Iteration 42300, Train Loss: 2.1778, Validation Loss: 2.4311\n",
      "Iteration 42400, Train Loss: 2.4506, Validation Loss: 2.3680\n",
      "Iteration 42500, Train Loss: 2.3695, Validation Loss: 2.6596\n",
      "Iteration 42600, Train Loss: 2.2123, Validation Loss: 2.6330\n",
      "Iteration 42700, Train Loss: 2.4657, Validation Loss: 2.6958\n",
      "Iteration 42800, Train Loss: 2.3843, Validation Loss: 2.6118\n",
      "Iteration 42900, Train Loss: 2.3786, Validation Loss: 2.5632\n",
      "Iteration 43000, Train Loss: 2.6636, Validation Loss: 3.0439\n",
      "Iteration 43100, Train Loss: 2.4311, Validation Loss: 2.6294\n",
      "Iteration 43200, Train Loss: 2.5319, Validation Loss: 2.3397\n",
      "Iteration 43300, Train Loss: 3.2539, Validation Loss: 2.1301\n",
      "Iteration 43400, Train Loss: 2.2510, Validation Loss: 2.6498\n",
      "Iteration 43500, Train Loss: 2.4783, Validation Loss: 2.7714\n",
      "Iteration 43600, Train Loss: 2.5817, Validation Loss: 2.7653\n",
      "Iteration 43700, Train Loss: 2.3903, Validation Loss: 2.5256\n",
      "Iteration 43800, Train Loss: 2.5158, Validation Loss: 2.9511\n",
      "Iteration 43900, Train Loss: 2.3048, Validation Loss: 2.6152\n",
      "Iteration 44000, Train Loss: 2.9789, Validation Loss: 2.2370\n",
      "Iteration 44100, Train Loss: 2.4527, Validation Loss: 2.5659\n",
      "Iteration 44200, Train Loss: 2.7558, Validation Loss: 2.5243\n",
      "Iteration 44300, Train Loss: 2.5574, Validation Loss: 2.4812\n",
      "Iteration 44400, Train Loss: 2.4823, Validation Loss: 2.2943\n",
      "Iteration 44500, Train Loss: 2.3045, Validation Loss: 2.7063\n",
      "Iteration 44600, Train Loss: 2.8649, Validation Loss: 2.9074\n",
      "Iteration 44700, Train Loss: 2.7501, Validation Loss: 2.4803\n",
      "Iteration 44800, Train Loss: 2.6361, Validation Loss: 2.4088\n",
      "Iteration 44900, Train Loss: 2.2548, Validation Loss: 2.6213\n",
      "Iteration 45000, Train Loss: 2.1664, Validation Loss: 2.3569\n",
      "Iteration 45100, Train Loss: 2.3488, Validation Loss: 2.3976\n",
      "Iteration 45200, Train Loss: 2.0483, Validation Loss: 2.6035\n",
      "Iteration 45300, Train Loss: 2.6458, Validation Loss: 2.3095\n",
      "Iteration 45400, Train Loss: 2.8719, Validation Loss: 2.0436\n",
      "Iteration 45500, Train Loss: 2.3317, Validation Loss: 3.0322\n",
      "Iteration 45600, Train Loss: 2.5244, Validation Loss: 1.9943\n",
      "Iteration 45700, Train Loss: 2.3825, Validation Loss: 2.7169\n",
      "Iteration 45800, Train Loss: 2.6095, Validation Loss: 2.5465\n",
      "Iteration 45900, Train Loss: 2.5233, Validation Loss: 2.2447\n",
      "Iteration 46000, Train Loss: 2.8080, Validation Loss: 2.6404\n",
      "Iteration 46100, Train Loss: 2.5494, Validation Loss: 2.6370\n",
      "Iteration 46200, Train Loss: 2.4413, Validation Loss: 2.3906\n",
      "Iteration 46300, Train Loss: 2.5857, Validation Loss: 2.7008\n",
      "Iteration 46400, Train Loss: 2.6855, Validation Loss: 2.5977\n",
      "Iteration 46500, Train Loss: 2.5758, Validation Loss: 2.4020\n",
      "Iteration 46600, Train Loss: 2.6813, Validation Loss: 2.7029\n",
      "Iteration 46700, Train Loss: 2.5633, Validation Loss: 2.6821\n",
      "Iteration 46800, Train Loss: 2.6091, Validation Loss: 2.5822\n",
      "Iteration 46900, Train Loss: 2.1668, Validation Loss: 2.7037\n",
      "Iteration 47000, Train Loss: 2.3068, Validation Loss: 2.3585\n",
      "Iteration 47100, Train Loss: 2.5579, Validation Loss: 2.7436\n",
      "Iteration 47200, Train Loss: 2.6204, Validation Loss: 2.7422\n",
      "Iteration 47300, Train Loss: 2.7092, Validation Loss: 2.3972\n",
      "Iteration 47400, Train Loss: 2.5453, Validation Loss: 2.2638\n",
      "Iteration 47500, Train Loss: 2.4271, Validation Loss: 2.2341\n",
      "Iteration 47600, Train Loss: 2.4507, Validation Loss: 2.5580\n",
      "Iteration 47700, Train Loss: 2.2657, Validation Loss: 2.6826\n",
      "Iteration 47800, Train Loss: 2.6034, Validation Loss: 2.9099\n",
      "Iteration 47900, Train Loss: 2.4467, Validation Loss: 2.6142\n",
      "Iteration 48000, Train Loss: 2.7657, Validation Loss: 2.7179\n",
      "Iteration 48100, Train Loss: 2.2985, Validation Loss: 2.6854\n",
      "Iteration 48200, Train Loss: 2.3065, Validation Loss: 2.4440\n",
      "Iteration 48300, Train Loss: 3.0053, Validation Loss: 2.4900\n",
      "Iteration 48400, Train Loss: 2.7857, Validation Loss: 2.4014\n",
      "Iteration 48500, Train Loss: 2.4366, Validation Loss: 1.9649\n",
      "Iteration 48600, Train Loss: 2.3049, Validation Loss: 2.5323\n",
      "Iteration 48700, Train Loss: 2.5926, Validation Loss: 2.8309\n",
      "Iteration 48800, Train Loss: 2.5706, Validation Loss: 2.3194\n",
      "Iteration 48900, Train Loss: 2.8114, Validation Loss: 2.7468\n",
      "Iteration 49000, Train Loss: 2.4566, Validation Loss: 2.7220\n",
      "Iteration 49100, Train Loss: 2.6393, Validation Loss: 2.5102\n",
      "Iteration 49200, Train Loss: 2.7808, Validation Loss: 2.5011\n",
      "Iteration 49300, Train Loss: 2.3952, Validation Loss: 2.6564\n",
      "Iteration 49400, Train Loss: 2.6926, Validation Loss: 2.2293\n",
      "Iteration 49500, Train Loss: 2.4067, Validation Loss: 2.6446\n",
      "Iteration 49600, Train Loss: 2.1820, Validation Loss: 2.4162\n",
      "Iteration 49700, Train Loss: 2.6131, Validation Loss: 2.1381\n",
      "Iteration 49800, Train Loss: 2.7849, Validation Loss: 2.3168\n",
      "Iteration 49900, Train Loss: 2.8113, Validation Loss: 2.7413\n",
      "Iteration 50000, Train Loss: 2.3934, Validation Loss: 2.9792\n",
      "Iteration 50100, Train Loss: 2.6669, Validation Loss: 2.4942\n",
      "Iteration 50200, Train Loss: 2.2712, Validation Loss: 2.2586\n",
      "Iteration 50300, Train Loss: 2.6062, Validation Loss: 2.6795\n",
      "Iteration 50400, Train Loss: 2.3181, Validation Loss: 2.2253\n",
      "Iteration 50500, Train Loss: 2.3963, Validation Loss: 2.4932\n",
      "Iteration 50600, Train Loss: 2.3282, Validation Loss: 2.3562\n",
      "Iteration 50700, Train Loss: 2.3823, Validation Loss: 2.7703\n",
      "Iteration 50800, Train Loss: 2.5593, Validation Loss: 2.9008\n",
      "Iteration 50900, Train Loss: 2.7101, Validation Loss: 2.1791\n",
      "Iteration 51000, Train Loss: 2.4093, Validation Loss: 2.3649\n",
      "Iteration 51100, Train Loss: 2.2188, Validation Loss: 2.7802\n",
      "Iteration 51200, Train Loss: 2.2233, Validation Loss: 2.1024\n",
      "Iteration 51300, Train Loss: 2.5095, Validation Loss: 2.4911\n",
      "Iteration 51400, Train Loss: 2.8741, Validation Loss: 2.5984\n",
      "Iteration 51500, Train Loss: 2.3970, Validation Loss: 2.2893\n",
      "Iteration 51600, Train Loss: 2.3131, Validation Loss: 2.6755\n",
      "Iteration 51700, Train Loss: 2.3043, Validation Loss: 2.8612\n",
      "Iteration 51800, Train Loss: 2.8701, Validation Loss: 2.3843\n",
      "Iteration 51900, Train Loss: 2.2856, Validation Loss: 1.8782\n",
      "Iteration 52000, Train Loss: 2.9166, Validation Loss: 2.5521\n",
      "Iteration 52100, Train Loss: 2.8142, Validation Loss: 2.6718\n",
      "Iteration 52200, Train Loss: 2.3828, Validation Loss: 2.7321\n",
      "Iteration 52300, Train Loss: 2.7916, Validation Loss: 2.8546\n",
      "Iteration 52400, Train Loss: 2.7356, Validation Loss: 2.5670\n",
      "Iteration 52500, Train Loss: 2.8944, Validation Loss: 2.4313\n",
      "Iteration 52600, Train Loss: 2.4917, Validation Loss: 2.2884\n",
      "Iteration 52700, Train Loss: 2.7371, Validation Loss: 2.3754\n",
      "Iteration 52800, Train Loss: 2.2666, Validation Loss: 2.4504\n",
      "Iteration 52900, Train Loss: 2.4919, Validation Loss: 2.0733\n",
      "Iteration 53000, Train Loss: 2.7152, Validation Loss: 2.3112\n",
      "Iteration 53100, Train Loss: 2.7110, Validation Loss: 2.5024\n",
      "Iteration 53200, Train Loss: 2.0342, Validation Loss: 2.1714\n",
      "Iteration 53300, Train Loss: 2.1150, Validation Loss: 2.3069\n",
      "Iteration 53400, Train Loss: 2.3892, Validation Loss: 2.3204\n",
      "Iteration 53500, Train Loss: 2.1318, Validation Loss: 2.8054\n",
      "Iteration 53600, Train Loss: 2.1958, Validation Loss: 2.3729\n",
      "Iteration 53700, Train Loss: 2.4829, Validation Loss: 2.5908\n",
      "Iteration 53800, Train Loss: 2.2688, Validation Loss: 2.3070\n",
      "Iteration 53900, Train Loss: 2.3348, Validation Loss: 2.4426\n",
      "Iteration 54000, Train Loss: 2.6587, Validation Loss: 3.1021\n",
      "Iteration 54100, Train Loss: 2.6301, Validation Loss: 2.4966\n",
      "Iteration 54200, Train Loss: 2.4898, Validation Loss: 2.4866\n",
      "Iteration 54300, Train Loss: 2.3224, Validation Loss: 2.3070\n",
      "Iteration 54400, Train Loss: 2.5542, Validation Loss: 2.3720\n",
      "Iteration 54500, Train Loss: 2.3333, Validation Loss: 2.2571\n",
      "Iteration 54600, Train Loss: 2.2004, Validation Loss: 2.7699\n",
      "Iteration 54700, Train Loss: 2.2127, Validation Loss: 2.5267\n",
      "Iteration 54800, Train Loss: 2.2305, Validation Loss: 2.5231\n",
      "Iteration 54900, Train Loss: 2.1867, Validation Loss: 2.3495\n",
      "Iteration 55000, Train Loss: 2.3762, Validation Loss: 2.3768\n",
      "Iteration 55100, Train Loss: 2.4815, Validation Loss: 2.8201\n",
      "Iteration 55200, Train Loss: 2.4059, Validation Loss: 2.4134\n",
      "Iteration 55300, Train Loss: 2.7335, Validation Loss: 2.7554\n",
      "Iteration 55400, Train Loss: 2.8160, Validation Loss: 2.3643\n",
      "Iteration 55500, Train Loss: 2.1822, Validation Loss: 2.3850\n",
      "Iteration 55600, Train Loss: 2.2960, Validation Loss: 2.2128\n",
      "Iteration 55700, Train Loss: 2.5128, Validation Loss: 2.5558\n",
      "Iteration 55800, Train Loss: 2.3701, Validation Loss: 2.5673\n",
      "Iteration 55900, Train Loss: 2.3628, Validation Loss: 2.1637\n",
      "Iteration 56000, Train Loss: 2.2874, Validation Loss: 2.2604\n",
      "Iteration 56100, Train Loss: 2.4804, Validation Loss: 2.4102\n",
      "Iteration 56200, Train Loss: 2.2865, Validation Loss: 2.5485\n",
      "Iteration 56300, Train Loss: 2.1508, Validation Loss: 2.0290\n",
      "Iteration 56400, Train Loss: 2.1613, Validation Loss: 2.2279\n",
      "Iteration 56500, Train Loss: 2.3991, Validation Loss: 2.2122\n",
      "Iteration 56600, Train Loss: 2.2616, Validation Loss: 2.3909\n",
      "Iteration 56700, Train Loss: 2.4761, Validation Loss: 2.4006\n",
      "Iteration 56800, Train Loss: 2.1024, Validation Loss: 2.6839\n",
      "Iteration 56900, Train Loss: 2.5630, Validation Loss: 2.7569\n",
      "Iteration 57000, Train Loss: 2.2255, Validation Loss: 2.6088\n",
      "Iteration 57100, Train Loss: 2.3006, Validation Loss: 2.3571\n",
      "Iteration 57200, Train Loss: 2.8992, Validation Loss: 2.4842\n",
      "Iteration 57300, Train Loss: 2.8148, Validation Loss: 2.3640\n",
      "Iteration 57400, Train Loss: 2.8268, Validation Loss: 2.2700\n",
      "Iteration 57500, Train Loss: 2.6218, Validation Loss: 2.2489\n",
      "Iteration 57600, Train Loss: 2.7564, Validation Loss: 2.3846\n",
      "Iteration 57700, Train Loss: 2.2913, Validation Loss: 2.4799\n",
      "Iteration 57800, Train Loss: 2.4218, Validation Loss: 2.2980\n",
      "Iteration 57900, Train Loss: 2.5635, Validation Loss: 2.4899\n",
      "Iteration 58000, Train Loss: 2.6170, Validation Loss: 2.2240\n",
      "Iteration 58100, Train Loss: 2.7338, Validation Loss: 2.5158\n",
      "Iteration 58200, Train Loss: 2.3706, Validation Loss: 2.3463\n",
      "Iteration 58300, Train Loss: 1.9967, Validation Loss: 2.3942\n",
      "Iteration 58400, Train Loss: 2.4594, Validation Loss: 2.6868\n",
      "Iteration 58500, Train Loss: 2.4321, Validation Loss: 2.4721\n",
      "Iteration 58600, Train Loss: 2.4745, Validation Loss: 2.4763\n",
      "Iteration 58700, Train Loss: 2.3998, Validation Loss: 3.2861\n",
      "Iteration 58800, Train Loss: 2.5472, Validation Loss: 2.3821\n",
      "Iteration 58900, Train Loss: 2.3242, Validation Loss: 2.4665\n",
      "Iteration 59000, Train Loss: 2.9116, Validation Loss: 2.3645\n",
      "Iteration 59100, Train Loss: 2.5656, Validation Loss: 2.2544\n",
      "Iteration 59200, Train Loss: 2.6291, Validation Loss: 2.0312\n",
      "Iteration 59300, Train Loss: 2.9719, Validation Loss: 2.6462\n",
      "Iteration 59400, Train Loss: 2.5263, Validation Loss: 2.3322\n",
      "Iteration 59500, Train Loss: 2.4013, Validation Loss: 2.4798\n",
      "Iteration 59600, Train Loss: 2.5484, Validation Loss: 2.8200\n",
      "Iteration 59700, Train Loss: 2.6570, Validation Loss: 2.6174\n",
      "Iteration 59800, Train Loss: 2.5261, Validation Loss: 2.2950\n",
      "Iteration 59900, Train Loss: 2.8679, Validation Loss: 3.0311\n",
      "Iteration 60000, Train Loss: 2.6720, Validation Loss: 2.2039\n",
      "Iteration 60100, Train Loss: 2.0497, Validation Loss: 2.5470\n",
      "Iteration 60200, Train Loss: 2.3406, Validation Loss: 2.6398\n",
      "Iteration 60300, Train Loss: 2.6055, Validation Loss: 2.5428\n",
      "Iteration 60400, Train Loss: 2.3940, Validation Loss: 2.4884\n",
      "Iteration 60500, Train Loss: 2.4144, Validation Loss: 2.4096\n",
      "Iteration 60600, Train Loss: 2.0451, Validation Loss: 2.6059\n",
      "Iteration 60700, Train Loss: 2.6301, Validation Loss: 2.5932\n",
      "Iteration 60800, Train Loss: 2.8021, Validation Loss: 2.3287\n",
      "Iteration 60900, Train Loss: 2.5604, Validation Loss: 2.2035\n",
      "Iteration 61000, Train Loss: 2.6362, Validation Loss: 2.1871\n",
      "Iteration 61100, Train Loss: 2.5337, Validation Loss: 2.4420\n",
      "Iteration 61200, Train Loss: 2.8859, Validation Loss: 2.3559\n",
      "Iteration 61300, Train Loss: 2.7392, Validation Loss: 2.3834\n",
      "Iteration 61400, Train Loss: 2.3579, Validation Loss: 2.8063\n",
      "Iteration 61500, Train Loss: 2.3021, Validation Loss: 2.4778\n",
      "Iteration 61600, Train Loss: 2.2069, Validation Loss: 2.3616\n",
      "Iteration 61700, Train Loss: 2.4616, Validation Loss: 3.1036\n",
      "Iteration 61800, Train Loss: 2.6845, Validation Loss: 2.3935\n",
      "Iteration 61900, Train Loss: 2.1189, Validation Loss: 2.3900\n",
      "Iteration 62000, Train Loss: 2.5668, Validation Loss: 2.4528\n",
      "Iteration 62100, Train Loss: 2.5150, Validation Loss: 2.7572\n",
      "Iteration 62200, Train Loss: 2.2540, Validation Loss: 2.4595\n",
      "Iteration 62300, Train Loss: 2.3282, Validation Loss: 2.4048\n",
      "Iteration 62400, Train Loss: 2.7794, Validation Loss: 2.6236\n",
      "Iteration 62500, Train Loss: 2.4629, Validation Loss: 2.1742\n",
      "Iteration 62600, Train Loss: 2.1840, Validation Loss: 2.2911\n",
      "Iteration 62700, Train Loss: 2.4968, Validation Loss: 2.4191\n",
      "Iteration 62800, Train Loss: 2.7982, Validation Loss: 2.3485\n",
      "Iteration 62900, Train Loss: 2.6936, Validation Loss: 2.8123\n",
      "Iteration 63000, Train Loss: 2.3927, Validation Loss: 2.6177\n",
      "Iteration 63100, Train Loss: 2.2598, Validation Loss: 2.8255\n",
      "Iteration 63200, Train Loss: 2.4928, Validation Loss: 2.5370\n",
      "Iteration 63300, Train Loss: 2.3489, Validation Loss: 2.4035\n",
      "Iteration 63400, Train Loss: 2.5905, Validation Loss: 2.5555\n",
      "Iteration 63500, Train Loss: 2.2936, Validation Loss: 2.6739\n",
      "Iteration 63600, Train Loss: 2.5712, Validation Loss: 2.5882\n",
      "Iteration 63700, Train Loss: 2.4389, Validation Loss: 2.0816\n",
      "Iteration 63800, Train Loss: 2.7205, Validation Loss: 2.7413\n",
      "Iteration 63900, Train Loss: 2.5169, Validation Loss: 2.3251\n",
      "Iteration 64000, Train Loss: 2.2947, Validation Loss: 3.1419\n",
      "Iteration 64100, Train Loss: 2.4058, Validation Loss: 2.4486\n",
      "Iteration 64200, Train Loss: 2.3638, Validation Loss: 2.3931\n",
      "Iteration 64300, Train Loss: 1.9898, Validation Loss: 2.4518\n",
      "Iteration 64400, Train Loss: 2.3374, Validation Loss: 2.2870\n",
      "Iteration 64500, Train Loss: 2.4332, Validation Loss: 2.4862\n",
      "Iteration 64600, Train Loss: 2.4948, Validation Loss: 2.0307\n",
      "Iteration 64700, Train Loss: 2.6705, Validation Loss: 2.1319\n",
      "Iteration 64800, Train Loss: 2.4082, Validation Loss: 2.1391\n",
      "Iteration 64900, Train Loss: 2.6176, Validation Loss: 2.2247\n",
      "Iteration 65000, Train Loss: 2.4278, Validation Loss: 2.3435\n",
      "Iteration 65100, Train Loss: 2.4657, Validation Loss: 2.2349\n",
      "Iteration 65200, Train Loss: 2.6411, Validation Loss: 2.3133\n",
      "Iteration 65300, Train Loss: 2.8116, Validation Loss: 2.7721\n",
      "Iteration 65400, Train Loss: 2.5053, Validation Loss: 2.4738\n",
      "Iteration 65500, Train Loss: 2.0845, Validation Loss: 2.1786\n",
      "Iteration 65600, Train Loss: 2.3242, Validation Loss: 2.7132\n",
      "Iteration 65700, Train Loss: 2.3604, Validation Loss: 2.5039\n",
      "Iteration 65800, Train Loss: 2.3748, Validation Loss: 2.8553\n",
      "Iteration 65900, Train Loss: 2.3884, Validation Loss: 2.4998\n",
      "Iteration 66000, Train Loss: 2.8353, Validation Loss: 2.1621\n",
      "Iteration 66100, Train Loss: 2.8125, Validation Loss: 2.7642\n",
      "Iteration 66200, Train Loss: 2.0895, Validation Loss: 2.2553\n",
      "Iteration 66300, Train Loss: 2.1670, Validation Loss: 2.3425\n",
      "Iteration 66400, Train Loss: 2.4961, Validation Loss: 2.8378\n",
      "Iteration 66500, Train Loss: 2.6503, Validation Loss: 2.2716\n",
      "Iteration 66600, Train Loss: 2.4859, Validation Loss: 2.4358\n",
      "Iteration 66700, Train Loss: 2.2483, Validation Loss: 2.6010\n",
      "Iteration 66800, Train Loss: 2.5540, Validation Loss: 2.3666\n",
      "Iteration 66900, Train Loss: 2.5667, Validation Loss: 2.4403\n",
      "Iteration 67000, Train Loss: 2.3591, Validation Loss: 2.3151\n",
      "Iteration 67100, Train Loss: 2.4965, Validation Loss: 2.5808\n",
      "Iteration 67200, Train Loss: 2.6148, Validation Loss: 2.2924\n",
      "Iteration 67300, Train Loss: 2.6173, Validation Loss: 2.9725\n",
      "Iteration 67400, Train Loss: 1.8267, Validation Loss: 2.1230\n",
      "Iteration 67500, Train Loss: 2.4714, Validation Loss: 2.6919\n",
      "Iteration 67600, Train Loss: 2.1356, Validation Loss: 2.7578\n",
      "Iteration 67700, Train Loss: 2.3339, Validation Loss: 2.5317\n",
      "Iteration 67800, Train Loss: 2.5487, Validation Loss: 2.8266\n",
      "Iteration 67900, Train Loss: 2.2494, Validation Loss: 2.4146\n",
      "Iteration 68000, Train Loss: 2.2242, Validation Loss: 2.2478\n",
      "Iteration 68100, Train Loss: 2.4503, Validation Loss: 2.8061\n",
      "Iteration 68200, Train Loss: 2.7067, Validation Loss: 2.4574\n",
      "Iteration 68300, Train Loss: 2.0957, Validation Loss: 2.2918\n",
      "Iteration 68400, Train Loss: 2.7455, Validation Loss: 2.5015\n",
      "Iteration 68500, Train Loss: 2.3420, Validation Loss: 2.5083\n",
      "Iteration 68600, Train Loss: 2.0457, Validation Loss: 2.3199\n",
      "Iteration 68700, Train Loss: 2.4235, Validation Loss: 2.2095\n",
      "Iteration 68800, Train Loss: 2.9604, Validation Loss: 2.9198\n",
      "Iteration 68900, Train Loss: 2.4436, Validation Loss: 2.6068\n",
      "Iteration 69000, Train Loss: 1.9094, Validation Loss: 2.4928\n",
      "Iteration 69100, Train Loss: 2.6765, Validation Loss: 2.2114\n",
      "Iteration 69200, Train Loss: 2.2313, Validation Loss: 2.3305\n",
      "Iteration 69300, Train Loss: 2.9024, Validation Loss: 2.7870\n",
      "Iteration 69400, Train Loss: 2.1885, Validation Loss: 2.5359\n",
      "Iteration 69500, Train Loss: 2.2963, Validation Loss: 2.7569\n",
      "Iteration 69600, Train Loss: 2.3531, Validation Loss: 2.4050\n",
      "Iteration 69700, Train Loss: 2.3266, Validation Loss: 2.4234\n",
      "Iteration 69800, Train Loss: 2.5701, Validation Loss: 2.3159\n",
      "Iteration 69900, Train Loss: 2.2919, Validation Loss: 2.7885\n",
      "Iteration 70000, Train Loss: 2.7828, Validation Loss: 2.5215\n",
      "Iteration 70100, Train Loss: 2.1891, Validation Loss: 2.6189\n",
      "Iteration 70200, Train Loss: 2.6887, Validation Loss: 2.4686\n",
      "Iteration 70300, Train Loss: 2.4610, Validation Loss: 2.6916\n",
      "Iteration 70400, Train Loss: 1.9075, Validation Loss: 2.4471\n",
      "Iteration 70500, Train Loss: 2.8283, Validation Loss: 2.1719\n",
      "Iteration 70600, Train Loss: 3.1258, Validation Loss: 2.3176\n",
      "Iteration 70700, Train Loss: 2.5390, Validation Loss: 2.2868\n",
      "Iteration 70800, Train Loss: 2.4489, Validation Loss: 2.3410\n",
      "Iteration 70900, Train Loss: 2.3332, Validation Loss: 2.3335\n",
      "Iteration 71000, Train Loss: 2.3471, Validation Loss: 2.5881\n",
      "Iteration 71100, Train Loss: 2.0787, Validation Loss: 2.3693\n",
      "Iteration 71200, Train Loss: 2.8588, Validation Loss: 2.4693\n",
      "Iteration 71300, Train Loss: 2.4574, Validation Loss: 2.3334\n",
      "Iteration 71400, Train Loss: 2.6117, Validation Loss: 2.2261\n",
      "Iteration 71500, Train Loss: 2.5854, Validation Loss: 2.5345\n",
      "Iteration 71600, Train Loss: 2.5828, Validation Loss: 2.6756\n",
      "Iteration 71700, Train Loss: 2.3226, Validation Loss: 2.3019\n",
      "Iteration 71800, Train Loss: 2.1577, Validation Loss: 2.3008\n",
      "Iteration 71900, Train Loss: 2.3376, Validation Loss: 2.7764\n",
      "Iteration 72000, Train Loss: 2.0773, Validation Loss: 2.0468\n",
      "Iteration 72100, Train Loss: 2.4294, Validation Loss: 2.5873\n",
      "Iteration 72200, Train Loss: 2.6880, Validation Loss: 2.5581\n",
      "Iteration 72300, Train Loss: 2.8437, Validation Loss: 2.0035\n",
      "Iteration 72400, Train Loss: 2.3010, Validation Loss: 2.4645\n",
      "Iteration 72500, Train Loss: 2.8003, Validation Loss: 2.1377\n",
      "Iteration 72600, Train Loss: 2.3688, Validation Loss: 2.0078\n",
      "Iteration 72700, Train Loss: 2.3648, Validation Loss: 2.6427\n",
      "Iteration 72800, Train Loss: 2.5710, Validation Loss: 2.6613\n",
      "Iteration 72900, Train Loss: 2.3634, Validation Loss: 2.3521\n",
      "Iteration 73000, Train Loss: 2.4403, Validation Loss: 2.3711\n",
      "Iteration 73100, Train Loss: 2.6310, Validation Loss: 2.4380\n",
      "Iteration 73200, Train Loss: 2.3464, Validation Loss: 2.6545\n",
      "Iteration 73300, Train Loss: 2.5748, Validation Loss: 2.6781\n",
      "Iteration 73400, Train Loss: 2.4113, Validation Loss: 2.6980\n",
      "Iteration 73500, Train Loss: 2.5372, Validation Loss: 2.4205\n",
      "Iteration 73600, Train Loss: 2.5075, Validation Loss: 2.2040\n",
      "Iteration 73700, Train Loss: 2.4105, Validation Loss: 2.5241\n",
      "Iteration 73800, Train Loss: 2.3112, Validation Loss: 2.6269\n",
      "Iteration 73900, Train Loss: 2.1890, Validation Loss: 2.9305\n",
      "Iteration 74000, Train Loss: 2.5406, Validation Loss: 2.2206\n",
      "Iteration 74100, Train Loss: 2.0633, Validation Loss: 2.5215\n",
      "Iteration 74200, Train Loss: 2.4737, Validation Loss: 2.6265\n",
      "Iteration 74300, Train Loss: 2.0830, Validation Loss: 2.8659\n",
      "Iteration 74400, Train Loss: 2.6056, Validation Loss: 2.3968\n",
      "Iteration 74500, Train Loss: 2.5514, Validation Loss: 2.8558\n",
      "Iteration 74600, Train Loss: 2.3495, Validation Loss: 2.1806\n",
      "Iteration 74700, Train Loss: 2.4307, Validation Loss: 2.6700\n",
      "Iteration 74800, Train Loss: 2.4068, Validation Loss: 2.9419\n",
      "Iteration 74900, Train Loss: 2.1066, Validation Loss: 2.4519\n",
      "Iteration 75000, Train Loss: 2.3149, Validation Loss: 2.7735\n",
      "Iteration 75100, Train Loss: 2.5301, Validation Loss: 2.4930\n",
      "Iteration 75200, Train Loss: 2.4051, Validation Loss: 2.4711\n",
      "Iteration 75300, Train Loss: 2.3066, Validation Loss: 2.4714\n",
      "Iteration 75400, Train Loss: 2.7375, Validation Loss: 2.6618\n",
      "Iteration 75500, Train Loss: 2.4840, Validation Loss: 2.2123\n",
      "Iteration 75600, Train Loss: 2.1445, Validation Loss: 2.0934\n",
      "Iteration 75700, Train Loss: 2.1198, Validation Loss: 2.7304\n",
      "Iteration 75800, Train Loss: 2.8672, Validation Loss: 2.1304\n",
      "Iteration 75900, Train Loss: 2.1881, Validation Loss: 2.6472\n",
      "Iteration 76000, Train Loss: 2.6894, Validation Loss: 2.0751\n",
      "Iteration 76100, Train Loss: 2.1934, Validation Loss: 2.5680\n",
      "Iteration 76200, Train Loss: 2.8911, Validation Loss: 3.1461\n",
      "Iteration 76300, Train Loss: 2.4579, Validation Loss: 2.7511\n",
      "Iteration 76400, Train Loss: 2.6360, Validation Loss: 2.5674\n",
      "Iteration 76500, Train Loss: 2.7066, Validation Loss: 2.5692\n",
      "Iteration 76600, Train Loss: 2.1623, Validation Loss: 2.3301\n",
      "Iteration 76700, Train Loss: 2.6031, Validation Loss: 2.5273\n",
      "Iteration 76800, Train Loss: 2.6638, Validation Loss: 2.2846\n",
      "Iteration 76900, Train Loss: 2.7066, Validation Loss: 2.3301\n",
      "Iteration 77000, Train Loss: 2.2909, Validation Loss: 2.5079\n",
      "Iteration 77100, Train Loss: 2.8099, Validation Loss: 2.0475\n",
      "Iteration 77200, Train Loss: 2.2810, Validation Loss: 2.0228\n",
      "Iteration 77300, Train Loss: 2.3251, Validation Loss: 2.4281\n",
      "Iteration 77400, Train Loss: 2.4235, Validation Loss: 2.7012\n",
      "Iteration 77500, Train Loss: 2.6136, Validation Loss: 2.7516\n",
      "Iteration 77600, Train Loss: 2.2703, Validation Loss: 2.5368\n",
      "Iteration 77700, Train Loss: 2.2181, Validation Loss: 2.3636\n",
      "Iteration 77800, Train Loss: 2.6032, Validation Loss: 2.4843\n",
      "Iteration 77900, Train Loss: 2.3700, Validation Loss: 2.8404\n",
      "Iteration 78000, Train Loss: 2.8336, Validation Loss: 2.1998\n",
      "Iteration 78100, Train Loss: 2.6749, Validation Loss: 2.2875\n",
      "Iteration 78200, Train Loss: 2.0151, Validation Loss: 2.3190\n",
      "Iteration 78300, Train Loss: 2.4084, Validation Loss: 2.4264\n",
      "Iteration 78400, Train Loss: 2.7098, Validation Loss: 2.0620\n",
      "Iteration 78500, Train Loss: 2.6695, Validation Loss: 2.2785\n",
      "Iteration 78600, Train Loss: 2.2568, Validation Loss: 2.5114\n",
      "Iteration 78700, Train Loss: 2.0125, Validation Loss: 2.2659\n",
      "Iteration 78800, Train Loss: 2.5744, Validation Loss: 2.6242\n",
      "Iteration 78900, Train Loss: 2.6271, Validation Loss: 2.3568\n",
      "Iteration 79000, Train Loss: 2.4375, Validation Loss: 2.0007\n",
      "Iteration 79100, Train Loss: 2.4829, Validation Loss: 2.2259\n",
      "Iteration 79200, Train Loss: 2.6825, Validation Loss: 2.5892\n",
      "Iteration 79300, Train Loss: 2.7186, Validation Loss: 2.7858\n",
      "Iteration 79400, Train Loss: 2.4321, Validation Loss: 2.0827\n",
      "Iteration 79500, Train Loss: 2.2978, Validation Loss: 2.7837\n",
      "Iteration 79600, Train Loss: 2.5417, Validation Loss: 2.2277\n",
      "Iteration 79700, Train Loss: 2.7332, Validation Loss: 2.3194\n",
      "Iteration 79800, Train Loss: 2.4974, Validation Loss: 2.7280\n",
      "Iteration 79900, Train Loss: 2.4192, Validation Loss: 2.7691\n",
      "Iteration 80000, Train Loss: 2.5840, Validation Loss: 2.7370\n",
      "Iteration 80100, Train Loss: 2.4109, Validation Loss: 2.6152\n",
      "Iteration 80200, Train Loss: 2.9836, Validation Loss: 2.2387\n",
      "Iteration 80300, Train Loss: 2.2633, Validation Loss: 2.5301\n",
      "Iteration 80400, Train Loss: 2.1566, Validation Loss: 2.4217\n",
      "Iteration 80500, Train Loss: 2.3323, Validation Loss: 2.5937\n",
      "Iteration 80600, Train Loss: 2.1962, Validation Loss: 2.1344\n",
      "Iteration 80700, Train Loss: 2.4918, Validation Loss: 3.0973\n",
      "Iteration 80800, Train Loss: 2.3843, Validation Loss: 2.5618\n",
      "Iteration 80900, Train Loss: 2.7490, Validation Loss: 2.5928\n",
      "Iteration 81000, Train Loss: 2.8065, Validation Loss: 2.8413\n",
      "Iteration 81100, Train Loss: 2.3935, Validation Loss: 2.3759\n",
      "Iteration 81200, Train Loss: 2.6228, Validation Loss: 2.4436\n",
      "Iteration 81300, Train Loss: 2.5913, Validation Loss: 2.6324\n",
      "Iteration 81400, Train Loss: 2.3529, Validation Loss: 2.6724\n",
      "Iteration 81500, Train Loss: 2.4430, Validation Loss: 2.5338\n",
      "Iteration 81600, Train Loss: 2.7424, Validation Loss: 2.7696\n",
      "Iteration 81700, Train Loss: 2.7212, Validation Loss: 2.5112\n",
      "Iteration 81800, Train Loss: 2.1210, Validation Loss: 2.3713\n",
      "Iteration 81900, Train Loss: 2.5329, Validation Loss: 2.4084\n",
      "Iteration 82000, Train Loss: 2.0784, Validation Loss: 2.2863\n",
      "Iteration 82100, Train Loss: 2.1106, Validation Loss: 2.4835\n",
      "Iteration 82200, Train Loss: 2.4019, Validation Loss: 2.2447\n",
      "Iteration 82300, Train Loss: 2.5357, Validation Loss: 2.4671\n",
      "Iteration 82400, Train Loss: 2.0657, Validation Loss: 2.1432\n",
      "Iteration 82500, Train Loss: 2.3487, Validation Loss: 1.9866\n",
      "Iteration 82600, Train Loss: 2.4232, Validation Loss: 2.2823\n",
      "Iteration 82700, Train Loss: 2.3511, Validation Loss: 2.7797\n",
      "Iteration 82800, Train Loss: 2.3501, Validation Loss: 2.5511\n",
      "Iteration 82900, Train Loss: 2.3805, Validation Loss: 2.2419\n",
      "Iteration 83000, Train Loss: 2.4535, Validation Loss: 2.3277\n",
      "Iteration 83100, Train Loss: 2.3173, Validation Loss: 2.5446\n",
      "Iteration 83200, Train Loss: 2.6637, Validation Loss: 2.1142\n",
      "Iteration 83300, Train Loss: 2.5382, Validation Loss: 2.5062\n",
      "Iteration 83400, Train Loss: 2.4486, Validation Loss: 2.2501\n",
      "Iteration 83500, Train Loss: 2.3931, Validation Loss: 2.1632\n",
      "Iteration 83600, Train Loss: 2.7299, Validation Loss: 1.9471\n",
      "Iteration 83700, Train Loss: 1.9438, Validation Loss: 2.5396\n",
      "Iteration 83800, Train Loss: 2.6866, Validation Loss: 2.0866\n",
      "Iteration 83900, Train Loss: 2.4398, Validation Loss: 2.3468\n",
      "Iteration 84000, Train Loss: 2.4334, Validation Loss: 2.5891\n",
      "Iteration 84100, Train Loss: 2.7232, Validation Loss: 2.3233\n",
      "Iteration 84200, Train Loss: 2.5254, Validation Loss: 2.5256\n",
      "Iteration 84300, Train Loss: 2.0284, Validation Loss: 2.7498\n",
      "Iteration 84400, Train Loss: 2.2090, Validation Loss: 2.4110\n",
      "Iteration 84500, Train Loss: 2.2572, Validation Loss: 2.4579\n",
      "Iteration 84600, Train Loss: 2.3875, Validation Loss: 1.8982\n",
      "Iteration 84700, Train Loss: 2.3952, Validation Loss: 2.3370\n",
      "Iteration 84800, Train Loss: 2.0286, Validation Loss: 2.6911\n",
      "Iteration 84900, Train Loss: 2.5287, Validation Loss: 2.1918\n",
      "Iteration 85000, Train Loss: 2.3017, Validation Loss: 2.8281\n",
      "Iteration 85100, Train Loss: 2.0480, Validation Loss: 2.3688\n",
      "Iteration 85200, Train Loss: 2.1055, Validation Loss: 2.3817\n",
      "Iteration 85300, Train Loss: 2.4248, Validation Loss: 2.7763\n",
      "Iteration 85400, Train Loss: 2.4541, Validation Loss: 2.5602\n",
      "Iteration 85500, Train Loss: 2.5146, Validation Loss: 2.4503\n",
      "Iteration 85600, Train Loss: 2.5345, Validation Loss: 2.1918\n",
      "Iteration 85700, Train Loss: 2.4887, Validation Loss: 2.7811\n",
      "Iteration 85800, Train Loss: 2.6638, Validation Loss: 2.8657\n",
      "Iteration 85900, Train Loss: 2.5350, Validation Loss: 2.2227\n",
      "Iteration 86000, Train Loss: 2.5653, Validation Loss: 2.6832\n",
      "Iteration 86100, Train Loss: 2.7794, Validation Loss: 2.4352\n",
      "Iteration 86200, Train Loss: 2.2874, Validation Loss: 2.3569\n",
      "Iteration 86300, Train Loss: 2.7520, Validation Loss: 2.3420\n",
      "Iteration 86400, Train Loss: 2.3771, Validation Loss: 2.2566\n",
      "Iteration 86500, Train Loss: 2.7494, Validation Loss: 2.4294\n",
      "Iteration 86600, Train Loss: 2.6479, Validation Loss: 2.1042\n",
      "Iteration 86700, Train Loss: 2.5689, Validation Loss: 2.5017\n",
      "Iteration 86800, Train Loss: 2.1105, Validation Loss: 2.2438\n",
      "Iteration 86900, Train Loss: 2.3206, Validation Loss: 2.3191\n",
      "Iteration 87000, Train Loss: 2.1347, Validation Loss: 2.2583\n",
      "Iteration 87100, Train Loss: 2.0714, Validation Loss: 1.9861\n",
      "Iteration 87200, Train Loss: 2.4166, Validation Loss: 2.1736\n",
      "Iteration 87300, Train Loss: 2.9003, Validation Loss: 2.3629\n",
      "Iteration 87400, Train Loss: 2.2144, Validation Loss: 2.6991\n",
      "Iteration 87500, Train Loss: 2.3253, Validation Loss: 2.7356\n",
      "Iteration 87600, Train Loss: 2.4497, Validation Loss: 2.2463\n",
      "Iteration 87700, Train Loss: 2.9403, Validation Loss: 2.3573\n",
      "Iteration 87800, Train Loss: 2.8120, Validation Loss: 2.4887\n",
      "Iteration 87900, Train Loss: 2.0692, Validation Loss: 2.4507\n",
      "Iteration 88000, Train Loss: 2.4335, Validation Loss: 2.2491\n",
      "Iteration 88100, Train Loss: 2.0503, Validation Loss: 2.8182\n",
      "Iteration 88200, Train Loss: 1.9471, Validation Loss: 2.2839\n",
      "Iteration 88300, Train Loss: 2.5157, Validation Loss: 1.9727\n",
      "Iteration 88400, Train Loss: 2.6486, Validation Loss: 2.5312\n",
      "Iteration 88500, Train Loss: 2.3966, Validation Loss: 2.4436\n",
      "Iteration 88600, Train Loss: 2.7055, Validation Loss: 2.1344\n",
      "Iteration 88700, Train Loss: 2.4352, Validation Loss: 2.2334\n",
      "Iteration 88800, Train Loss: 2.1185, Validation Loss: 2.5348\n",
      "Iteration 88900, Train Loss: 2.8145, Validation Loss: 2.0836\n",
      "Iteration 89000, Train Loss: 2.3429, Validation Loss: 2.5336\n",
      "Iteration 89100, Train Loss: 2.3494, Validation Loss: 2.4290\n",
      "Iteration 89200, Train Loss: 2.2784, Validation Loss: 2.2936\n",
      "Iteration 89300, Train Loss: 2.0242, Validation Loss: 2.1203\n",
      "Iteration 89400, Train Loss: 2.1332, Validation Loss: 2.8045\n",
      "Iteration 89500, Train Loss: 2.5049, Validation Loss: 2.6596\n",
      "Iteration 89600, Train Loss: 2.5846, Validation Loss: 2.5419\n",
      "Iteration 89700, Train Loss: 2.9158, Validation Loss: 2.1586\n",
      "Iteration 89800, Train Loss: 2.4618, Validation Loss: 2.6495\n",
      "Iteration 89900, Train Loss: 2.3678, Validation Loss: 2.2964\n",
      "Iteration 90000, Train Loss: 2.6805, Validation Loss: 2.0511\n",
      "Iteration 90100, Train Loss: 2.3783, Validation Loss: 2.6325\n",
      "Iteration 90200, Train Loss: 2.6332, Validation Loss: 1.9176\n",
      "Iteration 90300, Train Loss: 2.4475, Validation Loss: 2.6431\n",
      "Iteration 90400, Train Loss: 2.2385, Validation Loss: 2.0987\n",
      "Iteration 90500, Train Loss: 3.0480, Validation Loss: 2.3201\n",
      "Iteration 90600, Train Loss: 2.4345, Validation Loss: 2.0780\n",
      "Iteration 90700, Train Loss: 2.2725, Validation Loss: 2.7203\n",
      "Iteration 90800, Train Loss: 2.5126, Validation Loss: 2.4253\n",
      "Iteration 90900, Train Loss: 2.4931, Validation Loss: 2.6215\n",
      "Iteration 91000, Train Loss: 2.4998, Validation Loss: 2.5163\n",
      "Iteration 91100, Train Loss: 2.4889, Validation Loss: 2.0709\n",
      "Iteration 91200, Train Loss: 2.2630, Validation Loss: 2.1295\n",
      "Iteration 91300, Train Loss: 2.6276, Validation Loss: 2.3702\n",
      "Iteration 91400, Train Loss: 2.2016, Validation Loss: 2.2852\n",
      "Iteration 91500, Train Loss: 2.2167, Validation Loss: 2.4089\n",
      "Iteration 91600, Train Loss: 2.3246, Validation Loss: 2.1497\n",
      "Iteration 91700, Train Loss: 2.1284, Validation Loss: 2.4120\n",
      "Iteration 91800, Train Loss: 2.5691, Validation Loss: 2.3070\n",
      "Iteration 91900, Train Loss: 2.3016, Validation Loss: 2.3704\n",
      "Iteration 92000, Train Loss: 2.7868, Validation Loss: 2.4675\n",
      "Iteration 92100, Train Loss: 2.1588, Validation Loss: 2.0115\n",
      "Iteration 92200, Train Loss: 2.2619, Validation Loss: 2.1140\n",
      "Iteration 92300, Train Loss: 2.4179, Validation Loss: 2.4174\n",
      "Iteration 92400, Train Loss: 2.6263, Validation Loss: 2.2036\n",
      "Iteration 92500, Train Loss: 2.5702, Validation Loss: 2.3792\n",
      "Iteration 92600, Train Loss: 2.2064, Validation Loss: 2.4439\n",
      "Iteration 92700, Train Loss: 2.5294, Validation Loss: 2.5873\n",
      "Iteration 92800, Train Loss: 2.6799, Validation Loss: 2.8546\n",
      "Iteration 92900, Train Loss: 2.3577, Validation Loss: 2.4483\n",
      "Iteration 93000, Train Loss: 2.9418, Validation Loss: 2.6020\n",
      "Iteration 93100, Train Loss: 2.3960, Validation Loss: 2.4889\n",
      "Iteration 93200, Train Loss: 2.4499, Validation Loss: 2.8349\n",
      "Iteration 93300, Train Loss: 2.3496, Validation Loss: 2.2921\n",
      "Iteration 93400, Train Loss: 1.9725, Validation Loss: 2.5423\n",
      "Iteration 93500, Train Loss: 1.9477, Validation Loss: 2.3429\n",
      "Iteration 93600, Train Loss: 2.2226, Validation Loss: 2.0543\n",
      "Iteration 93700, Train Loss: 2.3393, Validation Loss: 2.2157\n",
      "Iteration 93800, Train Loss: 2.6449, Validation Loss: 2.4808\n",
      "Iteration 93900, Train Loss: 2.3420, Validation Loss: 2.5822\n",
      "Iteration 94000, Train Loss: 2.7464, Validation Loss: 2.3906\n",
      "Iteration 94100, Train Loss: 2.4576, Validation Loss: 2.6039\n",
      "Iteration 94200, Train Loss: 2.3732, Validation Loss: 2.6557\n",
      "Iteration 94300, Train Loss: 2.5309, Validation Loss: 2.2842\n",
      "Iteration 94400, Train Loss: 2.4364, Validation Loss: 2.8984\n",
      "Iteration 94500, Train Loss: 2.3541, Validation Loss: 2.4996\n",
      "Iteration 94600, Train Loss: 2.3395, Validation Loss: 2.8627\n",
      "Iteration 94700, Train Loss: 2.4613, Validation Loss: 1.9784\n",
      "Iteration 94800, Train Loss: 2.3506, Validation Loss: 2.2671\n",
      "Iteration 94900, Train Loss: 2.2635, Validation Loss: 2.4441\n",
      "Iteration 95000, Train Loss: 2.4388, Validation Loss: 2.2042\n",
      "Iteration 95100, Train Loss: 2.3979, Validation Loss: 2.5014\n",
      "Iteration 95200, Train Loss: 2.8250, Validation Loss: 2.2071\n",
      "Iteration 95300, Train Loss: 2.4850, Validation Loss: 2.6341\n",
      "Iteration 95400, Train Loss: 3.0710, Validation Loss: 2.3432\n",
      "Iteration 95500, Train Loss: 2.3826, Validation Loss: 2.3519\n",
      "Iteration 95600, Train Loss: 2.5393, Validation Loss: 2.2653\n",
      "Iteration 95700, Train Loss: 2.3528, Validation Loss: 2.5620\n",
      "Iteration 95800, Train Loss: 2.9668, Validation Loss: 2.2064\n",
      "Iteration 95900, Train Loss: 1.9786, Validation Loss: 2.4434\n",
      "Iteration 96000, Train Loss: 2.4798, Validation Loss: 2.6769\n",
      "Iteration 96100, Train Loss: 2.2999, Validation Loss: 2.5128\n",
      "Iteration 96200, Train Loss: 2.2641, Validation Loss: 2.3537\n",
      "Iteration 96300, Train Loss: 2.5837, Validation Loss: 2.3981\n",
      "Iteration 96400, Train Loss: 2.3367, Validation Loss: 2.9053\n",
      "Iteration 96500, Train Loss: 2.4673, Validation Loss: 2.4009\n",
      "Iteration 96600, Train Loss: 2.3195, Validation Loss: 2.3788\n",
      "Iteration 96700, Train Loss: 2.0547, Validation Loss: 2.1318\n",
      "Iteration 96800, Train Loss: 2.3857, Validation Loss: 2.0198\n",
      "Iteration 96900, Train Loss: 2.3380, Validation Loss: 2.4104\n",
      "Iteration 97000, Train Loss: 2.6249, Validation Loss: 2.7983\n",
      "Iteration 97100, Train Loss: 2.1187, Validation Loss: 2.5019\n",
      "Iteration 97200, Train Loss: 2.2942, Validation Loss: 2.6414\n",
      "Iteration 97300, Train Loss: 2.6830, Validation Loss: 2.4771\n",
      "Iteration 97400, Train Loss: 2.8978, Validation Loss: 2.5138\n",
      "Iteration 97500, Train Loss: 2.4442, Validation Loss: 2.4062\n",
      "Iteration 97600, Train Loss: 2.1546, Validation Loss: 2.3335\n",
      "Iteration 97700, Train Loss: 2.8052, Validation Loss: 2.5939\n",
      "Iteration 97800, Train Loss: 2.8934, Validation Loss: 2.0009\n",
      "Iteration 97900, Train Loss: 2.0419, Validation Loss: 2.7075\n",
      "Iteration 98000, Train Loss: 2.2870, Validation Loss: 2.2612\n",
      "Iteration 98100, Train Loss: 2.3461, Validation Loss: 2.4287\n",
      "Iteration 98200, Train Loss: 2.7659, Validation Loss: 2.5128\n",
      "Iteration 98300, Train Loss: 2.3084, Validation Loss: 2.8617\n",
      "Iteration 98400, Train Loss: 2.3614, Validation Loss: 1.9546\n",
      "Iteration 98500, Train Loss: 2.5200, Validation Loss: 2.5468\n",
      "Iteration 98600, Train Loss: 2.4624, Validation Loss: 2.7571\n",
      "Iteration 98700, Train Loss: 2.4693, Validation Loss: 2.8340\n",
      "Iteration 98800, Train Loss: 2.3877, Validation Loss: 2.4890\n",
      "Iteration 98900, Train Loss: 2.3613, Validation Loss: 2.5476\n",
      "Iteration 99000, Train Loss: 2.8423, Validation Loss: 2.5172\n",
      "Iteration 99100, Train Loss: 2.7218, Validation Loss: 2.3541\n",
      "Iteration 99200, Train Loss: 2.4163, Validation Loss: 2.6139\n",
      "Iteration 99300, Train Loss: 2.6309, Validation Loss: 2.6301\n",
      "Iteration 99400, Train Loss: 2.4391, Validation Loss: 2.1056\n",
      "Iteration 99500, Train Loss: 2.7651, Validation Loss: 2.5613\n",
      "Iteration 99600, Train Loss: 2.1548, Validation Loss: 2.4419\n",
      "Iteration 99700, Train Loss: 2.4554, Validation Loss: 2.0359\n",
      "Iteration 99800, Train Loss: 2.3501, Validation Loss: 2.6309\n",
      "Iteration 99900, Train Loss: 2.3380, Validation Loss: 2.2293\n",
      "Iteration 100000, Train Loss: 2.3566, Validation Loss: 2.4209\n",
      "Iteration 100100, Train Loss: 2.6579, Validation Loss: 2.2064\n",
      "Iteration 100200, Train Loss: 2.3960, Validation Loss: 2.7777\n",
      "Iteration 100300, Train Loss: 2.0755, Validation Loss: 2.0527\n",
      "Iteration 100400, Train Loss: 2.2793, Validation Loss: 2.2650\n",
      "Iteration 100500, Train Loss: 2.2097, Validation Loss: 2.3483\n",
      "Iteration 100600, Train Loss: 2.6416, Validation Loss: 2.5510\n",
      "Iteration 100700, Train Loss: 2.5045, Validation Loss: 2.2091\n",
      "Iteration 100800, Train Loss: 2.2652, Validation Loss: 2.7093\n",
      "Iteration 100900, Train Loss: 2.3548, Validation Loss: 2.9863\n",
      "Iteration 101000, Train Loss: 2.8529, Validation Loss: 2.8851\n",
      "Iteration 101100, Train Loss: 2.4057, Validation Loss: 2.3794\n",
      "Iteration 101200, Train Loss: 2.4908, Validation Loss: 2.4112\n",
      "Iteration 101300, Train Loss: 2.5026, Validation Loss: 2.5768\n",
      "Iteration 101400, Train Loss: 2.7639, Validation Loss: 2.0201\n",
      "Iteration 101500, Train Loss: 2.3143, Validation Loss: 1.8859\n",
      "Iteration 101600, Train Loss: 2.8878, Validation Loss: 2.7937\n",
      "Iteration 101700, Train Loss: 2.4536, Validation Loss: 2.5398\n",
      "Iteration 101800, Train Loss: 2.7096, Validation Loss: 2.1762\n",
      "Iteration 101900, Train Loss: 2.5514, Validation Loss: 2.5012\n",
      "Iteration 102000, Train Loss: 2.4829, Validation Loss: 2.4836\n",
      "Iteration 102100, Train Loss: 2.0792, Validation Loss: 2.6101\n",
      "Iteration 102200, Train Loss: 2.3295, Validation Loss: 2.1640\n",
      "Iteration 102300, Train Loss: 2.4454, Validation Loss: 2.2168\n",
      "Iteration 102400, Train Loss: 2.5532, Validation Loss: 2.3729\n",
      "Iteration 102500, Train Loss: 2.2652, Validation Loss: 2.5849\n",
      "Iteration 102600, Train Loss: 2.2146, Validation Loss: 1.8587\n",
      "Iteration 102700, Train Loss: 2.4577, Validation Loss: 2.9089\n",
      "Iteration 102800, Train Loss: 2.3803, Validation Loss: 2.2474\n",
      "Iteration 102900, Train Loss: 2.8745, Validation Loss: 2.7049\n",
      "Iteration 103000, Train Loss: 2.3921, Validation Loss: 2.4959\n",
      "Iteration 103100, Train Loss: 2.3737, Validation Loss: 2.0887\n",
      "Iteration 103200, Train Loss: 2.6427, Validation Loss: 2.3158\n",
      "Iteration 103300, Train Loss: 2.2266, Validation Loss: 1.9595\n",
      "Iteration 103400, Train Loss: 2.2930, Validation Loss: 2.0514\n",
      "Iteration 103500, Train Loss: 2.4752, Validation Loss: 2.4571\n",
      "Iteration 103600, Train Loss: 2.2013, Validation Loss: 2.4380\n",
      "Iteration 103700, Train Loss: 2.4282, Validation Loss: 2.5896\n",
      "Iteration 103800, Train Loss: 2.2161, Validation Loss: 2.5934\n",
      "Iteration 103900, Train Loss: 2.1944, Validation Loss: 2.2790\n",
      "Iteration 104000, Train Loss: 2.5822, Validation Loss: 2.4677\n",
      "Iteration 104100, Train Loss: 2.2879, Validation Loss: 2.6577\n",
      "Iteration 104200, Train Loss: 1.9660, Validation Loss: 2.4929\n",
      "Iteration 104300, Train Loss: 2.1327, Validation Loss: 2.4228\n",
      "Iteration 104400, Train Loss: 2.4557, Validation Loss: 2.3756\n",
      "Iteration 104500, Train Loss: 2.5007, Validation Loss: 2.2225\n",
      "Iteration 104600, Train Loss: 2.5067, Validation Loss: 2.4477\n",
      "Iteration 104700, Train Loss: 2.1480, Validation Loss: 2.4380\n",
      "Iteration 104800, Train Loss: 2.3233, Validation Loss: 2.1064\n",
      "Iteration 104900, Train Loss: 2.8978, Validation Loss: 2.4185\n",
      "Iteration 105000, Train Loss: 2.5386, Validation Loss: 2.4762\n",
      "Iteration 105100, Train Loss: 2.5345, Validation Loss: 2.3085\n",
      "Iteration 105200, Train Loss: 2.2370, Validation Loss: 2.2928\n",
      "Iteration 105300, Train Loss: 2.4621, Validation Loss: 2.8998\n",
      "Iteration 105400, Train Loss: 1.7678, Validation Loss: 2.2122\n",
      "Iteration 105500, Train Loss: 2.5875, Validation Loss: 2.6441\n",
      "Iteration 105600, Train Loss: 2.5529, Validation Loss: 2.7919\n",
      "Iteration 105700, Train Loss: 2.3109, Validation Loss: 2.7936\n",
      "Iteration 105800, Train Loss: 2.3128, Validation Loss: 2.2251\n",
      "Iteration 105900, Train Loss: 2.5080, Validation Loss: 2.4628\n",
      "Iteration 106000, Train Loss: 2.2165, Validation Loss: 2.2810\n",
      "Iteration 106100, Train Loss: 2.3576, Validation Loss: 2.4885\n",
      "Iteration 106200, Train Loss: 2.4994, Validation Loss: 2.5714\n",
      "Iteration 106300, Train Loss: 2.7084, Validation Loss: 1.9913\n",
      "Iteration 106400, Train Loss: 2.3299, Validation Loss: 2.1973\n",
      "Iteration 106500, Train Loss: 2.4695, Validation Loss: 2.6238\n",
      "Iteration 106600, Train Loss: 2.4006, Validation Loss: 2.0692\n",
      "Iteration 106700, Train Loss: 2.1966, Validation Loss: 2.7915\n",
      "Iteration 106800, Train Loss: 2.5337, Validation Loss: 2.0511\n",
      "Iteration 106900, Train Loss: 3.0179, Validation Loss: 2.0972\n",
      "Iteration 107000, Train Loss: 2.6058, Validation Loss: 2.2190\n",
      "Iteration 107100, Train Loss: 2.8693, Validation Loss: 2.4522\n",
      "Iteration 107200, Train Loss: 2.1042, Validation Loss: 2.3815\n",
      "Iteration 107300, Train Loss: 2.0758, Validation Loss: 2.7264\n",
      "Iteration 107400, Train Loss: 2.3581, Validation Loss: 2.8854\n",
      "Iteration 107500, Train Loss: 2.5426, Validation Loss: 2.1606\n",
      "Iteration 107600, Train Loss: 2.1188, Validation Loss: 2.4703\n",
      "Iteration 107700, Train Loss: 2.2946, Validation Loss: 2.4394\n",
      "Iteration 107800, Train Loss: 2.1866, Validation Loss: 2.6065\n",
      "Iteration 107900, Train Loss: 2.5066, Validation Loss: 2.1804\n",
      "Iteration 108000, Train Loss: 2.4893, Validation Loss: 3.0432\n",
      "Iteration 108100, Train Loss: 2.2383, Validation Loss: 2.2854\n",
      "Iteration 108200, Train Loss: 2.1954, Validation Loss: 1.9480\n",
      "Iteration 108300, Train Loss: 2.7585, Validation Loss: 2.0357\n",
      "Iteration 108400, Train Loss: 2.4343, Validation Loss: 2.4037\n",
      "Iteration 108500, Train Loss: 2.4404, Validation Loss: 2.6757\n",
      "Iteration 108600, Train Loss: 2.3146, Validation Loss: 2.2463\n",
      "Iteration 108700, Train Loss: 2.6398, Validation Loss: 1.9869\n",
      "Iteration 108800, Train Loss: 2.2552, Validation Loss: 2.5878\n",
      "Iteration 108900, Train Loss: 2.8412, Validation Loss: 2.3211\n",
      "Iteration 109000, Train Loss: 2.1429, Validation Loss: 2.3326\n",
      "Iteration 109100, Train Loss: 2.0558, Validation Loss: 2.2521\n",
      "Iteration 109200, Train Loss: 2.5100, Validation Loss: 2.3917\n",
      "Iteration 109300, Train Loss: 2.5945, Validation Loss: 2.3104\n",
      "Iteration 109400, Train Loss: 2.7339, Validation Loss: 2.0571\n",
      "Iteration 109500, Train Loss: 2.1664, Validation Loss: 2.3660\n",
      "Iteration 109600, Train Loss: 2.4989, Validation Loss: 2.3843\n",
      "Iteration 109700, Train Loss: 2.5421, Validation Loss: 2.4631\n",
      "Iteration 109800, Train Loss: 2.0912, Validation Loss: 2.3035\n",
      "Iteration 109900, Train Loss: 2.3584, Validation Loss: 1.8741\n",
      "Iteration 110000, Train Loss: 2.3381, Validation Loss: 2.6917\n",
      "Iteration 110100, Train Loss: 2.6673, Validation Loss: 2.1509\n",
      "Iteration 110200, Train Loss: 2.4359, Validation Loss: 2.3852\n",
      "Iteration 110300, Train Loss: 2.9764, Validation Loss: 2.4429\n",
      "Iteration 110400, Train Loss: 2.7203, Validation Loss: 2.3246\n",
      "Iteration 110500, Train Loss: 2.1925, Validation Loss: 2.2966\n",
      "Iteration 110600, Train Loss: 2.3509, Validation Loss: 2.5527\n",
      "Iteration 110700, Train Loss: 2.6038, Validation Loss: 2.3543\n",
      "Iteration 110800, Train Loss: 2.1711, Validation Loss: 3.1165\n",
      "Iteration 110900, Train Loss: 2.1963, Validation Loss: 1.9577\n",
      "Iteration 111000, Train Loss: 2.4634, Validation Loss: 2.2656\n",
      "Iteration 111100, Train Loss: 1.9805, Validation Loss: 2.5748\n",
      "Iteration 111200, Train Loss: 2.2562, Validation Loss: 2.3083\n",
      "Iteration 111300, Train Loss: 2.6599, Validation Loss: 2.7746\n",
      "Iteration 111400, Train Loss: 2.3538, Validation Loss: 2.2447\n",
      "Iteration 111500, Train Loss: 2.7220, Validation Loss: 2.5695\n",
      "Iteration 111600, Train Loss: 1.9828, Validation Loss: 2.4290\n",
      "Iteration 111700, Train Loss: 2.2150, Validation Loss: 2.3813\n",
      "Iteration 111800, Train Loss: 2.5436, Validation Loss: 2.2024\n",
      "Iteration 111900, Train Loss: 2.1181, Validation Loss: 2.3424\n",
      "Iteration 112000, Train Loss: 2.4561, Validation Loss: 2.6844\n",
      "Iteration 112100, Train Loss: 2.3984, Validation Loss: 2.4438\n",
      "Iteration 112200, Train Loss: 2.4641, Validation Loss: 2.5265\n",
      "Iteration 112300, Train Loss: 2.1852, Validation Loss: 2.3177\n",
      "Iteration 112400, Train Loss: 2.2297, Validation Loss: 2.3579\n",
      "Iteration 112500, Train Loss: 2.3736, Validation Loss: 2.1937\n",
      "Iteration 112600, Train Loss: 2.6217, Validation Loss: 2.1513\n",
      "Iteration 112700, Train Loss: 2.3987, Validation Loss: 2.4793\n",
      "Iteration 112800, Train Loss: 2.6395, Validation Loss: 2.5829\n",
      "Iteration 112900, Train Loss: 2.3233, Validation Loss: 2.7687\n",
      "Iteration 113000, Train Loss: 2.6498, Validation Loss: 2.1780\n",
      "Iteration 113100, Train Loss: 2.6204, Validation Loss: 2.5839\n",
      "Iteration 113200, Train Loss: 2.2696, Validation Loss: 2.3374\n",
      "Iteration 113300, Train Loss: 2.4793, Validation Loss: 2.4304\n",
      "Iteration 113400, Train Loss: 2.0396, Validation Loss: 2.2467\n",
      "Iteration 113500, Train Loss: 2.5780, Validation Loss: 2.4471\n",
      "Iteration 113600, Train Loss: 2.5283, Validation Loss: 2.2851\n",
      "Iteration 113700, Train Loss: 2.5757, Validation Loss: 2.4086\n",
      "Iteration 113800, Train Loss: 2.3988, Validation Loss: 2.8678\n",
      "Iteration 113900, Train Loss: 2.2998, Validation Loss: 2.0397\n",
      "Iteration 114000, Train Loss: 2.2500, Validation Loss: 2.4297\n",
      "Iteration 114100, Train Loss: 2.2902, Validation Loss: 2.7024\n",
      "Iteration 114200, Train Loss: 2.3230, Validation Loss: 2.4990\n",
      "Iteration 114300, Train Loss: 2.4071, Validation Loss: 2.3430\n",
      "Iteration 114400, Train Loss: 2.2439, Validation Loss: 2.0190\n",
      "Iteration 114500, Train Loss: 3.0038, Validation Loss: 2.6150\n",
      "Iteration 114600, Train Loss: 2.5409, Validation Loss: 2.1889\n",
      "Iteration 114700, Train Loss: 2.3253, Validation Loss: 2.6167\n",
      "Iteration 114800, Train Loss: 2.5060, Validation Loss: 2.6067\n",
      "Iteration 114900, Train Loss: 2.4454, Validation Loss: 2.2332\n",
      "Iteration 115000, Train Loss: 2.4884, Validation Loss: 2.6816\n",
      "Iteration 115100, Train Loss: 2.3852, Validation Loss: 2.2959\n",
      "Iteration 115200, Train Loss: 2.1179, Validation Loss: 2.2603\n",
      "Iteration 115300, Train Loss: 2.4112, Validation Loss: 2.4734\n",
      "Iteration 115400, Train Loss: 2.1652, Validation Loss: 2.3866\n",
      "Iteration 115500, Train Loss: 2.6588, Validation Loss: 2.7552\n",
      "Iteration 115600, Train Loss: 2.9984, Validation Loss: 2.6582\n",
      "Iteration 115700, Train Loss: 2.7137, Validation Loss: 2.4217\n",
      "Iteration 115800, Train Loss: 1.9265, Validation Loss: 2.2302\n",
      "Iteration 115900, Train Loss: 2.0093, Validation Loss: 2.1601\n",
      "Iteration 116000, Train Loss: 2.6188, Validation Loss: 2.5779\n",
      "Iteration 116100, Train Loss: 2.3772, Validation Loss: 2.9439\n",
      "Iteration 116200, Train Loss: 2.2306, Validation Loss: 2.6405\n",
      "Iteration 116300, Train Loss: 2.1640, Validation Loss: 2.2515\n",
      "Iteration 116400, Train Loss: 2.2680, Validation Loss: 2.1385\n",
      "Iteration 116500, Train Loss: 2.0245, Validation Loss: 1.9624\n",
      "Iteration 116600, Train Loss: 2.9200, Validation Loss: 2.2160\n",
      "Iteration 116700, Train Loss: 2.8046, Validation Loss: 2.3061\n",
      "Iteration 116800, Train Loss: 2.6328, Validation Loss: 2.3027\n",
      "Iteration 116900, Train Loss: 2.8453, Validation Loss: 2.4831\n",
      "Iteration 117000, Train Loss: 2.3122, Validation Loss: 2.5451\n",
      "Iteration 117100, Train Loss: 2.6470, Validation Loss: 2.3021\n",
      "Iteration 117200, Train Loss: 2.0550, Validation Loss: 2.4059\n",
      "Iteration 117300, Train Loss: 2.4814, Validation Loss: 2.7237\n",
      "Iteration 117400, Train Loss: 2.6175, Validation Loss: 2.7595\n",
      "Iteration 117500, Train Loss: 2.5588, Validation Loss: 2.2424\n",
      "Iteration 117600, Train Loss: 2.5248, Validation Loss: 2.4868\n",
      "Iteration 117700, Train Loss: 2.2351, Validation Loss: 2.6074\n",
      "Iteration 117800, Train Loss: 1.7135, Validation Loss: 2.7080\n",
      "Iteration 117900, Train Loss: 2.4300, Validation Loss: 2.1948\n",
      "Iteration 118000, Train Loss: 2.3788, Validation Loss: 2.4420\n",
      "Iteration 118100, Train Loss: 2.4474, Validation Loss: 3.1730\n",
      "Iteration 118200, Train Loss: 2.3135, Validation Loss: 2.4353\n",
      "Iteration 118300, Train Loss: 2.6453, Validation Loss: 2.4535\n",
      "Iteration 118400, Train Loss: 2.2558, Validation Loss: 2.5914\n",
      "Iteration 118500, Train Loss: 2.2515, Validation Loss: 2.3042\n",
      "Iteration 118600, Train Loss: 2.3201, Validation Loss: 2.6988\n",
      "Iteration 118700, Train Loss: 2.3097, Validation Loss: 2.5173\n",
      "Iteration 118800, Train Loss: 2.2031, Validation Loss: 2.7523\n",
      "Iteration 118900, Train Loss: 2.3708, Validation Loss: 2.3813\n",
      "Iteration 119000, Train Loss: 2.4703, Validation Loss: 2.2774\n",
      "Iteration 119100, Train Loss: 2.3092, Validation Loss: 2.3933\n",
      "Iteration 119200, Train Loss: 2.7670, Validation Loss: 2.0071\n",
      "Iteration 119300, Train Loss: 2.0812, Validation Loss: 2.6163\n",
      "Iteration 119400, Train Loss: 2.0636, Validation Loss: 2.5292\n",
      "Iteration 119500, Train Loss: 2.5805, Validation Loss: 2.0978\n",
      "Iteration 119600, Train Loss: 2.6317, Validation Loss: 2.1907\n",
      "Iteration 119700, Train Loss: 2.2489, Validation Loss: 2.2640\n",
      "Iteration 119800, Train Loss: 2.5260, Validation Loss: 2.2550\n",
      "Iteration 119900, Train Loss: 2.2942, Validation Loss: 2.3225\n",
      "Iteration 120000, Train Loss: 2.0358, Validation Loss: 2.2797\n",
      "Iteration 120100, Train Loss: 2.2803, Validation Loss: 2.0947\n",
      "Iteration 120200, Train Loss: 2.3808, Validation Loss: 2.7043\n",
      "Iteration 120300, Train Loss: 2.1025, Validation Loss: 2.2059\n",
      "Iteration 120400, Train Loss: 2.7171, Validation Loss: 2.1227\n",
      "Iteration 120500, Train Loss: 2.4064, Validation Loss: 2.1147\n",
      "Iteration 120600, Train Loss: 2.3862, Validation Loss: 2.5377\n",
      "Iteration 120700, Train Loss: 2.1882, Validation Loss: 2.6199\n",
      "Iteration 120800, Train Loss: 2.4184, Validation Loss: 2.3131\n",
      "Iteration 120900, Train Loss: 2.3073, Validation Loss: 2.2360\n",
      "Iteration 121000, Train Loss: 2.8959, Validation Loss: 2.6107\n",
      "Iteration 121100, Train Loss: 2.4140, Validation Loss: 2.8492\n",
      "Iteration 121200, Train Loss: 2.3484, Validation Loss: 2.2623\n",
      "Iteration 121300, Train Loss: 2.6723, Validation Loss: 2.0128\n",
      "Iteration 121400, Train Loss: 2.2863, Validation Loss: 2.8716\n",
      "Iteration 121500, Train Loss: 2.4364, Validation Loss: 2.6723\n",
      "Iteration 121600, Train Loss: 2.1349, Validation Loss: 2.2829\n",
      "Iteration 121700, Train Loss: 2.4182, Validation Loss: 2.3123\n",
      "Iteration 121800, Train Loss: 2.4474, Validation Loss: 1.8718\n",
      "Iteration 121900, Train Loss: 2.3442, Validation Loss: 2.2634\n",
      "Iteration 122000, Train Loss: 2.9341, Validation Loss: 2.4803\n",
      "Iteration 122100, Train Loss: 2.9754, Validation Loss: 2.6147\n",
      "Iteration 122200, Train Loss: 2.2952, Validation Loss: 2.4881\n",
      "Iteration 122300, Train Loss: 2.8129, Validation Loss: 2.3104\n",
      "Iteration 122400, Train Loss: 2.4686, Validation Loss: 2.2888\n",
      "Iteration 122500, Train Loss: 2.4258, Validation Loss: 2.7995\n",
      "Iteration 122600, Train Loss: 2.6244, Validation Loss: 2.2016\n",
      "Iteration 122700, Train Loss: 2.6388, Validation Loss: 2.7747\n",
      "Iteration 122800, Train Loss: 2.2119, Validation Loss: 2.2979\n",
      "Iteration 122900, Train Loss: 2.2854, Validation Loss: 2.3674\n",
      "Iteration 123000, Train Loss: 2.2347, Validation Loss: 2.4332\n",
      "Iteration 123100, Train Loss: 2.3659, Validation Loss: 2.5172\n",
      "Iteration 123200, Train Loss: 2.2762, Validation Loss: 2.1718\n",
      "Iteration 123300, Train Loss: 2.8646, Validation Loss: 2.0561\n",
      "Iteration 123400, Train Loss: 2.5668, Validation Loss: 2.5852\n",
      "Iteration 123500, Train Loss: 2.2540, Validation Loss: 2.2592\n",
      "Iteration 123600, Train Loss: 1.9020, Validation Loss: 2.3688\n",
      "Iteration 123700, Train Loss: 2.5190, Validation Loss: 2.1427\n",
      "Iteration 123800, Train Loss: 2.7931, Validation Loss: 2.0698\n",
      "Iteration 123900, Train Loss: 2.1070, Validation Loss: 2.3972\n",
      "Iteration 124000, Train Loss: 2.5535, Validation Loss: 2.2253\n",
      "Iteration 124100, Train Loss: 2.2083, Validation Loss: 2.8911\n",
      "Iteration 124200, Train Loss: 2.4336, Validation Loss: 2.1856\n",
      "Iteration 124300, Train Loss: 2.2037, Validation Loss: 2.4127\n",
      "Iteration 124400, Train Loss: 2.4206, Validation Loss: 2.6376\n",
      "Iteration 124500, Train Loss: 2.3690, Validation Loss: 2.1228\n",
      "Iteration 124600, Train Loss: 2.4927, Validation Loss: 2.3194\n",
      "Iteration 124700, Train Loss: 3.0060, Validation Loss: 2.3196\n",
      "Iteration 124800, Train Loss: 2.3994, Validation Loss: 2.6733\n",
      "Iteration 124900, Train Loss: 2.5593, Validation Loss: 2.7955\n",
      "Iteration 125000, Train Loss: 2.4858, Validation Loss: 2.5276\n",
      "Iteration 125100, Train Loss: 2.3356, Validation Loss: 3.1358\n",
      "Iteration 125200, Train Loss: 3.0557, Validation Loss: 2.4378\n",
      "Iteration 125300, Train Loss: 2.8454, Validation Loss: 2.4568\n",
      "Iteration 125400, Train Loss: 2.4709, Validation Loss: 2.7103\n",
      "Iteration 125500, Train Loss: 2.5887, Validation Loss: 2.0895\n",
      "Iteration 125600, Train Loss: 2.2059, Validation Loss: 2.4008\n",
      "Iteration 125700, Train Loss: 2.1634, Validation Loss: 2.6950\n",
      "Iteration 125800, Train Loss: 2.5621, Validation Loss: 2.3634\n",
      "Iteration 125900, Train Loss: 2.5916, Validation Loss: 2.7667\n",
      "Iteration 126000, Train Loss: 2.4008, Validation Loss: 2.1566\n",
      "Iteration 126100, Train Loss: 2.1630, Validation Loss: 2.3750\n",
      "Iteration 126200, Train Loss: 2.1075, Validation Loss: 2.0238\n",
      "Iteration 126300, Train Loss: 2.7129, Validation Loss: 2.1514\n",
      "Iteration 126400, Train Loss: 2.1290, Validation Loss: 2.6496\n",
      "Iteration 126500, Train Loss: 2.3974, Validation Loss: 2.9871\n",
      "Iteration 126600, Train Loss: 2.5894, Validation Loss: 1.7842\n",
      "Iteration 126700, Train Loss: 2.3469, Validation Loss: 2.7580\n",
      "Iteration 126800, Train Loss: 2.6104, Validation Loss: 1.9950\n",
      "Iteration 126900, Train Loss: 2.1668, Validation Loss: 2.5357\n",
      "Iteration 127000, Train Loss: 2.5144, Validation Loss: 2.5403\n",
      "Iteration 127100, Train Loss: 2.4735, Validation Loss: 2.1843\n",
      "Iteration 127200, Train Loss: 2.2426, Validation Loss: 2.4859\n",
      "Iteration 127300, Train Loss: 2.2348, Validation Loss: 2.2689\n",
      "Iteration 127400, Train Loss: 2.4547, Validation Loss: 2.5304\n",
      "Iteration 127500, Train Loss: 2.3184, Validation Loss: 2.3103\n",
      "Iteration 127600, Train Loss: 2.4055, Validation Loss: 2.4508\n",
      "Iteration 127700, Train Loss: 2.3127, Validation Loss: 2.1574\n",
      "Iteration 127800, Train Loss: 2.5606, Validation Loss: 2.6358\n",
      "Iteration 127900, Train Loss: 2.5956, Validation Loss: 2.4020\n",
      "Iteration 128000, Train Loss: 2.1033, Validation Loss: 2.5073\n",
      "Iteration 128100, Train Loss: 2.5663, Validation Loss: 2.3346\n",
      "Iteration 128200, Train Loss: 2.2788, Validation Loss: 2.5777\n",
      "Iteration 128300, Train Loss: 2.0804, Validation Loss: 2.4345\n",
      "Iteration 128400, Train Loss: 2.6136, Validation Loss: 2.7281\n",
      "Iteration 128500, Train Loss: 2.0850, Validation Loss: 1.8392\n",
      "Iteration 128600, Train Loss: 2.6922, Validation Loss: 2.5831\n",
      "Iteration 128700, Train Loss: 2.0810, Validation Loss: 2.8310\n",
      "Iteration 128800, Train Loss: 2.6159, Validation Loss: 2.2555\n",
      "Iteration 128900, Train Loss: 2.5798, Validation Loss: 2.7093\n",
      "Iteration 129000, Train Loss: 2.5572, Validation Loss: 2.1883\n",
      "Iteration 129100, Train Loss: 2.4981, Validation Loss: 2.0979\n",
      "Iteration 129200, Train Loss: 2.0807, Validation Loss: 2.3890\n",
      "Iteration 129300, Train Loss: 2.5867, Validation Loss: 2.3487\n",
      "Iteration 129400, Train Loss: 2.2491, Validation Loss: 2.4374\n",
      "Iteration 129500, Train Loss: 2.6000, Validation Loss: 2.5890\n",
      "Iteration 129600, Train Loss: 2.0780, Validation Loss: 2.1901\n",
      "Iteration 129700, Train Loss: 2.3289, Validation Loss: 2.2612\n",
      "Iteration 129800, Train Loss: 2.2076, Validation Loss: 2.2274\n",
      "Iteration 129900, Train Loss: 2.0479, Validation Loss: 2.4695\n",
      "Iteration 130000, Train Loss: 2.4523, Validation Loss: 2.1512\n",
      "Iteration 130100, Train Loss: 2.1084, Validation Loss: 2.0963\n",
      "Iteration 130200, Train Loss: 2.0122, Validation Loss: 2.4733\n",
      "Iteration 130300, Train Loss: 2.4128, Validation Loss: 2.5397\n",
      "Iteration 130400, Train Loss: 2.6171, Validation Loss: 2.4344\n",
      "Iteration 130500, Train Loss: 2.2408, Validation Loss: 2.3681\n",
      "Iteration 130600, Train Loss: 2.0885, Validation Loss: 2.3660\n",
      "Iteration 130700, Train Loss: 2.3353, Validation Loss: 2.3500\n",
      "Iteration 130800, Train Loss: 2.3217, Validation Loss: 2.4527\n",
      "Iteration 130900, Train Loss: 2.5239, Validation Loss: 2.2025\n",
      "Iteration 131000, Train Loss: 2.9756, Validation Loss: 2.5359\n",
      "Iteration 131100, Train Loss: 2.2538, Validation Loss: 2.6440\n",
      "Iteration 131200, Train Loss: 2.1571, Validation Loss: 2.6018\n",
      "Iteration 131300, Train Loss: 2.5502, Validation Loss: 1.9457\n",
      "Iteration 131400, Train Loss: 2.5509, Validation Loss: 2.3814\n",
      "Iteration 131500, Train Loss: 2.8678, Validation Loss: 2.3647\n",
      "Iteration 131600, Train Loss: 2.1996, Validation Loss: 2.0584\n",
      "Iteration 131700, Train Loss: 2.3980, Validation Loss: 1.9553\n",
      "Iteration 131800, Train Loss: 2.3349, Validation Loss: 2.4849\n",
      "Iteration 131900, Train Loss: 2.2491, Validation Loss: 2.3736\n",
      "Iteration 132000, Train Loss: 2.0805, Validation Loss: 2.1566\n",
      "Iteration 132100, Train Loss: 2.7208, Validation Loss: 2.6638\n",
      "Iteration 132200, Train Loss: 2.4208, Validation Loss: 2.2168\n",
      "Iteration 132300, Train Loss: 2.5228, Validation Loss: 2.7716\n",
      "Iteration 132400, Train Loss: 2.2895, Validation Loss: 2.3214\n",
      "Iteration 132500, Train Loss: 2.6499, Validation Loss: 2.9933\n",
      "Iteration 132600, Train Loss: 2.5400, Validation Loss: 2.5325\n",
      "Iteration 132700, Train Loss: 2.3271, Validation Loss: 2.6522\n",
      "Iteration 132800, Train Loss: 2.4692, Validation Loss: 2.2096\n",
      "Iteration 132900, Train Loss: 2.1941, Validation Loss: 2.7691\n",
      "Iteration 133000, Train Loss: 2.1350, Validation Loss: 1.9700\n",
      "Iteration 133100, Train Loss: 2.6854, Validation Loss: 2.1283\n",
      "Iteration 133200, Train Loss: 2.6504, Validation Loss: 2.4931\n",
      "Iteration 133300, Train Loss: 2.0997, Validation Loss: 2.6694\n",
      "Iteration 133400, Train Loss: 2.4978, Validation Loss: 2.4253\n",
      "Iteration 133500, Train Loss: 1.9960, Validation Loss: 2.4482\n",
      "Iteration 133600, Train Loss: 2.3036, Validation Loss: 2.1506\n",
      "Iteration 133700, Train Loss: 2.4107, Validation Loss: 2.1208\n",
      "Iteration 133800, Train Loss: 2.3148, Validation Loss: 2.4139\n",
      "Iteration 133900, Train Loss: 2.4073, Validation Loss: 2.1681\n",
      "Iteration 134000, Train Loss: 2.2840, Validation Loss: 2.4872\n",
      "Iteration 134100, Train Loss: 1.9785, Validation Loss: 2.3086\n",
      "Iteration 134200, Train Loss: 2.2480, Validation Loss: 2.3468\n",
      "Iteration 134300, Train Loss: 2.4752, Validation Loss: 2.3008\n",
      "Iteration 134400, Train Loss: 2.4076, Validation Loss: 3.3815\n",
      "Iteration 134500, Train Loss: 2.3181, Validation Loss: 2.2708\n",
      "Iteration 134600, Train Loss: 2.2792, Validation Loss: 2.3920\n",
      "Iteration 134700, Train Loss: 2.0820, Validation Loss: 2.6087\n",
      "Iteration 134800, Train Loss: 2.3252, Validation Loss: 2.2830\n",
      "Iteration 134900, Train Loss: 2.3951, Validation Loss: 2.3632\n",
      "Iteration 135000, Train Loss: 2.4176, Validation Loss: 2.1842\n",
      "Iteration 135100, Train Loss: 2.2633, Validation Loss: 2.0667\n",
      "Iteration 135200, Train Loss: 1.9817, Validation Loss: 2.4740\n",
      "Iteration 135300, Train Loss: 2.7555, Validation Loss: 2.4930\n",
      "Iteration 135400, Train Loss: 2.1305, Validation Loss: 2.5241\n",
      "Iteration 135500, Train Loss: 2.8275, Validation Loss: 2.3654\n",
      "Iteration 135600, Train Loss: 2.2827, Validation Loss: 2.5125\n",
      "Iteration 135700, Train Loss: 2.1007, Validation Loss: 2.3584\n",
      "Iteration 135800, Train Loss: 2.3825, Validation Loss: 2.5473\n",
      "Iteration 135900, Train Loss: 2.6238, Validation Loss: 2.2953\n",
      "Iteration 136000, Train Loss: 2.5071, Validation Loss: 2.1367\n",
      "Iteration 136100, Train Loss: 2.6738, Validation Loss: 2.6161\n",
      "Iteration 136200, Train Loss: 2.7436, Validation Loss: 2.4793\n",
      "Iteration 136300, Train Loss: 2.3638, Validation Loss: 2.6371\n",
      "Iteration 136400, Train Loss: 1.9549, Validation Loss: 2.3049\n",
      "Iteration 136500, Train Loss: 2.2298, Validation Loss: 2.3121\n",
      "Iteration 136600, Train Loss: 2.4034, Validation Loss: 2.4807\n",
      "Iteration 136700, Train Loss: 2.2423, Validation Loss: 2.1453\n",
      "Iteration 136800, Train Loss: 2.3210, Validation Loss: 2.9181\n",
      "Iteration 136900, Train Loss: 2.4003, Validation Loss: 1.9368\n",
      "Iteration 137000, Train Loss: 2.3678, Validation Loss: 2.3125\n",
      "Iteration 137100, Train Loss: 2.5851, Validation Loss: 2.3321\n",
      "Iteration 137200, Train Loss: 2.5588, Validation Loss: 2.4692\n",
      "Iteration 137300, Train Loss: 2.1799, Validation Loss: 2.5046\n",
      "Iteration 137400, Train Loss: 2.2623, Validation Loss: 2.6566\n",
      "Iteration 137500, Train Loss: 2.2160, Validation Loss: 2.9828\n",
      "Iteration 137600, Train Loss: 2.1304, Validation Loss: 2.1851\n",
      "Iteration 137700, Train Loss: 1.8864, Validation Loss: 2.1873\n",
      "Iteration 137800, Train Loss: 2.3256, Validation Loss: 2.2351\n",
      "Iteration 137900, Train Loss: 2.6511, Validation Loss: 2.7855\n",
      "Iteration 138000, Train Loss: 2.7362, Validation Loss: 2.3222\n",
      "Iteration 138100, Train Loss: 2.1953, Validation Loss: 2.6224\n",
      "Iteration 138200, Train Loss: 2.5802, Validation Loss: 2.3110\n",
      "Iteration 138300, Train Loss: 2.3134, Validation Loss: 2.4278\n",
      "Iteration 138400, Train Loss: 2.1668, Validation Loss: 2.7723\n",
      "Iteration 138500, Train Loss: 2.7086, Validation Loss: 2.3314\n",
      "Iteration 138600, Train Loss: 2.3835, Validation Loss: 2.2851\n",
      "Iteration 138700, Train Loss: 2.3241, Validation Loss: 2.3248\n",
      "Iteration 138800, Train Loss: 2.2857, Validation Loss: 2.8616\n",
      "Iteration 138900, Train Loss: 2.0006, Validation Loss: 2.3378\n",
      "Iteration 139000, Train Loss: 2.2491, Validation Loss: 2.3841\n",
      "Iteration 139100, Train Loss: 2.6733, Validation Loss: 2.5775\n",
      "Iteration 139200, Train Loss: 2.4678, Validation Loss: 2.0615\n",
      "Iteration 139300, Train Loss: 2.4352, Validation Loss: 3.1009\n",
      "Iteration 139400, Train Loss: 2.1676, Validation Loss: 2.9585\n",
      "Iteration 139500, Train Loss: 2.1297, Validation Loss: 1.8670\n",
      "Iteration 139600, Train Loss: 2.2416, Validation Loss: 2.6839\n",
      "Iteration 139700, Train Loss: 2.7857, Validation Loss: 2.7696\n",
      "Iteration 139800, Train Loss: 2.4487, Validation Loss: 2.2535\n",
      "Iteration 139900, Train Loss: 2.1482, Validation Loss: 1.8684\n",
      "Iteration 140000, Train Loss: 2.9548, Validation Loss: 2.2941\n",
      "Iteration 140100, Train Loss: 2.9605, Validation Loss: 2.3798\n",
      "Iteration 140200, Train Loss: 2.0665, Validation Loss: 2.2820\n",
      "Iteration 140300, Train Loss: 2.2716, Validation Loss: 2.4586\n",
      "Iteration 140400, Train Loss: 2.7478, Validation Loss: 2.2516\n",
      "Iteration 140500, Train Loss: 2.0084, Validation Loss: 2.5160\n",
      "Iteration 140600, Train Loss: 1.9484, Validation Loss: 2.5429\n",
      "Iteration 140700, Train Loss: 2.6229, Validation Loss: 2.2678\n",
      "Iteration 140800, Train Loss: 2.4011, Validation Loss: 2.2906\n",
      "Iteration 140900, Train Loss: 2.2259, Validation Loss: 2.1965\n",
      "Iteration 141000, Train Loss: 2.1397, Validation Loss: 2.7630\n",
      "Iteration 141100, Train Loss: 2.2413, Validation Loss: 2.5447\n",
      "Iteration 141200, Train Loss: 2.2311, Validation Loss: 2.0420\n",
      "Iteration 141300, Train Loss: 2.3814, Validation Loss: 2.5701\n",
      "Iteration 141400, Train Loss: 2.3063, Validation Loss: 2.4812\n",
      "Iteration 141500, Train Loss: 2.4604, Validation Loss: 2.0164\n",
      "Iteration 141600, Train Loss: 2.4394, Validation Loss: 2.5884\n",
      "Iteration 141700, Train Loss: 1.9797, Validation Loss: 2.4412\n",
      "Iteration 141800, Train Loss: 2.5138, Validation Loss: 2.3812\n",
      "Iteration 141900, Train Loss: 2.0053, Validation Loss: 2.5062\n",
      "Iteration 142000, Train Loss: 2.3131, Validation Loss: 2.4382\n",
      "Iteration 142100, Train Loss: 2.6087, Validation Loss: 2.4367\n",
      "Iteration 142200, Train Loss: 2.3471, Validation Loss: 2.5911\n",
      "Iteration 142300, Train Loss: 2.2899, Validation Loss: 2.5132\n",
      "Iteration 142400, Train Loss: 2.4886, Validation Loss: 2.3427\n",
      "Iteration 142500, Train Loss: 2.3854, Validation Loss: 2.1488\n",
      "Iteration 142600, Train Loss: 2.0735, Validation Loss: 2.2869\n",
      "Iteration 142700, Train Loss: 1.9981, Validation Loss: 2.4368\n",
      "Iteration 142800, Train Loss: 2.3549, Validation Loss: 2.2721\n",
      "Iteration 142900, Train Loss: 2.3956, Validation Loss: 2.1735\n",
      "Iteration 143000, Train Loss: 2.5651, Validation Loss: 2.2473\n",
      "Iteration 143100, Train Loss: 1.8454, Validation Loss: 2.6173\n",
      "Iteration 143200, Train Loss: 2.3852, Validation Loss: 2.2377\n",
      "Iteration 143300, Train Loss: 2.3887, Validation Loss: 2.5013\n",
      "Iteration 143400, Train Loss: 2.7143, Validation Loss: 2.1467\n",
      "Iteration 143500, Train Loss: 2.3017, Validation Loss: 2.4409\n",
      "Iteration 143600, Train Loss: 2.3598, Validation Loss: 2.8442\n",
      "Iteration 143700, Train Loss: 2.1710, Validation Loss: 2.4528\n",
      "Iteration 143800, Train Loss: 2.4256, Validation Loss: 2.4714\n",
      "Iteration 143900, Train Loss: 2.4681, Validation Loss: 2.6109\n",
      "Iteration 144000, Train Loss: 2.7497, Validation Loss: 2.3976\n",
      "Iteration 144100, Train Loss: 2.9667, Validation Loss: 2.9226\n",
      "Iteration 144200, Train Loss: 2.2469, Validation Loss: 2.3303\n",
      "Iteration 144300, Train Loss: 2.5843, Validation Loss: 2.6548\n",
      "Iteration 144400, Train Loss: 2.3492, Validation Loss: 2.7198\n",
      "Iteration 144500, Train Loss: 2.3775, Validation Loss: 2.3351\n",
      "Iteration 144600, Train Loss: 2.2454, Validation Loss: 2.8452\n",
      "Iteration 144700, Train Loss: 2.2492, Validation Loss: 2.0333\n",
      "Iteration 144800, Train Loss: 2.3608, Validation Loss: 2.3942\n",
      "Iteration 144900, Train Loss: 2.5030, Validation Loss: 1.9873\n",
      "Iteration 145000, Train Loss: 1.9255, Validation Loss: 2.7857\n",
      "Iteration 145100, Train Loss: 2.5806, Validation Loss: 2.3986\n",
      "Iteration 145200, Train Loss: 3.0177, Validation Loss: 2.3419\n",
      "Iteration 145300, Train Loss: 2.4660, Validation Loss: 2.4302\n",
      "Iteration 145400, Train Loss: 2.3035, Validation Loss: 2.0196\n",
      "Iteration 145500, Train Loss: 2.3005, Validation Loss: 2.0001\n",
      "Iteration 145600, Train Loss: 2.0321, Validation Loss: 2.0482\n",
      "Iteration 145700, Train Loss: 2.5024, Validation Loss: 2.6201\n",
      "Iteration 145800, Train Loss: 2.0976, Validation Loss: 2.5828\n",
      "Iteration 145900, Train Loss: 2.0772, Validation Loss: 2.2981\n",
      "Iteration 146000, Train Loss: 2.5231, Validation Loss: 2.4360\n",
      "Iteration 146100, Train Loss: 2.3794, Validation Loss: 2.5525\n",
      "Iteration 146200, Train Loss: 2.5899, Validation Loss: 2.0902\n",
      "Iteration 146300, Train Loss: 2.0338, Validation Loss: 2.1734\n",
      "Iteration 146400, Train Loss: 2.0114, Validation Loss: 2.7860\n",
      "Iteration 146500, Train Loss: 2.3613, Validation Loss: 2.2637\n",
      "Iteration 146600, Train Loss: 2.6575, Validation Loss: 2.3413\n",
      "Iteration 146700, Train Loss: 2.3565, Validation Loss: 2.3029\n",
      "Iteration 146800, Train Loss: 2.8068, Validation Loss: 2.6353\n",
      "Iteration 146900, Train Loss: 2.5312, Validation Loss: 2.3661\n",
      "Iteration 147000, Train Loss: 1.9740, Validation Loss: 2.5993\n",
      "Iteration 147100, Train Loss: 2.3681, Validation Loss: 2.0787\n",
      "Iteration 147200, Train Loss: 2.3578, Validation Loss: 2.5135\n",
      "Iteration 147300, Train Loss: 2.3264, Validation Loss: 2.5979\n",
      "Iteration 147400, Train Loss: 2.2987, Validation Loss: 2.5155\n",
      "Iteration 147500, Train Loss: 2.6931, Validation Loss: 2.3714\n",
      "Iteration 147600, Train Loss: 2.4109, Validation Loss: 2.2448\n",
      "Iteration 147700, Train Loss: 2.4012, Validation Loss: 2.0400\n",
      "Iteration 147800, Train Loss: 2.1564, Validation Loss: 2.3330\n",
      "Iteration 147900, Train Loss: 2.3210, Validation Loss: 2.5216\n",
      "Iteration 148000, Train Loss: 2.6909, Validation Loss: 2.3368\n",
      "Iteration 148100, Train Loss: 2.1141, Validation Loss: 2.4357\n",
      "Iteration 148200, Train Loss: 2.7885, Validation Loss: 2.3554\n",
      "Iteration 148300, Train Loss: 2.3702, Validation Loss: 2.3282\n",
      "Iteration 148400, Train Loss: 2.6927, Validation Loss: 2.3460\n",
      "Iteration 148500, Train Loss: 2.5407, Validation Loss: 2.2289\n",
      "Iteration 148600, Train Loss: 2.5165, Validation Loss: 2.4540\n",
      "Iteration 148700, Train Loss: 2.1930, Validation Loss: 2.7728\n",
      "Iteration 148800, Train Loss: 1.8687, Validation Loss: 1.8664\n",
      "Iteration 148900, Train Loss: 2.4339, Validation Loss: 2.1148\n",
      "Iteration 149000, Train Loss: 1.8855, Validation Loss: 2.2173\n",
      "Iteration 149100, Train Loss: 2.4888, Validation Loss: 2.2871\n",
      "Iteration 149200, Train Loss: 2.4207, Validation Loss: 2.2043\n",
      "Iteration 149300, Train Loss: 2.2134, Validation Loss: 2.2822\n",
      "Iteration 149400, Train Loss: 2.2449, Validation Loss: 1.9777\n",
      "Iteration 149500, Train Loss: 2.1542, Validation Loss: 2.4951\n",
      "Iteration 149600, Train Loss: 2.1491, Validation Loss: 2.0415\n",
      "Iteration 149700, Train Loss: 2.4255, Validation Loss: 2.4384\n",
      "Iteration 149800, Train Loss: 1.8117, Validation Loss: 2.3305\n",
      "Iteration 149900, Train Loss: 2.7761, Validation Loss: 2.2553\n",
      "Iteration 150000, Train Loss: 2.8797, Validation Loss: 2.6788\n",
      "Iteration 150100, Train Loss: 2.4799, Validation Loss: 2.4361\n",
      "Iteration 150200, Train Loss: 2.3675, Validation Loss: 2.9087\n",
      "Iteration 150300, Train Loss: 2.3255, Validation Loss: 2.8302\n",
      "Iteration 150400, Train Loss: 2.1148, Validation Loss: 2.5319\n",
      "Iteration 150500, Train Loss: 2.4685, Validation Loss: 2.1391\n",
      "Iteration 150600, Train Loss: 2.1169, Validation Loss: 2.7035\n",
      "Iteration 150700, Train Loss: 2.6691, Validation Loss: 2.7936\n",
      "Iteration 150800, Train Loss: 2.9241, Validation Loss: 2.0784\n",
      "Iteration 150900, Train Loss: 2.4107, Validation Loss: 2.6349\n",
      "Iteration 151000, Train Loss: 1.9608, Validation Loss: 2.2924\n",
      "Iteration 151100, Train Loss: 2.5370, Validation Loss: 2.7433\n",
      "Iteration 151200, Train Loss: 2.5701, Validation Loss: 2.5125\n",
      "Iteration 151300, Train Loss: 2.1043, Validation Loss: 2.4870\n",
      "Iteration 151400, Train Loss: 2.6379, Validation Loss: 2.3214\n",
      "Iteration 151500, Train Loss: 2.3181, Validation Loss: 2.2844\n",
      "Iteration 151600, Train Loss: 2.5686, Validation Loss: 2.1497\n",
      "Iteration 151700, Train Loss: 2.4529, Validation Loss: 2.3298\n",
      "Iteration 151800, Train Loss: 2.2616, Validation Loss: 2.6028\n",
      "Iteration 151900, Train Loss: 2.4754, Validation Loss: 2.3994\n",
      "Iteration 152000, Train Loss: 2.8593, Validation Loss: 2.4768\n",
      "Iteration 152100, Train Loss: 2.2912, Validation Loss: 2.4181\n",
      "Iteration 152200, Train Loss: 1.9955, Validation Loss: 2.4384\n",
      "Iteration 152300, Train Loss: 2.5485, Validation Loss: 1.9088\n",
      "Iteration 152400, Train Loss: 2.0020, Validation Loss: 2.6076\n",
      "Iteration 152500, Train Loss: 2.5087, Validation Loss: 2.9804\n",
      "Iteration 152600, Train Loss: 2.2789, Validation Loss: 2.1895\n",
      "Iteration 152700, Train Loss: 2.5461, Validation Loss: 2.3698\n",
      "Iteration 152800, Train Loss: 1.6071, Validation Loss: 2.0803\n",
      "Iteration 152900, Train Loss: 2.2892, Validation Loss: 2.2587\n",
      "Iteration 153000, Train Loss: 2.2669, Validation Loss: 2.3121\n",
      "Iteration 153100, Train Loss: 2.3448, Validation Loss: 2.7895\n",
      "Iteration 153200, Train Loss: 2.4814, Validation Loss: 2.4845\n",
      "Iteration 153300, Train Loss: 2.4920, Validation Loss: 1.9808\n",
      "Iteration 153400, Train Loss: 2.3683, Validation Loss: 2.2663\n",
      "Iteration 153500, Train Loss: 2.4124, Validation Loss: 2.2119\n",
      "Iteration 153600, Train Loss: 2.4501, Validation Loss: 2.5051\n",
      "Iteration 153700, Train Loss: 2.0529, Validation Loss: 2.5002\n",
      "Iteration 153800, Train Loss: 2.2311, Validation Loss: 2.1712\n",
      "Iteration 153900, Train Loss: 2.3715, Validation Loss: 2.3511\n",
      "Iteration 154000, Train Loss: 2.1637, Validation Loss: 2.6566\n",
      "Iteration 154100, Train Loss: 2.4634, Validation Loss: 2.5908\n",
      "Iteration 154200, Train Loss: 2.4667, Validation Loss: 3.1743\n",
      "Iteration 154300, Train Loss: 1.8686, Validation Loss: 2.3698\n",
      "Iteration 154400, Train Loss: 2.3149, Validation Loss: 2.1718\n",
      "Iteration 154500, Train Loss: 2.5706, Validation Loss: 2.4879\n",
      "Iteration 154600, Train Loss: 2.3088, Validation Loss: 2.2015\n",
      "Iteration 154700, Train Loss: 2.3399, Validation Loss: 2.6908\n",
      "Iteration 154800, Train Loss: 2.0489, Validation Loss: 2.4244\n",
      "Iteration 154900, Train Loss: 2.3607, Validation Loss: 2.8271\n",
      "Iteration 155000, Train Loss: 2.7125, Validation Loss: 2.3767\n",
      "Iteration 155100, Train Loss: 2.9001, Validation Loss: 2.3770\n",
      "Iteration 155200, Train Loss: 2.7330, Validation Loss: 2.3972\n",
      "Iteration 155300, Train Loss: 2.1521, Validation Loss: 2.3722\n",
      "Iteration 155400, Train Loss: 2.5287, Validation Loss: 2.6517\n",
      "Iteration 155500, Train Loss: 2.0631, Validation Loss: 2.0433\n",
      "Iteration 155600, Train Loss: 2.3536, Validation Loss: 2.2991\n",
      "Iteration 155700, Train Loss: 2.2606, Validation Loss: 1.9182\n",
      "Iteration 155800, Train Loss: 2.5734, Validation Loss: 2.2710\n",
      "Iteration 155900, Train Loss: 2.3051, Validation Loss: 2.4120\n",
      "Iteration 156000, Train Loss: 2.2464, Validation Loss: 2.3088\n",
      "Iteration 156100, Train Loss: 2.1521, Validation Loss: 2.8681\n",
      "Iteration 156200, Train Loss: 2.2502, Validation Loss: 2.2970\n",
      "Iteration 156300, Train Loss: 2.1121, Validation Loss: 2.6476\n",
      "Iteration 156400, Train Loss: 2.1684, Validation Loss: 2.2745\n",
      "Iteration 156500, Train Loss: 2.5193, Validation Loss: 2.5851\n",
      "Iteration 156600, Train Loss: 2.9129, Validation Loss: 3.1019\n",
      "Iteration 156700, Train Loss: 2.3516, Validation Loss: 2.5631\n",
      "Iteration 156800, Train Loss: 2.0520, Validation Loss: 2.0842\n",
      "Iteration 156900, Train Loss: 2.5585, Validation Loss: 2.4085\n",
      "Iteration 157000, Train Loss: 2.3896, Validation Loss: 2.0544\n",
      "Iteration 157100, Train Loss: 2.6949, Validation Loss: 2.6700\n",
      "Iteration 157200, Train Loss: 2.5893, Validation Loss: 2.0508\n",
      "Iteration 157300, Train Loss: 2.3128, Validation Loss: 2.3530\n",
      "Iteration 157400, Train Loss: 2.6306, Validation Loss: 2.8231\n",
      "Iteration 157500, Train Loss: 2.2088, Validation Loss: 1.8571\n",
      "Iteration 157600, Train Loss: 2.4762, Validation Loss: 2.4735\n",
      "Iteration 157700, Train Loss: 2.0567, Validation Loss: 2.2007\n",
      "Iteration 157800, Train Loss: 2.1980, Validation Loss: 2.2184\n",
      "Iteration 157900, Train Loss: 2.2113, Validation Loss: 2.3362\n",
      "Iteration 158000, Train Loss: 2.3997, Validation Loss: 2.0804\n",
      "Iteration 158100, Train Loss: 2.2851, Validation Loss: 2.7112\n",
      "Iteration 158200, Train Loss: 2.2802, Validation Loss: 2.4163\n",
      "Iteration 158300, Train Loss: 2.2529, Validation Loss: 2.1942\n",
      "Iteration 158400, Train Loss: 2.1682, Validation Loss: 2.5872\n",
      "Iteration 158500, Train Loss: 1.9414, Validation Loss: 2.2868\n",
      "Iteration 158600, Train Loss: 2.2917, Validation Loss: 2.1913\n",
      "Iteration 158700, Train Loss: 2.3484, Validation Loss: 2.3838\n",
      "Iteration 158800, Train Loss: 2.6523, Validation Loss: 2.3547\n",
      "Iteration 158900, Train Loss: 2.3652, Validation Loss: 2.4295\n",
      "Iteration 159000, Train Loss: 2.5793, Validation Loss: 2.4257\n",
      "Iteration 159100, Train Loss: 2.4009, Validation Loss: 2.4900\n",
      "Iteration 159200, Train Loss: 2.5553, Validation Loss: 2.2840\n",
      "Iteration 159300, Train Loss: 1.9215, Validation Loss: 2.1856\n",
      "Iteration 159400, Train Loss: 2.4627, Validation Loss: 2.5100\n",
      "Iteration 159500, Train Loss: 2.4160, Validation Loss: 2.3868\n",
      "Iteration 159600, Train Loss: 2.8659, Validation Loss: 2.4439\n",
      "Iteration 159700, Train Loss: 2.3994, Validation Loss: 2.4378\n",
      "Iteration 159800, Train Loss: 2.1385, Validation Loss: 2.5398\n",
      "Iteration 159900, Train Loss: 2.5133, Validation Loss: 2.0749\n",
      "Iteration 160000, Train Loss: 2.8838, Validation Loss: 2.4592\n",
      "Iteration 160100, Train Loss: 2.3905, Validation Loss: 2.2562\n",
      "Iteration 160200, Train Loss: 2.4906, Validation Loss: 2.4967\n",
      "Iteration 160300, Train Loss: 2.4007, Validation Loss: 2.2750\n",
      "Iteration 160400, Train Loss: 2.3444, Validation Loss: 2.1198\n",
      "Iteration 160500, Train Loss: 2.2423, Validation Loss: 2.4315\n",
      "Iteration 160600, Train Loss: 2.8930, Validation Loss: 2.7658\n",
      "Iteration 160700, Train Loss: 2.3530, Validation Loss: 2.0088\n",
      "Iteration 160800, Train Loss: 2.0290, Validation Loss: 1.9819\n",
      "Iteration 160900, Train Loss: 2.6969, Validation Loss: 2.2545\n",
      "Iteration 161000, Train Loss: 2.4621, Validation Loss: 2.0312\n",
      "Iteration 161100, Train Loss: 2.4226, Validation Loss: 2.5086\n",
      "Iteration 161200, Train Loss: 3.0090, Validation Loss: 2.2988\n",
      "Iteration 161300, Train Loss: 2.3734, Validation Loss: 2.2155\n",
      "Iteration 161400, Train Loss: 2.2300, Validation Loss: 2.6893\n",
      "Iteration 161500, Train Loss: 2.4235, Validation Loss: 2.3322\n",
      "Iteration 161600, Train Loss: 2.3017, Validation Loss: 3.0092\n",
      "Iteration 161700, Train Loss: 1.9813, Validation Loss: 2.1442\n",
      "Iteration 161800, Train Loss: 1.7868, Validation Loss: 2.8495\n",
      "Iteration 161900, Train Loss: 2.7838, Validation Loss: 2.2871\n",
      "Iteration 162000, Train Loss: 2.4325, Validation Loss: 2.9255\n",
      "Iteration 162100, Train Loss: 2.4246, Validation Loss: 2.4768\n",
      "Iteration 162200, Train Loss: 2.0769, Validation Loss: 2.1167\n",
      "Iteration 162300, Train Loss: 2.5759, Validation Loss: 1.8252\n",
      "Iteration 162400, Train Loss: 2.1719, Validation Loss: 2.1839\n",
      "Iteration 162500, Train Loss: 2.6607, Validation Loss: 2.7438\n",
      "Iteration 162600, Train Loss: 2.5075, Validation Loss: 2.2960\n",
      "Iteration 162700, Train Loss: 2.5427, Validation Loss: 2.6140\n",
      "Iteration 162800, Train Loss: 2.2247, Validation Loss: 2.7048\n",
      "Iteration 162900, Train Loss: 2.4124, Validation Loss: 2.3571\n",
      "Iteration 163000, Train Loss: 2.1909, Validation Loss: 2.0994\n",
      "Iteration 163100, Train Loss: 2.3092, Validation Loss: 2.6156\n",
      "Iteration 163200, Train Loss: 2.3171, Validation Loss: 3.1136\n",
      "Iteration 163300, Train Loss: 2.4448, Validation Loss: 2.2218\n",
      "Iteration 163400, Train Loss: 1.9599, Validation Loss: 2.1160\n",
      "Iteration 163500, Train Loss: 2.3277, Validation Loss: 2.5670\n",
      "Iteration 163600, Train Loss: 2.6219, Validation Loss: 2.3695\n",
      "Iteration 163700, Train Loss: 2.5777, Validation Loss: 2.7172\n",
      "Iteration 163800, Train Loss: 2.5033, Validation Loss: 2.7387\n",
      "Iteration 163900, Train Loss: 2.1636, Validation Loss: 2.1612\n",
      "Iteration 164000, Train Loss: 2.7677, Validation Loss: 2.2942\n",
      "Iteration 164100, Train Loss: 2.9133, Validation Loss: 2.5306\n",
      "Iteration 164200, Train Loss: 2.3541, Validation Loss: 2.5028\n",
      "Iteration 164300, Train Loss: 2.2035, Validation Loss: 2.1685\n",
      "Iteration 164400, Train Loss: 2.1351, Validation Loss: 2.3239\n",
      "Iteration 164500, Train Loss: 1.8496, Validation Loss: 2.4638\n",
      "Iteration 164600, Train Loss: 2.1537, Validation Loss: 2.6245\n",
      "Iteration 164700, Train Loss: 2.3841, Validation Loss: 2.5200\n",
      "Iteration 164800, Train Loss: 2.2843, Validation Loss: 3.0883\n",
      "Iteration 164900, Train Loss: 2.8559, Validation Loss: 2.4723\n",
      "Iteration 165000, Train Loss: 2.1578, Validation Loss: 2.4212\n",
      "Iteration 165100, Train Loss: 2.4084, Validation Loss: 2.3483\n",
      "Iteration 165200, Train Loss: 2.2965, Validation Loss: 1.8998\n",
      "Iteration 165300, Train Loss: 2.2110, Validation Loss: 2.2781\n",
      "Iteration 165400, Train Loss: 2.3401, Validation Loss: 2.2244\n",
      "Iteration 165500, Train Loss: 2.2817, Validation Loss: 2.6316\n",
      "Iteration 165600, Train Loss: 2.4741, Validation Loss: 2.3234\n",
      "Iteration 165700, Train Loss: 2.5028, Validation Loss: 1.9585\n",
      "Iteration 165800, Train Loss: 2.4877, Validation Loss: 2.5245\n",
      "Iteration 165900, Train Loss: 2.0237, Validation Loss: 2.5100\n",
      "Iteration 166000, Train Loss: 2.6254, Validation Loss: 2.6585\n",
      "Iteration 166100, Train Loss: 2.3286, Validation Loss: 2.2765\n",
      "Iteration 166200, Train Loss: 2.2640, Validation Loss: 2.1447\n",
      "Iteration 166300, Train Loss: 2.1301, Validation Loss: 2.3691\n",
      "Iteration 166400, Train Loss: 2.3077, Validation Loss: 1.8879\n",
      "Iteration 166500, Train Loss: 2.1534, Validation Loss: 2.8822\n",
      "Iteration 166600, Train Loss: 2.5565, Validation Loss: 2.7052\n",
      "Iteration 166700, Train Loss: 2.0102, Validation Loss: 1.9824\n",
      "Iteration 166800, Train Loss: 2.3978, Validation Loss: 2.8312\n",
      "Iteration 166900, Train Loss: 2.3430, Validation Loss: 2.2053\n",
      "Iteration 167000, Train Loss: 2.2878, Validation Loss: 2.2771\n",
      "Iteration 167100, Train Loss: 2.7628, Validation Loss: 2.2985\n",
      "Iteration 167200, Train Loss: 2.4136, Validation Loss: 2.1176\n",
      "Iteration 167300, Train Loss: 2.5125, Validation Loss: 2.5213\n",
      "Iteration 167400, Train Loss: 2.2604, Validation Loss: 2.2342\n",
      "Iteration 167500, Train Loss: 2.2124, Validation Loss: 2.1629\n",
      "Iteration 167600, Train Loss: 2.2034, Validation Loss: 2.1319\n",
      "Iteration 167700, Train Loss: 2.4878, Validation Loss: 2.2404\n",
      "Iteration 167800, Train Loss: 2.1768, Validation Loss: 2.1838\n",
      "Iteration 167900, Train Loss: 2.7087, Validation Loss: 2.2240\n",
      "Iteration 168000, Train Loss: 2.3346, Validation Loss: 2.2618\n",
      "Iteration 168100, Train Loss: 2.3397, Validation Loss: 2.3608\n",
      "Iteration 168200, Train Loss: 2.0274, Validation Loss: 2.2988\n",
      "Iteration 168300, Train Loss: 2.3934, Validation Loss: 2.3952\n",
      "Iteration 168400, Train Loss: 2.3069, Validation Loss: 1.8192\n",
      "Iteration 168500, Train Loss: 2.5098, Validation Loss: 2.1396\n",
      "Iteration 168600, Train Loss: 2.7607, Validation Loss: 2.0994\n",
      "Iteration 168700, Train Loss: 2.3191, Validation Loss: 2.0968\n",
      "Iteration 168800, Train Loss: 1.9919, Validation Loss: 2.5984\n",
      "Iteration 168900, Train Loss: 2.3315, Validation Loss: 2.0936\n",
      "Iteration 169000, Train Loss: 2.4179, Validation Loss: 2.3992\n",
      "Iteration 169100, Train Loss: 2.4101, Validation Loss: 2.2765\n",
      "Iteration 169200, Train Loss: 2.2535, Validation Loss: 2.0453\n",
      "Iteration 169300, Train Loss: 2.2872, Validation Loss: 2.2982\n",
      "Iteration 169400, Train Loss: 2.3554, Validation Loss: 2.4688\n",
      "Iteration 169500, Train Loss: 2.3842, Validation Loss: 2.4050\n",
      "Iteration 169600, Train Loss: 2.5367, Validation Loss: 2.0880\n",
      "Iteration 169700, Train Loss: 2.4483, Validation Loss: 2.6242\n",
      "Iteration 169800, Train Loss: 2.3630, Validation Loss: 2.3442\n",
      "Iteration 169900, Train Loss: 2.2087, Validation Loss: 1.9403\n",
      "Iteration 170000, Train Loss: 2.3619, Validation Loss: 1.8778\n",
      "Iteration 170100, Train Loss: 2.4987, Validation Loss: 2.2435\n",
      "Iteration 170200, Train Loss: 2.3494, Validation Loss: 2.2212\n",
      "Iteration 170300, Train Loss: 2.1908, Validation Loss: 2.3463\n",
      "Iteration 170400, Train Loss: 2.2721, Validation Loss: 2.1586\n",
      "Iteration 170500, Train Loss: 2.0316, Validation Loss: 2.2205\n",
      "Iteration 170600, Train Loss: 2.4703, Validation Loss: 2.1613\n",
      "Iteration 170700, Train Loss: 2.4195, Validation Loss: 2.2756\n",
      "Iteration 170800, Train Loss: 2.3095, Validation Loss: 2.2034\n",
      "Iteration 170900, Train Loss: 2.1363, Validation Loss: 2.1546\n",
      "Iteration 171000, Train Loss: 2.4998, Validation Loss: 2.3248\n",
      "Iteration 171100, Train Loss: 2.0391, Validation Loss: 2.4965\n",
      "Iteration 171200, Train Loss: 2.6614, Validation Loss: 2.4461\n",
      "Iteration 171300, Train Loss: 2.0329, Validation Loss: 2.1400\n",
      "Iteration 171400, Train Loss: 2.5900, Validation Loss: 2.8603\n",
      "Iteration 171500, Train Loss: 2.4013, Validation Loss: 2.3488\n",
      "Iteration 171600, Train Loss: 2.8666, Validation Loss: 2.2265\n",
      "Iteration 171700, Train Loss: 1.8367, Validation Loss: 2.6617\n",
      "Iteration 171800, Train Loss: 2.8783, Validation Loss: 2.6303\n",
      "Iteration 171900, Train Loss: 2.3250, Validation Loss: 2.6491\n",
      "Iteration 172000, Train Loss: 2.2391, Validation Loss: 2.3993\n",
      "Iteration 172100, Train Loss: 2.0616, Validation Loss: 2.5556\n",
      "Iteration 172200, Train Loss: 2.4390, Validation Loss: 2.2790\n",
      "Iteration 172300, Train Loss: 2.1511, Validation Loss: 2.0950\n",
      "Iteration 172400, Train Loss: 2.1102, Validation Loss: 2.8459\n",
      "Iteration 172500, Train Loss: 2.6585, Validation Loss: 2.4093\n",
      "Iteration 172600, Train Loss: 2.7763, Validation Loss: 2.2882\n",
      "Iteration 172700, Train Loss: 2.8274, Validation Loss: 2.9560\n",
      "Iteration 172800, Train Loss: 2.6223, Validation Loss: 2.0336\n",
      "Iteration 172900, Train Loss: 2.1057, Validation Loss: 2.4420\n",
      "Iteration 173000, Train Loss: 2.4921, Validation Loss: 2.2207\n",
      "Iteration 173100, Train Loss: 1.8606, Validation Loss: 2.6709\n",
      "Iteration 173200, Train Loss: 2.6288, Validation Loss: 2.5571\n",
      "Iteration 173300, Train Loss: 2.3328, Validation Loss: 2.2227\n",
      "Iteration 173400, Train Loss: 2.5075, Validation Loss: 2.3785\n",
      "Iteration 173500, Train Loss: 2.3404, Validation Loss: 2.0547\n",
      "Iteration 173600, Train Loss: 2.3462, Validation Loss: 2.7717\n",
      "Iteration 173700, Train Loss: 2.3249, Validation Loss: 2.4484\n",
      "Iteration 173800, Train Loss: 2.1623, Validation Loss: 2.2889\n",
      "Iteration 173900, Train Loss: 2.5456, Validation Loss: 2.3149\n",
      "Iteration 174000, Train Loss: 2.3202, Validation Loss: 2.2900\n",
      "Iteration 174100, Train Loss: 2.5547, Validation Loss: 2.3219\n",
      "Iteration 174200, Train Loss: 2.3090, Validation Loss: 2.1633\n",
      "Iteration 174300, Train Loss: 2.5680, Validation Loss: 2.7053\n",
      "Iteration 174400, Train Loss: 2.3858, Validation Loss: 2.2959\n",
      "Iteration 174500, Train Loss: 1.9878, Validation Loss: 2.6387\n",
      "Iteration 174600, Train Loss: 2.2699, Validation Loss: 1.9709\n",
      "Iteration 174700, Train Loss: 1.7163, Validation Loss: 2.1631\n",
      "Iteration 174800, Train Loss: 2.0384, Validation Loss: 2.3163\n",
      "Iteration 174900, Train Loss: 2.1709, Validation Loss: 2.6325\n",
      "Iteration 175000, Train Loss: 2.1739, Validation Loss: 2.1302\n",
      "Iteration 175100, Train Loss: 2.3233, Validation Loss: 2.4006\n",
      "Iteration 175200, Train Loss: 2.1726, Validation Loss: 2.1792\n",
      "Iteration 175300, Train Loss: 2.3344, Validation Loss: 2.8148\n",
      "Iteration 175400, Train Loss: 2.3903, Validation Loss: 2.3804\n",
      "Iteration 175500, Train Loss: 2.5402, Validation Loss: 2.5720\n",
      "Iteration 175600, Train Loss: 2.3190, Validation Loss: 2.2968\n",
      "Iteration 175700, Train Loss: 2.0340, Validation Loss: 2.3972\n",
      "Iteration 175800, Train Loss: 2.4423, Validation Loss: 1.8852\n",
      "Iteration 175900, Train Loss: 2.5767, Validation Loss: 2.3489\n",
      "Iteration 176000, Train Loss: 2.1811, Validation Loss: 2.4808\n",
      "Iteration 176100, Train Loss: 2.3053, Validation Loss: 2.4624\n",
      "Iteration 176200, Train Loss: 2.7060, Validation Loss: 1.9359\n",
      "Iteration 176300, Train Loss: 2.2977, Validation Loss: 2.2269\n",
      "Iteration 176400, Train Loss: 2.4515, Validation Loss: 2.7341\n",
      "Iteration 176500, Train Loss: 2.1626, Validation Loss: 2.4109\n",
      "Iteration 176600, Train Loss: 2.3710, Validation Loss: 2.5125\n",
      "Iteration 176700, Train Loss: 2.0919, Validation Loss: 2.3484\n",
      "Iteration 176800, Train Loss: 2.0873, Validation Loss: 1.9227\n",
      "Iteration 176900, Train Loss: 2.2904, Validation Loss: 2.1891\n",
      "Iteration 177000, Train Loss: 2.3406, Validation Loss: 2.4900\n",
      "Iteration 177100, Train Loss: 2.1749, Validation Loss: 2.3644\n",
      "Iteration 177200, Train Loss: 2.2161, Validation Loss: 2.1365\n",
      "Iteration 177300, Train Loss: 2.2323, Validation Loss: 2.8961\n",
      "Iteration 177400, Train Loss: 2.1067, Validation Loss: 2.2244\n",
      "Iteration 177500, Train Loss: 2.4463, Validation Loss: 2.2184\n",
      "Iteration 177600, Train Loss: 2.2650, Validation Loss: 2.0689\n",
      "Iteration 177700, Train Loss: 1.9684, Validation Loss: 2.2855\n",
      "Iteration 177800, Train Loss: 2.3066, Validation Loss: 2.0760\n",
      "Iteration 177900, Train Loss: 2.3290, Validation Loss: 2.4196\n",
      "Iteration 178000, Train Loss: 2.7445, Validation Loss: 2.2582\n",
      "Iteration 178100, Train Loss: 2.1535, Validation Loss: 2.2778\n",
      "Iteration 178200, Train Loss: 2.6075, Validation Loss: 3.0374\n",
      "Iteration 178300, Train Loss: 1.9619, Validation Loss: 2.1510\n",
      "Iteration 178400, Train Loss: 2.3408, Validation Loss: 2.5220\n",
      "Iteration 178500, Train Loss: 2.3102, Validation Loss: 2.0874\n",
      "Iteration 178600, Train Loss: 2.4438, Validation Loss: 2.3638\n",
      "Iteration 178700, Train Loss: 3.0422, Validation Loss: 2.2380\n",
      "Iteration 178800, Train Loss: 2.4145, Validation Loss: 2.6167\n",
      "Iteration 178900, Train Loss: 2.4915, Validation Loss: 1.9622\n",
      "Iteration 179000, Train Loss: 2.0898, Validation Loss: 2.5118\n",
      "Iteration 179100, Train Loss: 2.2019, Validation Loss: 2.5330\n",
      "Iteration 179200, Train Loss: 2.5587, Validation Loss: 2.5796\n",
      "Iteration 179300, Train Loss: 2.3388, Validation Loss: 2.5237\n",
      "Iteration 179400, Train Loss: 2.0206, Validation Loss: 2.5204\n",
      "Iteration 179500, Train Loss: 2.2731, Validation Loss: 2.3391\n",
      "Iteration 179600, Train Loss: 2.2460, Validation Loss: 2.3092\n",
      "Iteration 179700, Train Loss: 2.3320, Validation Loss: 2.0494\n",
      "Iteration 179800, Train Loss: 1.9265, Validation Loss: 2.4673\n",
      "Iteration 179900, Train Loss: 2.2947, Validation Loss: 2.5175\n",
      "Iteration 180000, Train Loss: 2.1043, Validation Loss: 2.9162\n",
      "Iteration 180100, Train Loss: 2.4128, Validation Loss: 2.2970\n",
      "Iteration 180200, Train Loss: 2.7141, Validation Loss: 2.2297\n",
      "Iteration 180300, Train Loss: 2.0289, Validation Loss: 1.9198\n",
      "Iteration 180400, Train Loss: 2.1699, Validation Loss: 2.4569\n",
      "Iteration 180500, Train Loss: 2.3074, Validation Loss: 2.4065\n",
      "Iteration 180600, Train Loss: 2.6851, Validation Loss: 2.5449\n",
      "Iteration 180700, Train Loss: 2.3821, Validation Loss: 2.3093\n",
      "Iteration 180800, Train Loss: 2.5875, Validation Loss: 2.6617\n",
      "Iteration 180900, Train Loss: 2.3098, Validation Loss: 2.7673\n",
      "Iteration 181000, Train Loss: 2.3705, Validation Loss: 2.6740\n",
      "Iteration 181100, Train Loss: 2.0092, Validation Loss: 2.6410\n",
      "Iteration 181200, Train Loss: 2.3613, Validation Loss: 2.1003\n",
      "Iteration 181300, Train Loss: 2.5905, Validation Loss: 2.2197\n",
      "Iteration 181400, Train Loss: 2.6903, Validation Loss: 2.5254\n",
      "Iteration 181500, Train Loss: 2.0928, Validation Loss: 1.9708\n",
      "Iteration 181600, Train Loss: 2.3066, Validation Loss: 2.5834\n",
      "Iteration 181700, Train Loss: 2.0210, Validation Loss: 2.3465\n",
      "Iteration 181800, Train Loss: 2.7846, Validation Loss: 2.4861\n",
      "Iteration 181900, Train Loss: 2.2049, Validation Loss: 2.2848\n",
      "Iteration 182000, Train Loss: 2.6532, Validation Loss: 2.4784\n",
      "Iteration 182100, Train Loss: 2.0799, Validation Loss: 2.5198\n",
      "Iteration 182200, Train Loss: 3.1450, Validation Loss: 2.3344\n",
      "Iteration 182300, Train Loss: 2.3128, Validation Loss: 2.8334\n",
      "Iteration 182400, Train Loss: 2.1127, Validation Loss: 2.2715\n",
      "Iteration 182500, Train Loss: 2.2457, Validation Loss: 2.5924\n",
      "Iteration 182600, Train Loss: 2.3695, Validation Loss: 2.4960\n",
      "Iteration 182700, Train Loss: 2.7467, Validation Loss: 2.6992\n",
      "Iteration 182800, Train Loss: 2.3008, Validation Loss: 2.6578\n",
      "Iteration 182900, Train Loss: 2.1627, Validation Loss: 2.5293\n",
      "Iteration 183000, Train Loss: 2.1891, Validation Loss: 2.6354\n",
      "Iteration 183100, Train Loss: 2.6048, Validation Loss: 2.1641\n",
      "Iteration 183200, Train Loss: 2.3220, Validation Loss: 2.4937\n",
      "Iteration 183300, Train Loss: 1.8667, Validation Loss: 2.5647\n",
      "Iteration 183400, Train Loss: 2.6476, Validation Loss: 2.5829\n",
      "Iteration 183500, Train Loss: 2.2703, Validation Loss: 2.4528\n",
      "Iteration 183600, Train Loss: 2.3049, Validation Loss: 2.3607\n",
      "Iteration 183700, Train Loss: 2.4479, Validation Loss: 2.5659\n",
      "Iteration 183800, Train Loss: 2.3039, Validation Loss: 2.5503\n",
      "Iteration 183900, Train Loss: 1.9428, Validation Loss: 2.1857\n",
      "Iteration 184000, Train Loss: 2.3141, Validation Loss: 2.2652\n",
      "Iteration 184100, Train Loss: 2.4766, Validation Loss: 3.0929\n",
      "Iteration 184200, Train Loss: 2.4920, Validation Loss: 2.4157\n",
      "Iteration 184300, Train Loss: 2.4848, Validation Loss: 2.5801\n",
      "Iteration 184400, Train Loss: 2.3738, Validation Loss: 2.0824\n",
      "Iteration 184500, Train Loss: 2.2331, Validation Loss: 2.5979\n",
      "Iteration 184600, Train Loss: 2.2722, Validation Loss: 2.2144\n",
      "Iteration 184700, Train Loss: 2.1854, Validation Loss: 2.3999\n",
      "Iteration 184800, Train Loss: 2.6986, Validation Loss: 2.0825\n",
      "Iteration 184900, Train Loss: 2.0407, Validation Loss: 2.4738\n",
      "Iteration 185000, Train Loss: 2.6161, Validation Loss: 2.1827\n",
      "Iteration 185100, Train Loss: 2.7770, Validation Loss: 2.3948\n",
      "Iteration 185200, Train Loss: 2.4089, Validation Loss: 2.6075\n",
      "Iteration 185300, Train Loss: 2.1827, Validation Loss: 2.5953\n",
      "Iteration 185400, Train Loss: 2.4919, Validation Loss: 2.3475\n",
      "Iteration 185500, Train Loss: 2.4772, Validation Loss: 2.3090\n",
      "Iteration 185600, Train Loss: 2.3256, Validation Loss: 2.0923\n",
      "Iteration 185700, Train Loss: 1.9799, Validation Loss: 2.6028\n",
      "Iteration 185800, Train Loss: 2.1614, Validation Loss: 2.2644\n",
      "Iteration 185900, Train Loss: 2.2400, Validation Loss: 2.6396\n",
      "Iteration 186000, Train Loss: 2.2383, Validation Loss: 2.6955\n",
      "Iteration 186100, Train Loss: 2.4099, Validation Loss: 2.1898\n",
      "Iteration 186200, Train Loss: 2.5475, Validation Loss: 2.1617\n",
      "Iteration 186300, Train Loss: 2.4759, Validation Loss: 2.3161\n",
      "Iteration 186400, Train Loss: 1.9867, Validation Loss: 2.4025\n",
      "Iteration 186500, Train Loss: 1.7431, Validation Loss: 1.9293\n",
      "Iteration 186600, Train Loss: 2.5401, Validation Loss: 2.3937\n",
      "Iteration 186700, Train Loss: 1.9991, Validation Loss: 2.2621\n",
      "Iteration 186800, Train Loss: 2.3620, Validation Loss: 2.2189\n",
      "Iteration 186900, Train Loss: 2.2809, Validation Loss: 2.5088\n",
      "Iteration 187000, Train Loss: 2.1648, Validation Loss: 2.6260\n",
      "Iteration 187100, Train Loss: 1.9399, Validation Loss: 2.5909\n",
      "Iteration 187200, Train Loss: 2.0305, Validation Loss: 2.8037\n",
      "Iteration 187300, Train Loss: 2.2969, Validation Loss: 2.5967\n",
      "Iteration 187400, Train Loss: 2.4256, Validation Loss: 2.2751\n",
      "Iteration 187500, Train Loss: 2.4126, Validation Loss: 2.1462\n",
      "Iteration 187600, Train Loss: 2.8114, Validation Loss: 2.5577\n",
      "Iteration 187700, Train Loss: 2.2236, Validation Loss: 2.3904\n",
      "Iteration 187800, Train Loss: 2.1531, Validation Loss: 2.1158\n",
      "Iteration 187900, Train Loss: 2.1530, Validation Loss: 2.1642\n",
      "Iteration 188000, Train Loss: 2.0458, Validation Loss: 2.2431\n",
      "Iteration 188100, Train Loss: 2.5171, Validation Loss: 2.1714\n",
      "Iteration 188200, Train Loss: 1.8307, Validation Loss: 2.0474\n",
      "Iteration 188300, Train Loss: 2.1584, Validation Loss: 2.1845\n",
      "Iteration 188400, Train Loss: 2.2507, Validation Loss: 2.1859\n",
      "Iteration 188500, Train Loss: 2.8109, Validation Loss: 2.5660\n",
      "Iteration 188600, Train Loss: 2.3089, Validation Loss: 2.5483\n",
      "Iteration 188700, Train Loss: 1.8733, Validation Loss: 2.3260\n",
      "Iteration 188800, Train Loss: 2.5651, Validation Loss: 2.2152\n",
      "Iteration 188900, Train Loss: 2.2602, Validation Loss: 2.3529\n",
      "Iteration 189000, Train Loss: 2.7934, Validation Loss: 2.4360\n",
      "Iteration 189100, Train Loss: 2.2288, Validation Loss: 2.1680\n",
      "Iteration 189200, Train Loss: 2.2218, Validation Loss: 2.4366\n",
      "Iteration 189300, Train Loss: 2.2026, Validation Loss: 2.2100\n",
      "Iteration 189400, Train Loss: 2.0409, Validation Loss: 2.2045\n",
      "Iteration 189500, Train Loss: 2.3720, Validation Loss: 2.2359\n",
      "Iteration 189600, Train Loss: 2.4298, Validation Loss: 2.7466\n",
      "Iteration 189700, Train Loss: 2.4475, Validation Loss: 2.6472\n",
      "Iteration 189800, Train Loss: 1.9459, Validation Loss: 2.3292\n",
      "Iteration 189900, Train Loss: 2.5333, Validation Loss: 2.4117\n",
      "Iteration 190000, Train Loss: 2.3195, Validation Loss: 2.6224\n",
      "Iteration 190100, Train Loss: 2.2588, Validation Loss: 2.1151\n",
      "Iteration 190200, Train Loss: 2.8278, Validation Loss: 2.5196\n",
      "Iteration 190300, Train Loss: 2.3565, Validation Loss: 2.1542\n",
      "Iteration 190400, Train Loss: 2.2181, Validation Loss: 2.8062\n",
      "Iteration 190500, Train Loss: 2.4164, Validation Loss: 1.9786\n",
      "Iteration 190600, Train Loss: 2.4144, Validation Loss: 2.5233\n",
      "Iteration 190700, Train Loss: 1.9971, Validation Loss: 2.1747\n",
      "Iteration 190800, Train Loss: 2.1519, Validation Loss: 2.2344\n",
      "Iteration 190900, Train Loss: 2.4516, Validation Loss: 2.2913\n",
      "Iteration 191000, Train Loss: 2.3679, Validation Loss: 2.4976\n",
      "Iteration 191100, Train Loss: 1.8683, Validation Loss: 2.5452\n",
      "Iteration 191200, Train Loss: 2.8799, Validation Loss: 2.5780\n",
      "Iteration 191300, Train Loss: 2.4274, Validation Loss: 2.2022\n",
      "Iteration 191400, Train Loss: 2.5938, Validation Loss: 2.7860\n",
      "Iteration 191500, Train Loss: 2.0404, Validation Loss: 2.4647\n",
      "Iteration 191600, Train Loss: 2.4440, Validation Loss: 2.8065\n",
      "Iteration 191700, Train Loss: 2.1642, Validation Loss: 2.3102\n",
      "Iteration 191800, Train Loss: 2.2714, Validation Loss: 2.4350\n",
      "Iteration 191900, Train Loss: 1.9452, Validation Loss: 2.5256\n",
      "Iteration 192000, Train Loss: 2.0417, Validation Loss: 2.2158\n",
      "Iteration 192100, Train Loss: 2.4424, Validation Loss: 2.7622\n",
      "Iteration 192200, Train Loss: 1.9538, Validation Loss: 2.3400\n",
      "Iteration 192300, Train Loss: 2.6127, Validation Loss: 2.2337\n",
      "Iteration 192400, Train Loss: 2.0500, Validation Loss: 2.2339\n",
      "Iteration 192500, Train Loss: 2.2625, Validation Loss: 2.1941\n",
      "Iteration 192600, Train Loss: 2.2700, Validation Loss: 2.3455\n",
      "Iteration 192700, Train Loss: 2.5301, Validation Loss: 2.1311\n",
      "Iteration 192800, Train Loss: 2.1545, Validation Loss: 2.1684\n",
      "Iteration 192900, Train Loss: 2.4146, Validation Loss: 2.0352\n",
      "Iteration 193000, Train Loss: 2.0670, Validation Loss: 2.3768\n",
      "Iteration 193100, Train Loss: 2.1060, Validation Loss: 2.5584\n",
      "Iteration 193200, Train Loss: 2.1911, Validation Loss: 2.3806\n",
      "Iteration 193300, Train Loss: 1.9800, Validation Loss: 2.1435\n",
      "Iteration 193400, Train Loss: 2.1490, Validation Loss: 1.9556\n",
      "Iteration 193500, Train Loss: 2.1547, Validation Loss: 2.1761\n",
      "Iteration 193600, Train Loss: 2.3326, Validation Loss: 2.1969\n",
      "Iteration 193700, Train Loss: 2.4997, Validation Loss: 2.5841\n",
      "Iteration 193800, Train Loss: 2.7067, Validation Loss: 2.1950\n",
      "Iteration 193900, Train Loss: 2.4435, Validation Loss: 2.7160\n",
      "Iteration 194000, Train Loss: 2.6054, Validation Loss: 2.4618\n",
      "Iteration 194100, Train Loss: 2.3579, Validation Loss: 2.4990\n",
      "Iteration 194200, Train Loss: 2.2967, Validation Loss: 2.8734\n",
      "Iteration 194300, Train Loss: 2.5662, Validation Loss: 2.1081\n",
      "Iteration 194400, Train Loss: 2.0946, Validation Loss: 2.3351\n",
      "Iteration 194500, Train Loss: 2.3228, Validation Loss: 2.3267\n",
      "Iteration 194600, Train Loss: 2.1027, Validation Loss: 2.0814\n",
      "Iteration 194700, Train Loss: 2.1887, Validation Loss: 2.1617\n",
      "Iteration 194800, Train Loss: 2.8178, Validation Loss: 2.4372\n",
      "Iteration 194900, Train Loss: 2.3275, Validation Loss: 2.3015\n",
      "Iteration 195000, Train Loss: 2.5500, Validation Loss: 2.5721\n",
      "Iteration 195100, Train Loss: 2.5375, Validation Loss: 2.6152\n",
      "Iteration 195200, Train Loss: 2.2537, Validation Loss: 2.6416\n",
      "Iteration 195300, Train Loss: 1.9136, Validation Loss: 2.1916\n",
      "Iteration 195400, Train Loss: 2.0091, Validation Loss: 2.0898\n",
      "Iteration 195500, Train Loss: 2.3326, Validation Loss: 1.9427\n",
      "Iteration 195600, Train Loss: 2.2174, Validation Loss: 2.7309\n",
      "Iteration 195700, Train Loss: 2.6136, Validation Loss: 2.5052\n",
      "Iteration 195800, Train Loss: 2.2745, Validation Loss: 2.4332\n",
      "Iteration 195900, Train Loss: 2.4618, Validation Loss: 2.8259\n",
      "Iteration 196000, Train Loss: 1.9810, Validation Loss: 2.0154\n",
      "Iteration 196100, Train Loss: 2.6250, Validation Loss: 2.1506\n",
      "Iteration 196200, Train Loss: 3.0602, Validation Loss: 2.2891\n",
      "Iteration 196300, Train Loss: 2.7631, Validation Loss: 2.5633\n",
      "Iteration 196400, Train Loss: 2.0826, Validation Loss: 2.4855\n",
      "Iteration 196500, Train Loss: 2.4473, Validation Loss: 2.2326\n",
      "Iteration 196600, Train Loss: 2.1583, Validation Loss: 2.2914\n",
      "Iteration 196700, Train Loss: 2.4017, Validation Loss: 1.9373\n",
      "Iteration 196800, Train Loss: 2.1202, Validation Loss: 2.3480\n",
      "Iteration 196900, Train Loss: 2.9192, Validation Loss: 2.1856\n",
      "Iteration 197000, Train Loss: 2.0405, Validation Loss: 2.3520\n",
      "Iteration 197100, Train Loss: 2.2494, Validation Loss: 2.4447\n",
      "Iteration 197200, Train Loss: 2.5998, Validation Loss: 2.4484\n",
      "Iteration 197300, Train Loss: 2.3579, Validation Loss: 2.7653\n",
      "Iteration 197400, Train Loss: 2.2062, Validation Loss: 2.1751\n",
      "Iteration 197500, Train Loss: 2.2770, Validation Loss: 2.1288\n",
      "Iteration 197600, Train Loss: 2.2628, Validation Loss: 2.7361\n",
      "Iteration 197700, Train Loss: 2.5337, Validation Loss: 2.1344\n",
      "Iteration 197800, Train Loss: 2.5784, Validation Loss: 2.6010\n",
      "Iteration 197900, Train Loss: 2.0926, Validation Loss: 2.1352\n",
      "Iteration 198000, Train Loss: 2.1159, Validation Loss: 2.2686\n",
      "Iteration 198100, Train Loss: 2.2387, Validation Loss: 2.3468\n",
      "Iteration 198200, Train Loss: 2.5641, Validation Loss: 2.3011\n",
      "Iteration 198300, Train Loss: 2.6964, Validation Loss: 2.5069\n",
      "Iteration 198400, Train Loss: 1.9230, Validation Loss: 2.8157\n",
      "Iteration 198500, Train Loss: 2.4679, Validation Loss: 1.9156\n",
      "Iteration 198600, Train Loss: 2.2074, Validation Loss: 2.5897\n",
      "Iteration 198700, Train Loss: 2.2743, Validation Loss: 2.2494\n",
      "Iteration 198800, Train Loss: 2.2608, Validation Loss: 2.5751\n",
      "Iteration 198900, Train Loss: 2.4439, Validation Loss: 2.4338\n",
      "Iteration 199000, Train Loss: 2.1442, Validation Loss: 2.0382\n",
      "Iteration 199100, Train Loss: 2.5565, Validation Loss: 2.0670\n",
      "Iteration 199200, Train Loss: 2.2077, Validation Loss: 2.4916\n",
      "Iteration 199300, Train Loss: 2.0444, Validation Loss: 2.1617\n",
      "Iteration 199400, Train Loss: 2.4778, Validation Loss: 2.1458\n",
      "Iteration 199500, Train Loss: 2.2899, Validation Loss: 2.5559\n",
      "Iteration 199600, Train Loss: 2.3689, Validation Loss: 2.3946\n",
      "Iteration 199700, Train Loss: 2.0643, Validation Loss: 2.6030\n",
      "Iteration 199800, Train Loss: 2.2130, Validation Loss: 2.4365\n",
      "Iteration 199900, Train Loss: 2.3308, Validation Loss: 2.3820\n",
      "Iteration 200000, Train Loss: 2.6021, Validation Loss: 2.2514\n",
      "Iteration 200100, Train Loss: 2.3047, Validation Loss: 2.1722\n",
      "Iteration 200200, Train Loss: 2.3077, Validation Loss: 2.3761\n",
      "Iteration 200300, Train Loss: 2.2432, Validation Loss: 1.8495\n",
      "Iteration 200400, Train Loss: 2.3296, Validation Loss: 2.4825\n",
      "Iteration 200500, Train Loss: 2.5021, Validation Loss: 2.6293\n",
      "Iteration 200600, Train Loss: 2.6424, Validation Loss: 2.4067\n",
      "Iteration 200700, Train Loss: 2.4122, Validation Loss: 2.1695\n",
      "Iteration 200800, Train Loss: 2.4633, Validation Loss: 2.4460\n",
      "Iteration 200900, Train Loss: 1.8701, Validation Loss: 2.5402\n",
      "Iteration 201000, Train Loss: 2.5905, Validation Loss: 2.6540\n",
      "Iteration 201100, Train Loss: 2.2893, Validation Loss: 2.3816\n",
      "Iteration 201200, Train Loss: 1.9220, Validation Loss: 2.7138\n",
      "Iteration 201300, Train Loss: 2.4279, Validation Loss: 2.5415\n",
      "Iteration 201400, Train Loss: 2.3542, Validation Loss: 2.3564\n",
      "Iteration 201500, Train Loss: 2.7531, Validation Loss: 2.7971\n",
      "Iteration 201600, Train Loss: 2.1979, Validation Loss: 2.2470\n",
      "Iteration 201700, Train Loss: 2.2937, Validation Loss: 2.6856\n",
      "Iteration 201800, Train Loss: 2.2564, Validation Loss: 2.1163\n",
      "Iteration 201900, Train Loss: 2.5601, Validation Loss: 2.3007\n",
      "Iteration 202000, Train Loss: 2.3621, Validation Loss: 2.4858\n",
      "Iteration 202100, Train Loss: 2.6409, Validation Loss: 2.3014\n",
      "Iteration 202200, Train Loss: 2.4908, Validation Loss: 2.8411\n",
      "Iteration 202300, Train Loss: 2.5816, Validation Loss: 2.0716\n",
      "Iteration 202400, Train Loss: 2.0181, Validation Loss: 2.2985\n",
      "Iteration 202500, Train Loss: 2.4855, Validation Loss: 2.7410\n",
      "Iteration 202600, Train Loss: 2.2227, Validation Loss: 2.4159\n",
      "Iteration 202700, Train Loss: 2.0915, Validation Loss: 2.3962\n",
      "Iteration 202800, Train Loss: 2.3153, Validation Loss: 2.6860\n",
      "Iteration 202900, Train Loss: 2.1276, Validation Loss: 2.1082\n",
      "Iteration 203000, Train Loss: 2.3494, Validation Loss: 2.3849\n",
      "Iteration 203100, Train Loss: 2.6802, Validation Loss: 2.0249\n",
      "Iteration 203200, Train Loss: 1.9219, Validation Loss: 2.7439\n",
      "Iteration 203300, Train Loss: 2.7664, Validation Loss: 2.5132\n",
      "Iteration 203400, Train Loss: 2.2744, Validation Loss: 2.2503\n",
      "Iteration 203500, Train Loss: 2.1035, Validation Loss: 2.3721\n",
      "Iteration 203600, Train Loss: 2.1850, Validation Loss: 2.5324\n",
      "Iteration 203700, Train Loss: 2.3629, Validation Loss: 2.7930\n",
      "Iteration 203800, Train Loss: 2.6589, Validation Loss: 1.9967\n",
      "Iteration 203900, Train Loss: 2.0885, Validation Loss: 2.1939\n",
      "Iteration 204000, Train Loss: 2.2261, Validation Loss: 2.2496\n",
      "Iteration 204100, Train Loss: 2.0898, Validation Loss: 2.3538\n",
      "Iteration 204200, Train Loss: 2.3626, Validation Loss: 2.4484\n",
      "Iteration 204300, Train Loss: 2.3327, Validation Loss: 2.2870\n",
      "Iteration 204400, Train Loss: 2.1956, Validation Loss: 2.6530\n",
      "Iteration 204500, Train Loss: 2.0621, Validation Loss: 2.2680\n",
      "Iteration 204600, Train Loss: 1.9725, Validation Loss: 2.3485\n",
      "Iteration 204700, Train Loss: 2.6606, Validation Loss: 2.6104\n",
      "Iteration 204800, Train Loss: 2.5618, Validation Loss: 2.2416\n",
      "Iteration 204900, Train Loss: 2.4089, Validation Loss: 2.4045\n",
      "Iteration 205000, Train Loss: 2.6340, Validation Loss: 2.2017\n",
      "Iteration 205100, Train Loss: 2.3052, Validation Loss: 2.3230\n",
      "Iteration 205200, Train Loss: 2.1206, Validation Loss: 1.9637\n",
      "Iteration 205300, Train Loss: 2.4315, Validation Loss: 2.1489\n",
      "Iteration 205400, Train Loss: 2.1788, Validation Loss: 2.3913\n",
      "Iteration 205500, Train Loss: 2.3730, Validation Loss: 2.1130\n",
      "Iteration 205600, Train Loss: 2.4557, Validation Loss: 2.0884\n",
      "Iteration 205700, Train Loss: 2.2797, Validation Loss: 2.1171\n",
      "Iteration 205800, Train Loss: 2.0234, Validation Loss: 2.4595\n",
      "Iteration 205900, Train Loss: 2.2257, Validation Loss: 2.5044\n",
      "Iteration 206000, Train Loss: 2.5874, Validation Loss: 2.7148\n",
      "Iteration 206100, Train Loss: 2.5507, Validation Loss: 2.3938\n",
      "Iteration 206200, Train Loss: 2.6418, Validation Loss: 2.2649\n",
      "Iteration 206300, Train Loss: 2.1432, Validation Loss: 2.2829\n",
      "Iteration 206400, Train Loss: 2.5051, Validation Loss: 2.2951\n",
      "Iteration 206500, Train Loss: 2.4504, Validation Loss: 2.5302\n",
      "Iteration 206600, Train Loss: 1.9720, Validation Loss: 2.4007\n",
      "Iteration 206700, Train Loss: 2.3829, Validation Loss: 2.4715\n",
      "Iteration 206800, Train Loss: 2.3178, Validation Loss: 2.6540\n",
      "Iteration 206900, Train Loss: 2.2424, Validation Loss: 2.1485\n",
      "Iteration 207000, Train Loss: 1.7170, Validation Loss: 2.1198\n",
      "Iteration 207100, Train Loss: 2.0416, Validation Loss: 2.2806\n",
      "Iteration 207200, Train Loss: 1.9941, Validation Loss: 2.5200\n",
      "Iteration 207300, Train Loss: 2.4670, Validation Loss: 2.7090\n",
      "Iteration 207400, Train Loss: 2.3158, Validation Loss: 2.3543\n",
      "Iteration 207500, Train Loss: 2.6288, Validation Loss: 1.7038\n",
      "Iteration 207600, Train Loss: 2.2060, Validation Loss: 2.5738\n",
      "Iteration 207700, Train Loss: 2.2276, Validation Loss: 2.3802\n",
      "Iteration 207800, Train Loss: 2.5093, Validation Loss: 2.3966\n",
      "Iteration 207900, Train Loss: 2.1144, Validation Loss: 1.8368\n",
      "Iteration 208000, Train Loss: 2.2847, Validation Loss: 2.3444\n",
      "Iteration 208100, Train Loss: 2.0010, Validation Loss: 2.1760\n",
      "Iteration 208200, Train Loss: 2.0669, Validation Loss: 2.3284\n",
      "Iteration 208300, Train Loss: 2.3948, Validation Loss: 2.3790\n",
      "Iteration 208400, Train Loss: 1.8485, Validation Loss: 1.9936\n",
      "Iteration 208500, Train Loss: 2.2415, Validation Loss: 2.3590\n",
      "Iteration 208600, Train Loss: 2.3703, Validation Loss: 2.7136\n",
      "Iteration 208700, Train Loss: 2.2083, Validation Loss: 2.4091\n",
      "Iteration 208800, Train Loss: 2.4324, Validation Loss: 2.1467\n",
      "Iteration 208900, Train Loss: 1.8106, Validation Loss: 2.2519\n",
      "Iteration 209000, Train Loss: 2.2163, Validation Loss: 2.3751\n",
      "Iteration 209100, Train Loss: 2.2054, Validation Loss: 2.3131\n",
      "Iteration 209200, Train Loss: 2.7366, Validation Loss: 2.1227\n",
      "Iteration 209300, Train Loss: 2.2979, Validation Loss: 2.0960\n",
      "Iteration 209400, Train Loss: 2.5329, Validation Loss: 2.7860\n",
      "Iteration 209500, Train Loss: 2.2038, Validation Loss: 2.6197\n",
      "Iteration 209600, Train Loss: 2.2627, Validation Loss: 2.4324\n",
      "Iteration 209700, Train Loss: 2.3640, Validation Loss: 2.4445\n",
      "Iteration 209800, Train Loss: 2.2640, Validation Loss: 2.1005\n",
      "Iteration 209900, Train Loss: 2.4687, Validation Loss: 2.3549\n",
      "Iteration 210000, Train Loss: 2.2272, Validation Loss: 2.0381\n",
      "Iteration 210100, Train Loss: 2.5003, Validation Loss: 2.1373\n",
      "Iteration 210200, Train Loss: 2.2888, Validation Loss: 2.7407\n",
      "Iteration 210300, Train Loss: 2.3486, Validation Loss: 2.4875\n",
      "Iteration 210400, Train Loss: 2.3989, Validation Loss: 2.2504\n",
      "Iteration 210500, Train Loss: 2.4095, Validation Loss: 2.5531\n",
      "Iteration 210600, Train Loss: 2.1031, Validation Loss: 2.2296\n",
      "Iteration 210700, Train Loss: 2.3020, Validation Loss: 2.4474\n",
      "Iteration 210800, Train Loss: 2.0950, Validation Loss: 2.6764\n",
      "Iteration 210900, Train Loss: 2.3475, Validation Loss: 2.6832\n",
      "Iteration 211000, Train Loss: 2.6153, Validation Loss: 2.4134\n",
      "Iteration 211100, Train Loss: 2.2553, Validation Loss: 2.2456\n",
      "Iteration 211200, Train Loss: 1.8984, Validation Loss: 2.5339\n",
      "Iteration 211300, Train Loss: 2.4628, Validation Loss: 2.6687\n",
      "Iteration 211400, Train Loss: 2.4303, Validation Loss: 2.6704\n",
      "Iteration 211500, Train Loss: 1.9539, Validation Loss: 1.8910\n",
      "Iteration 211600, Train Loss: 2.3095, Validation Loss: 2.0982\n",
      "Iteration 211700, Train Loss: 2.3127, Validation Loss: 2.7658\n",
      "Iteration 211800, Train Loss: 2.6427, Validation Loss: 2.3792\n",
      "Iteration 211900, Train Loss: 2.5390, Validation Loss: 2.7154\n",
      "Iteration 212000, Train Loss: 2.5177, Validation Loss: 2.5081\n",
      "Iteration 212100, Train Loss: 2.7467, Validation Loss: 2.8628\n",
      "Iteration 212200, Train Loss: 2.6547, Validation Loss: 2.3053\n",
      "Iteration 212300, Train Loss: 2.4348, Validation Loss: 2.5540\n",
      "Iteration 212400, Train Loss: 2.3208, Validation Loss: 2.2867\n",
      "Iteration 212500, Train Loss: 2.2396, Validation Loss: 2.1974\n",
      "Iteration 212600, Train Loss: 2.2425, Validation Loss: 2.5212\n",
      "Iteration 212700, Train Loss: 2.7051, Validation Loss: 2.1252\n",
      "Iteration 212800, Train Loss: 2.0679, Validation Loss: 2.6897\n",
      "Iteration 212900, Train Loss: 2.0906, Validation Loss: 2.2317\n",
      "Iteration 213000, Train Loss: 2.9131, Validation Loss: 2.1786\n",
      "Iteration 213100, Train Loss: 2.1271, Validation Loss: 2.0506\n",
      "Iteration 213200, Train Loss: 2.4231, Validation Loss: 2.4158\n",
      "Iteration 213300, Train Loss: 2.6556, Validation Loss: 2.3173\n",
      "Iteration 213400, Train Loss: 2.6306, Validation Loss: 2.5086\n",
      "Iteration 213500, Train Loss: 2.2155, Validation Loss: 1.7280\n",
      "Iteration 213600, Train Loss: 2.5240, Validation Loss: 2.1751\n",
      "Iteration 213700, Train Loss: 2.7939, Validation Loss: 2.4114\n",
      "Iteration 213800, Train Loss: 2.3791, Validation Loss: 2.4227\n",
      "Iteration 213900, Train Loss: 2.1957, Validation Loss: 2.1914\n",
      "Iteration 214000, Train Loss: 2.3133, Validation Loss: 2.1556\n",
      "Iteration 214100, Train Loss: 2.3100, Validation Loss: 2.8975\n",
      "Iteration 214200, Train Loss: 2.4906, Validation Loss: 2.5074\n",
      "Iteration 214300, Train Loss: 2.5633, Validation Loss: 2.4239\n",
      "Iteration 214400, Train Loss: 2.6540, Validation Loss: 2.2689\n",
      "Iteration 214500, Train Loss: 2.4321, Validation Loss: 2.3627\n",
      "Iteration 214600, Train Loss: 2.3397, Validation Loss: 2.1310\n",
      "Iteration 214700, Train Loss: 2.0255, Validation Loss: 2.4563\n",
      "Iteration 214800, Train Loss: 2.5556, Validation Loss: 1.7898\n",
      "Iteration 214900, Train Loss: 2.5312, Validation Loss: 2.0567\n",
      "Iteration 215000, Train Loss: 2.6004, Validation Loss: 2.7748\n",
      "Iteration 215100, Train Loss: 2.4957, Validation Loss: 2.8885\n",
      "Iteration 215200, Train Loss: 2.6493, Validation Loss: 2.1351\n",
      "Iteration 215300, Train Loss: 2.1638, Validation Loss: 2.4776\n",
      "Iteration 215400, Train Loss: 2.6162, Validation Loss: 2.0819\n",
      "Iteration 215500, Train Loss: 2.4052, Validation Loss: 2.6025\n",
      "Iteration 215600, Train Loss: 2.2017, Validation Loss: 2.4804\n",
      "Iteration 215700, Train Loss: 2.4579, Validation Loss: 2.3650\n",
      "Iteration 215800, Train Loss: 2.4525, Validation Loss: 2.4651\n",
      "Iteration 215900, Train Loss: 2.3348, Validation Loss: 2.3343\n",
      "Iteration 216000, Train Loss: 2.2096, Validation Loss: 2.0582\n",
      "Iteration 216100, Train Loss: 2.5102, Validation Loss: 2.2777\n",
      "Iteration 216200, Train Loss: 2.1147, Validation Loss: 2.1898\n",
      "Iteration 216300, Train Loss: 2.1885, Validation Loss: 1.9964\n",
      "Iteration 216400, Train Loss: 2.8742, Validation Loss: 2.2831\n",
      "Iteration 216500, Train Loss: 2.3160, Validation Loss: 2.4776\n",
      "Iteration 216600, Train Loss: 2.0589, Validation Loss: 2.4342\n",
      "Iteration 216700, Train Loss: 2.3415, Validation Loss: 2.7097\n",
      "Iteration 216800, Train Loss: 2.2359, Validation Loss: 2.4093\n",
      "Iteration 216900, Train Loss: 2.3319, Validation Loss: 2.5643\n",
      "Iteration 217000, Train Loss: 2.4129, Validation Loss: 2.3831\n",
      "Iteration 217100, Train Loss: 2.2120, Validation Loss: 2.4897\n",
      "Iteration 217200, Train Loss: 2.5075, Validation Loss: 2.4590\n",
      "Iteration 217300, Train Loss: 2.6234, Validation Loss: 2.1799\n",
      "Iteration 217400, Train Loss: 2.3562, Validation Loss: 2.0519\n",
      "Iteration 217500, Train Loss: 1.9602, Validation Loss: 2.6912\n",
      "Iteration 217600, Train Loss: 2.3251, Validation Loss: 2.0817\n",
      "Iteration 217700, Train Loss: 2.2645, Validation Loss: 2.1391\n",
      "Iteration 217800, Train Loss: 2.1581, Validation Loss: 2.6473\n",
      "Iteration 217900, Train Loss: 2.7003, Validation Loss: 2.2081\n",
      "Iteration 218000, Train Loss: 2.3261, Validation Loss: 2.7700\n",
      "Iteration 218100, Train Loss: 2.4369, Validation Loss: 2.5135\n",
      "Iteration 218200, Train Loss: 1.7878, Validation Loss: 2.4971\n",
      "Iteration 218300, Train Loss: 1.9466, Validation Loss: 2.1483\n",
      "Iteration 218400, Train Loss: 2.8067, Validation Loss: 2.6342\n",
      "Iteration 218500, Train Loss: 2.4299, Validation Loss: 2.2294\n",
      "Iteration 218600, Train Loss: 2.4878, Validation Loss: 2.7646\n",
      "Iteration 218700, Train Loss: 2.1700, Validation Loss: 2.0172\n",
      "Iteration 218800, Train Loss: 2.2856, Validation Loss: 1.9825\n",
      "Iteration 218900, Train Loss: 2.0188, Validation Loss: 2.7457\n",
      "Iteration 219000, Train Loss: 2.2297, Validation Loss: 2.2011\n",
      "Iteration 219100, Train Loss: 2.4193, Validation Loss: 2.4465\n",
      "Iteration 219200, Train Loss: 2.1572, Validation Loss: 2.1526\n",
      "Iteration 219300, Train Loss: 2.2298, Validation Loss: 2.3340\n",
      "Iteration 219400, Train Loss: 2.1086, Validation Loss: 2.9009\n",
      "Iteration 219500, Train Loss: 1.8461, Validation Loss: 2.6588\n",
      "Iteration 219600, Train Loss: 2.1927, Validation Loss: 2.1201\n",
      "Iteration 219700, Train Loss: 2.1813, Validation Loss: 2.2732\n",
      "Iteration 219800, Train Loss: 2.0967, Validation Loss: 2.2311\n",
      "Iteration 219900, Train Loss: 2.2461, Validation Loss: 2.5809\n",
      "Iteration 220000, Train Loss: 2.5607, Validation Loss: 1.9654\n",
      "Iteration 220100, Train Loss: 2.4416, Validation Loss: 2.5259\n",
      "Iteration 220200, Train Loss: 2.3515, Validation Loss: 2.1566\n",
      "Iteration 220300, Train Loss: 2.5461, Validation Loss: 2.4638\n",
      "Iteration 220400, Train Loss: 2.8225, Validation Loss: 2.2797\n",
      "Iteration 220500, Train Loss: 2.1374, Validation Loss: 2.3322\n",
      "Iteration 220600, Train Loss: 2.2231, Validation Loss: 2.5918\n",
      "Iteration 220700, Train Loss: 2.3102, Validation Loss: 2.5551\n",
      "Iteration 220800, Train Loss: 2.0082, Validation Loss: 2.5375\n",
      "Iteration 220900, Train Loss: 1.8632, Validation Loss: 2.1960\n",
      "Iteration 221000, Train Loss: 2.1864, Validation Loss: 2.8062\n",
      "Iteration 221100, Train Loss: 2.1343, Validation Loss: 2.2189\n",
      "Iteration 221200, Train Loss: 2.2505, Validation Loss: 2.2908\n",
      "Iteration 221300, Train Loss: 2.1139, Validation Loss: 2.2636\n",
      "Iteration 221400, Train Loss: 2.5235, Validation Loss: 2.2617\n",
      "Iteration 221500, Train Loss: 2.1918, Validation Loss: 2.2059\n",
      "Iteration 221600, Train Loss: 2.4167, Validation Loss: 2.2487\n",
      "Iteration 221700, Train Loss: 2.7068, Validation Loss: 3.0785\n",
      "Iteration 221800, Train Loss: 2.4499, Validation Loss: 2.1618\n",
      "Iteration 221900, Train Loss: 2.0268, Validation Loss: 2.8018\n",
      "Iteration 222000, Train Loss: 2.3185, Validation Loss: 2.5260\n",
      "Iteration 222100, Train Loss: 2.0193, Validation Loss: 2.5230\n",
      "Iteration 222200, Train Loss: 2.4858, Validation Loss: 2.4006\n",
      "Iteration 222300, Train Loss: 2.2091, Validation Loss: 2.4850\n",
      "Iteration 222400, Train Loss: 1.9611, Validation Loss: 2.0576\n",
      "Iteration 222500, Train Loss: 2.2519, Validation Loss: 2.3525\n",
      "Iteration 222600, Train Loss: 2.2537, Validation Loss: 2.2654\n",
      "Iteration 222700, Train Loss: 2.3681, Validation Loss: 2.2750\n",
      "Iteration 222800, Train Loss: 2.6448, Validation Loss: 2.2212\n",
      "Iteration 222900, Train Loss: 2.8466, Validation Loss: 2.0361\n",
      "Iteration 223000, Train Loss: 2.3196, Validation Loss: 2.0895\n",
      "Iteration 223100, Train Loss: 2.7460, Validation Loss: 2.4676\n",
      "Iteration 223200, Train Loss: 2.3496, Validation Loss: 2.3658\n",
      "Iteration 223300, Train Loss: 2.2370, Validation Loss: 2.6451\n",
      "Iteration 223400, Train Loss: 2.0108, Validation Loss: 1.9751\n",
      "Iteration 223500, Train Loss: 2.1226, Validation Loss: 2.0404\n",
      "Iteration 223600, Train Loss: 2.4980, Validation Loss: 2.3599\n",
      "Iteration 223700, Train Loss: 2.8817, Validation Loss: 2.2805\n",
      "Iteration 223800, Train Loss: 2.0672, Validation Loss: 2.3331\n",
      "Iteration 223900, Train Loss: 2.1549, Validation Loss: 2.4250\n",
      "Iteration 224000, Train Loss: 2.2616, Validation Loss: 2.4147\n",
      "Iteration 224100, Train Loss: 2.1970, Validation Loss: 2.1538\n",
      "Iteration 224200, Train Loss: 2.6570, Validation Loss: 2.3906\n",
      "Iteration 224300, Train Loss: 2.0549, Validation Loss: 2.0563\n",
      "Iteration 224400, Train Loss: 2.1762, Validation Loss: 2.4124\n",
      "Iteration 224500, Train Loss: 2.2113, Validation Loss: 2.4918\n",
      "Iteration 224600, Train Loss: 2.7323, Validation Loss: 2.2129\n",
      "Iteration 224700, Train Loss: 2.7014, Validation Loss: 2.5542\n",
      "Iteration 224800, Train Loss: 2.5509, Validation Loss: 2.3800\n",
      "Iteration 224900, Train Loss: 2.7323, Validation Loss: 2.4401\n",
      "Iteration 225000, Train Loss: 2.4066, Validation Loss: 2.4026\n",
      "Iteration 225100, Train Loss: 2.1237, Validation Loss: 2.6598\n",
      "Iteration 225200, Train Loss: 2.1155, Validation Loss: 1.8511\n",
      "Iteration 225300, Train Loss: 2.3119, Validation Loss: 2.0498\n",
      "Iteration 225400, Train Loss: 2.1933, Validation Loss: 2.7130\n",
      "Iteration 225500, Train Loss: 2.2011, Validation Loss: 2.1622\n",
      "Iteration 225600, Train Loss: 2.4272, Validation Loss: 2.1222\n",
      "Iteration 225700, Train Loss: 2.1971, Validation Loss: 2.6025\n",
      "Iteration 225800, Train Loss: 2.3969, Validation Loss: 2.4963\n",
      "Iteration 225900, Train Loss: 2.8366, Validation Loss: 2.3249\n",
      "Iteration 226000, Train Loss: 2.1957, Validation Loss: 2.3817\n",
      "Iteration 226100, Train Loss: 2.1603, Validation Loss: 2.4165\n",
      "Iteration 226200, Train Loss: 2.6618, Validation Loss: 2.3133\n",
      "Iteration 226300, Train Loss: 1.9055, Validation Loss: 2.2025\n",
      "Iteration 226400, Train Loss: 2.0855, Validation Loss: 2.2776\n",
      "Iteration 226500, Train Loss: 2.3967, Validation Loss: 2.2836\n",
      "Iteration 226600, Train Loss: 2.6549, Validation Loss: 2.0668\n",
      "Iteration 226700, Train Loss: 2.8880, Validation Loss: 2.7353\n",
      "Iteration 226800, Train Loss: 2.2070, Validation Loss: 2.4210\n",
      "Iteration 226900, Train Loss: 2.4061, Validation Loss: 2.2110\n",
      "Iteration 227000, Train Loss: 2.0490, Validation Loss: 2.7221\n",
      "Iteration 227100, Train Loss: 2.2445, Validation Loss: 2.5007\n",
      "Iteration 227200, Train Loss: 2.3830, Validation Loss: 2.3337\n",
      "Iteration 227300, Train Loss: 1.9400, Validation Loss: 2.1345\n",
      "Iteration 227400, Train Loss: 1.8051, Validation Loss: 2.3080\n",
      "Iteration 227500, Train Loss: 2.4769, Validation Loss: 2.4047\n",
      "Iteration 227600, Train Loss: 2.3688, Validation Loss: 2.3857\n",
      "Iteration 227700, Train Loss: 2.3368, Validation Loss: 1.8552\n",
      "Iteration 227800, Train Loss: 2.2053, Validation Loss: 2.2983\n",
      "Iteration 227900, Train Loss: 2.4918, Validation Loss: 2.0819\n",
      "Iteration 228000, Train Loss: 2.1893, Validation Loss: 1.9990\n",
      "Iteration 228100, Train Loss: 1.8405, Validation Loss: 2.2566\n",
      "Iteration 228200, Train Loss: 2.2777, Validation Loss: 2.5095\n",
      "Iteration 228300, Train Loss: 2.4305, Validation Loss: 2.1077\n",
      "Iteration 228400, Train Loss: 2.4675, Validation Loss: 2.1407\n",
      "Iteration 228500, Train Loss: 2.1613, Validation Loss: 2.3199\n",
      "Iteration 228600, Train Loss: 2.4633, Validation Loss: 2.1743\n",
      "Iteration 228700, Train Loss: 2.2630, Validation Loss: 2.6659\n",
      "Iteration 228800, Train Loss: 2.3847, Validation Loss: 2.7002\n",
      "Iteration 228900, Train Loss: 2.2417, Validation Loss: 2.3442\n",
      "Iteration 229000, Train Loss: 2.2560, Validation Loss: 2.2647\n",
      "Iteration 229100, Train Loss: 2.4380, Validation Loss: 2.2380\n",
      "Iteration 229200, Train Loss: 2.2038, Validation Loss: 2.2294\n",
      "Iteration 229300, Train Loss: 2.4063, Validation Loss: 2.7811\n",
      "Iteration 229400, Train Loss: 2.0850, Validation Loss: 2.1494\n",
      "Iteration 229500, Train Loss: 2.3816, Validation Loss: 2.4737\n",
      "Iteration 229600, Train Loss: 2.3893, Validation Loss: 2.0912\n",
      "Iteration 229700, Train Loss: 2.3004, Validation Loss: 2.2429\n",
      "Iteration 229800, Train Loss: 1.9849, Validation Loss: 2.0987\n",
      "Iteration 229900, Train Loss: 2.3026, Validation Loss: 2.5093\n",
      "Iteration 230000, Train Loss: 2.1953, Validation Loss: 2.2561\n",
      "Iteration 230100, Train Loss: 2.1479, Validation Loss: 2.0920\n",
      "Iteration 230200, Train Loss: 2.2116, Validation Loss: 2.0551\n",
      "Iteration 230300, Train Loss: 2.1744, Validation Loss: 2.3827\n",
      "Iteration 230400, Train Loss: 2.3279, Validation Loss: 2.2170\n",
      "Iteration 230500, Train Loss: 2.5543, Validation Loss: 2.3611\n",
      "Iteration 230600, Train Loss: 2.2346, Validation Loss: 2.4005\n",
      "Iteration 230700, Train Loss: 2.6594, Validation Loss: 1.9568\n",
      "Iteration 230800, Train Loss: 2.4665, Validation Loss: 2.4963\n",
      "Iteration 230900, Train Loss: 2.0582, Validation Loss: 2.4029\n",
      "Iteration 231000, Train Loss: 1.8813, Validation Loss: 2.6816\n",
      "Iteration 231100, Train Loss: 2.4089, Validation Loss: 2.1942\n",
      "Iteration 231200, Train Loss: 2.5712, Validation Loss: 2.6522\n",
      "Iteration 231300, Train Loss: 2.1156, Validation Loss: 2.3479\n",
      "Iteration 231400, Train Loss: 2.6562, Validation Loss: 2.3870\n",
      "Iteration 231500, Train Loss: 2.0650, Validation Loss: 2.2606\n",
      "Iteration 231600, Train Loss: 2.3163, Validation Loss: 2.4540\n",
      "Iteration 231700, Train Loss: 2.5376, Validation Loss: 2.3508\n",
      "Iteration 231800, Train Loss: 2.5265, Validation Loss: 2.3361\n",
      "Iteration 231900, Train Loss: 2.4390, Validation Loss: 2.3281\n",
      "Iteration 232000, Train Loss: 2.1870, Validation Loss: 2.1131\n",
      "Iteration 232100, Train Loss: 2.4463, Validation Loss: 2.7951\n",
      "Iteration 232200, Train Loss: 2.0684, Validation Loss: 2.6036\n",
      "Iteration 232300, Train Loss: 2.4175, Validation Loss: 2.2911\n",
      "Iteration 232400, Train Loss: 2.3058, Validation Loss: 2.5670\n",
      "Iteration 232500, Train Loss: 2.7022, Validation Loss: 1.9747\n",
      "Iteration 232600, Train Loss: 2.3144, Validation Loss: 2.4345\n",
      "Iteration 232700, Train Loss: 2.2570, Validation Loss: 2.3818\n",
      "Iteration 232800, Train Loss: 2.3580, Validation Loss: 2.5959\n",
      "Iteration 232900, Train Loss: 2.1113, Validation Loss: 2.5900\n",
      "Iteration 233000, Train Loss: 2.2030, Validation Loss: 2.3515\n",
      "Iteration 233100, Train Loss: 2.6121, Validation Loss: 2.5899\n",
      "Iteration 233200, Train Loss: 2.2700, Validation Loss: 2.3327\n",
      "Iteration 233300, Train Loss: 2.5800, Validation Loss: 2.5283\n",
      "Iteration 233400, Train Loss: 2.0689, Validation Loss: 2.1151\n",
      "Iteration 233500, Train Loss: 2.5407, Validation Loss: 2.6095\n",
      "Iteration 233600, Train Loss: 2.2765, Validation Loss: 2.7994\n",
      "Iteration 233700, Train Loss: 2.5275, Validation Loss: 2.6693\n",
      "Iteration 233800, Train Loss: 2.3602, Validation Loss: 2.5258\n",
      "Iteration 233900, Train Loss: 2.5491, Validation Loss: 2.4213\n",
      "Iteration 234000, Train Loss: 2.2822, Validation Loss: 2.4095\n",
      "Iteration 234100, Train Loss: 2.1795, Validation Loss: 2.4574\n",
      "Iteration 234200, Train Loss: 2.1539, Validation Loss: 2.2564\n",
      "Iteration 234300, Train Loss: 2.4608, Validation Loss: 2.4244\n",
      "Iteration 234400, Train Loss: 2.3404, Validation Loss: 2.3762\n",
      "Iteration 234500, Train Loss: 2.2567, Validation Loss: 2.6069\n",
      "Iteration 234600, Train Loss: 2.4014, Validation Loss: 2.5873\n",
      "Iteration 234700, Train Loss: 2.0544, Validation Loss: 2.3433\n",
      "Iteration 234800, Train Loss: 2.3875, Validation Loss: 2.0753\n",
      "Iteration 234900, Train Loss: 2.0099, Validation Loss: 2.5183\n",
      "Iteration 235000, Train Loss: 2.2072, Validation Loss: 2.1750\n",
      "Iteration 235100, Train Loss: 2.4022, Validation Loss: 2.5383\n",
      "Iteration 235200, Train Loss: 2.2250, Validation Loss: 2.5505\n",
      "Iteration 235300, Train Loss: 2.1372, Validation Loss: 2.4392\n",
      "Iteration 235400, Train Loss: 2.3621, Validation Loss: 1.9991\n",
      "Iteration 235500, Train Loss: 2.2185, Validation Loss: 2.1195\n",
      "Iteration 235600, Train Loss: 2.1634, Validation Loss: 2.2412\n",
      "Iteration 235700, Train Loss: 2.1161, Validation Loss: 2.2815\n",
      "Iteration 235800, Train Loss: 2.3916, Validation Loss: 2.7088\n",
      "Iteration 235900, Train Loss: 2.1666, Validation Loss: 2.3180\n",
      "Iteration 236000, Train Loss: 2.3214, Validation Loss: 2.0853\n",
      "Iteration 236100, Train Loss: 2.1772, Validation Loss: 2.3370\n",
      "Iteration 236200, Train Loss: 2.1175, Validation Loss: 2.1572\n",
      "Iteration 236300, Train Loss: 2.5733, Validation Loss: 2.4689\n",
      "Iteration 236400, Train Loss: 2.5787, Validation Loss: 2.1913\n",
      "Iteration 236500, Train Loss: 2.0637, Validation Loss: 2.5755\n",
      "Iteration 236600, Train Loss: 2.1656, Validation Loss: 2.3831\n",
      "Iteration 236700, Train Loss: 2.3181, Validation Loss: 2.3989\n",
      "Iteration 236800, Train Loss: 2.1787, Validation Loss: 2.4387\n",
      "Iteration 236900, Train Loss: 2.2199, Validation Loss: 2.2371\n",
      "Iteration 237000, Train Loss: 2.2986, Validation Loss: 2.5416\n",
      "Iteration 237100, Train Loss: 2.4429, Validation Loss: 2.1817\n",
      "Iteration 237200, Train Loss: 2.3882, Validation Loss: 2.2624\n",
      "Iteration 237300, Train Loss: 2.2170, Validation Loss: 2.5264\n",
      "Iteration 237400, Train Loss: 2.0320, Validation Loss: 2.2653\n",
      "Iteration 237500, Train Loss: 2.5698, Validation Loss: 2.5953\n",
      "Iteration 237600, Train Loss: 2.0715, Validation Loss: 2.3267\n",
      "Iteration 237700, Train Loss: 2.3635, Validation Loss: 2.0492\n",
      "Iteration 237800, Train Loss: 2.1423, Validation Loss: 2.6533\n",
      "Iteration 237900, Train Loss: 1.7623, Validation Loss: 2.2608\n",
      "Iteration 238000, Train Loss: 2.1280, Validation Loss: 2.3224\n",
      "Iteration 238100, Train Loss: 2.1113, Validation Loss: 1.7371\n",
      "Iteration 238200, Train Loss: 2.0007, Validation Loss: 2.1527\n",
      "Iteration 238300, Train Loss: 1.7881, Validation Loss: 2.4654\n",
      "Iteration 238400, Train Loss: 2.6691, Validation Loss: 2.3111\n",
      "Iteration 238500, Train Loss: 2.3323, Validation Loss: 2.5162\n",
      "Iteration 238600, Train Loss: 2.2752, Validation Loss: 2.1788\n",
      "Iteration 238700, Train Loss: 2.5451, Validation Loss: 2.3502\n",
      "Iteration 238800, Train Loss: 2.6905, Validation Loss: 2.3985\n",
      "Iteration 238900, Train Loss: 2.5670, Validation Loss: 2.5288\n",
      "Iteration 239000, Train Loss: 2.2398, Validation Loss: 2.3184\n",
      "Iteration 239100, Train Loss: 2.0788, Validation Loss: 2.3533\n",
      "Iteration 239200, Train Loss: 2.8113, Validation Loss: 2.4974\n",
      "Iteration 239300, Train Loss: 2.5949, Validation Loss: 2.0580\n",
      "Iteration 239400, Train Loss: 2.6891, Validation Loss: 2.1974\n",
      "Iteration 239500, Train Loss: 2.2153, Validation Loss: 2.3578\n",
      "Iteration 239600, Train Loss: 2.2442, Validation Loss: 2.8465\n",
      "Iteration 239700, Train Loss: 2.3682, Validation Loss: 2.1695\n",
      "Iteration 239800, Train Loss: 2.3211, Validation Loss: 1.7698\n",
      "Iteration 239900, Train Loss: 2.1077, Validation Loss: 2.2698\n",
      "Iteration 240000, Train Loss: 2.1478, Validation Loss: 2.4781\n",
      "Iteration 240100, Train Loss: 2.5131, Validation Loss: 2.5400\n",
      "Iteration 240200, Train Loss: 2.3287, Validation Loss: 2.6476\n",
      "Iteration 240300, Train Loss: 2.2005, Validation Loss: 2.2988\n",
      "Iteration 240400, Train Loss: 2.3224, Validation Loss: 2.4428\n",
      "Iteration 240500, Train Loss: 2.1843, Validation Loss: 2.2684\n",
      "Iteration 240600, Train Loss: 2.0725, Validation Loss: 2.1184\n",
      "Iteration 240700, Train Loss: 2.9959, Validation Loss: 2.0194\n",
      "Iteration 240800, Train Loss: 2.3684, Validation Loss: 2.2432\n",
      "Iteration 240900, Train Loss: 2.2761, Validation Loss: 2.4787\n",
      "Iteration 241000, Train Loss: 2.2120, Validation Loss: 2.3324\n",
      "Iteration 241100, Train Loss: 1.8144, Validation Loss: 2.2702\n",
      "Iteration 241200, Train Loss: 2.5155, Validation Loss: 2.5752\n",
      "Iteration 241300, Train Loss: 2.2973, Validation Loss: 2.3728\n",
      "Iteration 241400, Train Loss: 2.2638, Validation Loss: 2.1226\n",
      "Iteration 241500, Train Loss: 2.6922, Validation Loss: 2.0609\n",
      "Iteration 241600, Train Loss: 2.5739, Validation Loss: 2.4068\n",
      "Iteration 241700, Train Loss: 2.1151, Validation Loss: 2.3040\n",
      "Iteration 241800, Train Loss: 2.3694, Validation Loss: 2.2554\n",
      "Iteration 241900, Train Loss: 2.1331, Validation Loss: 2.0582\n",
      "Iteration 242000, Train Loss: 2.4953, Validation Loss: 2.3282\n",
      "Iteration 242100, Train Loss: 2.7167, Validation Loss: 2.0432\n",
      "Iteration 242200, Train Loss: 2.3929, Validation Loss: 2.4225\n",
      "Iteration 242300, Train Loss: 2.1854, Validation Loss: 2.3807\n",
      "Iteration 242400, Train Loss: 2.3175, Validation Loss: 2.3066\n",
      "Iteration 242500, Train Loss: 2.0453, Validation Loss: 1.9347\n",
      "Iteration 242600, Train Loss: 2.3496, Validation Loss: 2.3860\n",
      "Iteration 242700, Train Loss: 2.3035, Validation Loss: 2.2496\n",
      "Iteration 242800, Train Loss: 2.0880, Validation Loss: 2.5135\n",
      "Iteration 242900, Train Loss: 2.5769, Validation Loss: 2.0598\n",
      "Iteration 243000, Train Loss: 2.2396, Validation Loss: 2.4664\n",
      "Iteration 243100, Train Loss: 2.5898, Validation Loss: 2.6649\n",
      "Iteration 243200, Train Loss: 2.1454, Validation Loss: 2.3462\n",
      "Iteration 243300, Train Loss: 2.3631, Validation Loss: 2.2519\n",
      "Iteration 243400, Train Loss: 2.3728, Validation Loss: 2.2359\n",
      "Iteration 243500, Train Loss: 1.9524, Validation Loss: 2.0851\n",
      "Iteration 243600, Train Loss: 2.3577, Validation Loss: 2.2520\n",
      "Iteration 243700, Train Loss: 2.2660, Validation Loss: 2.6486\n",
      "Iteration 243800, Train Loss: 2.2170, Validation Loss: 1.8629\n",
      "Iteration 243900, Train Loss: 2.4252, Validation Loss: 2.3069\n",
      "Iteration 244000, Train Loss: 2.5465, Validation Loss: 2.2320\n",
      "Iteration 244100, Train Loss: 2.6350, Validation Loss: 2.3467\n",
      "Iteration 244200, Train Loss: 2.7203, Validation Loss: 2.3999\n",
      "Iteration 244300, Train Loss: 2.2323, Validation Loss: 2.5876\n",
      "Iteration 244400, Train Loss: 2.3517, Validation Loss: 2.5253\n",
      "Iteration 244500, Train Loss: 2.0055, Validation Loss: 2.5874\n",
      "Iteration 244600, Train Loss: 2.3725, Validation Loss: 2.4967\n",
      "Iteration 244700, Train Loss: 2.3713, Validation Loss: 2.2067\n",
      "Iteration 244800, Train Loss: 2.3325, Validation Loss: 2.1892\n",
      "Iteration 244900, Train Loss: 1.8545, Validation Loss: 2.0381\n",
      "Iteration 245000, Train Loss: 2.3477, Validation Loss: 2.0332\n",
      "Iteration 245100, Train Loss: 2.1777, Validation Loss: 2.1788\n",
      "Iteration 245200, Train Loss: 1.9390, Validation Loss: 2.5294\n",
      "Iteration 245300, Train Loss: 2.1886, Validation Loss: 2.2520\n",
      "Iteration 245400, Train Loss: 2.2171, Validation Loss: 2.1871\n",
      "Iteration 245500, Train Loss: 1.9377, Validation Loss: 2.0798\n",
      "Iteration 245600, Train Loss: 2.3838, Validation Loss: 1.7186\n",
      "Iteration 245700, Train Loss: 1.9509, Validation Loss: 2.4418\n",
      "Iteration 245800, Train Loss: 2.1947, Validation Loss: 2.6061\n",
      "Iteration 245900, Train Loss: 2.5255, Validation Loss: 2.4561\n",
      "Iteration 246000, Train Loss: 2.2717, Validation Loss: 2.8315\n",
      "Iteration 246100, Train Loss: 2.4529, Validation Loss: 2.4151\n",
      "Iteration 246200, Train Loss: 2.6150, Validation Loss: 1.8520\n",
      "Iteration 246300, Train Loss: 2.3390, Validation Loss: 2.3178\n",
      "Iteration 246400, Train Loss: 2.3104, Validation Loss: 2.2449\n",
      "Iteration 246500, Train Loss: 2.3429, Validation Loss: 2.3577\n",
      "Iteration 246600, Train Loss: 2.6431, Validation Loss: 2.4762\n",
      "Iteration 246700, Train Loss: 2.1987, Validation Loss: 2.2190\n",
      "Iteration 246800, Train Loss: 2.4619, Validation Loss: 2.3926\n",
      "Iteration 246900, Train Loss: 1.9956, Validation Loss: 2.2254\n",
      "Iteration 247000, Train Loss: 2.4692, Validation Loss: 2.4797\n",
      "Iteration 247100, Train Loss: 2.6284, Validation Loss: 2.3737\n",
      "Iteration 247200, Train Loss: 2.4280, Validation Loss: 2.0531\n",
      "Iteration 247300, Train Loss: 2.8252, Validation Loss: 2.6116\n",
      "Iteration 247400, Train Loss: 1.8698, Validation Loss: 2.4482\n",
      "Iteration 247500, Train Loss: 2.3873, Validation Loss: 2.2163\n",
      "Iteration 247600, Train Loss: 2.6527, Validation Loss: 2.7589\n",
      "Iteration 247700, Train Loss: 2.4729, Validation Loss: 1.9860\n",
      "Iteration 247800, Train Loss: 2.0335, Validation Loss: 2.2072\n",
      "Iteration 247900, Train Loss: 2.6017, Validation Loss: 2.5733\n",
      "Iteration 248000, Train Loss: 2.0870, Validation Loss: 2.4432\n",
      "Iteration 248100, Train Loss: 1.8032, Validation Loss: 2.3218\n",
      "Iteration 248200, Train Loss: 2.8458, Validation Loss: 2.2559\n",
      "Iteration 248300, Train Loss: 2.0810, Validation Loss: 2.9605\n",
      "Iteration 248400, Train Loss: 2.3570, Validation Loss: 2.2391\n",
      "Iteration 248500, Train Loss: 1.8742, Validation Loss: 2.7162\n",
      "Iteration 248600, Train Loss: 2.2807, Validation Loss: 2.4915\n",
      "Iteration 248700, Train Loss: 1.9545, Validation Loss: 2.3653\n",
      "Iteration 248800, Train Loss: 2.2523, Validation Loss: 2.1793\n",
      "Iteration 248900, Train Loss: 2.2455, Validation Loss: 2.2856\n",
      "Iteration 249000, Train Loss: 2.6106, Validation Loss: 2.2168\n",
      "Iteration 249100, Train Loss: 2.4720, Validation Loss: 2.3260\n",
      "Iteration 249200, Train Loss: 2.2982, Validation Loss: 2.4729\n",
      "Iteration 249300, Train Loss: 2.4845, Validation Loss: 1.8892\n",
      "Iteration 249400, Train Loss: 2.2589, Validation Loss: 2.4115\n",
      "Iteration 249500, Train Loss: 2.3203, Validation Loss: 2.5444\n",
      "Iteration 249600, Train Loss: 2.6969, Validation Loss: 2.1540\n",
      "Iteration 249700, Train Loss: 2.2158, Validation Loss: 2.3724\n",
      "Iteration 249800, Train Loss: 2.4296, Validation Loss: 2.3976\n",
      "Iteration 249900, Train Loss: 2.2848, Validation Loss: 2.1512\n",
      "Iteration 250000, Train Loss: 2.2028, Validation Loss: 2.5947\n",
      "Iteration 250100, Train Loss: 2.4739, Validation Loss: 2.9078\n",
      "Iteration 250200, Train Loss: 2.7253, Validation Loss: 2.7157\n",
      "Iteration 250300, Train Loss: 2.3071, Validation Loss: 2.6946\n",
      "Iteration 250400, Train Loss: 2.5011, Validation Loss: 2.4784\n",
      "Iteration 250500, Train Loss: 2.1720, Validation Loss: 2.3702\n",
      "Iteration 250600, Train Loss: 2.0213, Validation Loss: 2.3903\n",
      "Iteration 250700, Train Loss: 2.3141, Validation Loss: 1.7829\n",
      "Iteration 250800, Train Loss: 2.5696, Validation Loss: 2.4441\n",
      "Iteration 250900, Train Loss: 2.1056, Validation Loss: 2.7224\n",
      "Iteration 251000, Train Loss: 1.9708, Validation Loss: 2.6933\n",
      "Iteration 251100, Train Loss: 2.5830, Validation Loss: 2.6705\n",
      "Iteration 251200, Train Loss: 2.3469, Validation Loss: 2.4186\n",
      "Iteration 251300, Train Loss: 1.9976, Validation Loss: 2.3233\n",
      "Iteration 251400, Train Loss: 2.8350, Validation Loss: 1.9366\n",
      "Iteration 251500, Train Loss: 2.2987, Validation Loss: 2.4571\n",
      "Iteration 251600, Train Loss: 2.4126, Validation Loss: 2.2076\n",
      "Iteration 251700, Train Loss: 2.7215, Validation Loss: 2.3366\n",
      "Iteration 251800, Train Loss: 2.3176, Validation Loss: 2.3611\n",
      "Iteration 251900, Train Loss: 2.7000, Validation Loss: 2.4249\n",
      "Iteration 252000, Train Loss: 2.1430, Validation Loss: 2.0848\n",
      "Iteration 252100, Train Loss: 2.5930, Validation Loss: 2.0922\n",
      "Iteration 252200, Train Loss: 2.7628, Validation Loss: 2.1856\n",
      "Iteration 252300, Train Loss: 2.4285, Validation Loss: 2.5028\n",
      "Iteration 252400, Train Loss: 2.2051, Validation Loss: 2.7503\n",
      "Iteration 252500, Train Loss: 2.3677, Validation Loss: 1.8163\n",
      "Iteration 252600, Train Loss: 2.2328, Validation Loss: 2.8939\n",
      "Iteration 252700, Train Loss: 2.0707, Validation Loss: 2.4155\n",
      "Iteration 252800, Train Loss: 2.3046, Validation Loss: 2.4127\n",
      "Iteration 252900, Train Loss: 2.5982, Validation Loss: 2.4907\n",
      "Iteration 253000, Train Loss: 2.5444, Validation Loss: 2.6859\n",
      "Iteration 253100, Train Loss: 2.4765, Validation Loss: 2.1389\n",
      "Iteration 253200, Train Loss: 2.5422, Validation Loss: 2.3163\n",
      "Iteration 253300, Train Loss: 1.9545, Validation Loss: 2.6513\n",
      "Iteration 253400, Train Loss: 2.1942, Validation Loss: 2.2856\n",
      "Iteration 253500, Train Loss: 2.0248, Validation Loss: 2.1651\n",
      "Iteration 253600, Train Loss: 2.5721, Validation Loss: 2.5279\n",
      "Iteration 253700, Train Loss: 2.2536, Validation Loss: 2.5747\n",
      "Iteration 253800, Train Loss: 2.1381, Validation Loss: 2.1301\n",
      "Iteration 253900, Train Loss: 2.0055, Validation Loss: 2.3647\n",
      "Iteration 254000, Train Loss: 2.2428, Validation Loss: 2.4275\n",
      "Iteration 254100, Train Loss: 2.1285, Validation Loss: 2.1296\n",
      "Iteration 254200, Train Loss: 2.2763, Validation Loss: 2.7228\n",
      "Iteration 254300, Train Loss: 2.2093, Validation Loss: 2.3553\n",
      "Iteration 254400, Train Loss: 2.3359, Validation Loss: 2.2223\n",
      "Iteration 254500, Train Loss: 2.3949, Validation Loss: 2.9033\n",
      "Iteration 254600, Train Loss: 2.5204, Validation Loss: 2.3355\n",
      "Iteration 254700, Train Loss: 2.4545, Validation Loss: 2.3403\n",
      "Iteration 254800, Train Loss: 2.5333, Validation Loss: 2.3017\n",
      "Iteration 254900, Train Loss: 2.1789, Validation Loss: 2.2950\n",
      "Iteration 255000, Train Loss: 2.3557, Validation Loss: 3.1131\n",
      "Iteration 255100, Train Loss: 2.1577, Validation Loss: 2.3181\n",
      "Iteration 255200, Train Loss: 2.2408, Validation Loss: 2.2502\n",
      "Iteration 255300, Train Loss: 2.3701, Validation Loss: 1.9592\n",
      "Iteration 255400, Train Loss: 2.2078, Validation Loss: 2.2852\n",
      "Iteration 255500, Train Loss: 2.4357, Validation Loss: 2.4754\n",
      "Iteration 255600, Train Loss: 2.2073, Validation Loss: 2.4417\n",
      "Iteration 255700, Train Loss: 2.1737, Validation Loss: 2.0423\n",
      "Iteration 255800, Train Loss: 2.6509, Validation Loss: 2.0417\n",
      "Iteration 255900, Train Loss: 2.5176, Validation Loss: 2.3866\n",
      "Iteration 256000, Train Loss: 2.6018, Validation Loss: 2.3449\n",
      "Iteration 256100, Train Loss: 2.2008, Validation Loss: 1.8479\n",
      "Iteration 256200, Train Loss: 2.2839, Validation Loss: 2.5736\n",
      "Iteration 256300, Train Loss: 2.0583, Validation Loss: 2.2918\n",
      "Iteration 256400, Train Loss: 2.1327, Validation Loss: 2.9981\n",
      "Iteration 256500, Train Loss: 2.2404, Validation Loss: 2.5201\n",
      "Iteration 256600, Train Loss: 2.3131, Validation Loss: 2.5269\n",
      "Iteration 256700, Train Loss: 1.9890, Validation Loss: 2.0984\n",
      "Iteration 256800, Train Loss: 2.8249, Validation Loss: 2.9730\n",
      "Iteration 256900, Train Loss: 3.0965, Validation Loss: 2.3357\n",
      "Iteration 257000, Train Loss: 2.5265, Validation Loss: 2.5169\n",
      "Iteration 257100, Train Loss: 2.4218, Validation Loss: 2.7157\n",
      "Iteration 257200, Train Loss: 2.1106, Validation Loss: 2.4703\n",
      "Iteration 257300, Train Loss: 2.0198, Validation Loss: 2.2451\n",
      "Iteration 257400, Train Loss: 2.4833, Validation Loss: 2.3341\n",
      "Iteration 257500, Train Loss: 2.1051, Validation Loss: 2.7337\n",
      "Iteration 257600, Train Loss: 2.2561, Validation Loss: 2.5017\n",
      "Iteration 257700, Train Loss: 2.2906, Validation Loss: 2.4978\n",
      "Iteration 257800, Train Loss: 2.3861, Validation Loss: 2.4855\n",
      "Iteration 257900, Train Loss: 2.5939, Validation Loss: 2.8547\n",
      "Iteration 258000, Train Loss: 2.3549, Validation Loss: 2.3518\n",
      "Iteration 258100, Train Loss: 2.7129, Validation Loss: 2.6408\n",
      "Iteration 258200, Train Loss: 2.3013, Validation Loss: 2.5873\n",
      "Iteration 258300, Train Loss: 1.9659, Validation Loss: 2.6247\n",
      "Iteration 258400, Train Loss: 2.0775, Validation Loss: 2.2329\n",
      "Iteration 258500, Train Loss: 2.2497, Validation Loss: 2.6467\n",
      "Iteration 258600, Train Loss: 1.8635, Validation Loss: 2.4348\n",
      "Iteration 258700, Train Loss: 2.3667, Validation Loss: 2.2775\n",
      "Iteration 258800, Train Loss: 2.4009, Validation Loss: 2.3530\n",
      "Iteration 258900, Train Loss: 2.2706, Validation Loss: 2.4637\n",
      "Iteration 259000, Train Loss: 2.7698, Validation Loss: 2.7629\n",
      "Iteration 259100, Train Loss: 2.1295, Validation Loss: 2.6028\n",
      "Iteration 259200, Train Loss: 2.4254, Validation Loss: 2.6928\n",
      "Iteration 259300, Train Loss: 2.2981, Validation Loss: 2.2103\n",
      "Iteration 259400, Train Loss: 2.1560, Validation Loss: 2.1572\n",
      "Iteration 259500, Train Loss: 2.4492, Validation Loss: 2.4567\n",
      "Iteration 259600, Train Loss: 2.1919, Validation Loss: 2.3742\n",
      "Iteration 259700, Train Loss: 2.5175, Validation Loss: 2.5761\n",
      "Iteration 259800, Train Loss: 2.2655, Validation Loss: 2.5235\n",
      "Iteration 259900, Train Loss: 1.8987, Validation Loss: 2.4386\n",
      "Iteration 260000, Train Loss: 2.3530, Validation Loss: 2.4856\n",
      "Iteration 260100, Train Loss: 2.7036, Validation Loss: 2.4410\n",
      "Iteration 260200, Train Loss: 2.4345, Validation Loss: 2.5672\n",
      "Iteration 260300, Train Loss: 2.6527, Validation Loss: 2.2971\n",
      "Iteration 260400, Train Loss: 2.2463, Validation Loss: 2.2114\n",
      "Iteration 260500, Train Loss: 2.2757, Validation Loss: 2.0183\n",
      "Iteration 260600, Train Loss: 2.0589, Validation Loss: 2.2041\n",
      "Iteration 260700, Train Loss: 2.2285, Validation Loss: 2.2084\n",
      "Iteration 260800, Train Loss: 2.3557, Validation Loss: 2.1647\n",
      "Iteration 260900, Train Loss: 2.2333, Validation Loss: 2.5164\n",
      "Iteration 261000, Train Loss: 2.1875, Validation Loss: 2.4074\n",
      "Iteration 261100, Train Loss: 2.2056, Validation Loss: 2.0898\n",
      "Iteration 261200, Train Loss: 2.4498, Validation Loss: 2.8457\n",
      "Iteration 261300, Train Loss: 2.6696, Validation Loss: 2.3902\n",
      "Iteration 261400, Train Loss: 2.4403, Validation Loss: 2.9195\n",
      "Iteration 261500, Train Loss: 2.1068, Validation Loss: 2.3903\n",
      "Iteration 261600, Train Loss: 2.3169, Validation Loss: 2.2722\n",
      "Iteration 261700, Train Loss: 2.7837, Validation Loss: 2.6417\n",
      "Iteration 261800, Train Loss: 2.0305, Validation Loss: 2.5217\n",
      "Iteration 261900, Train Loss: 2.4972, Validation Loss: 2.2568\n",
      "Iteration 262000, Train Loss: 2.4624, Validation Loss: 2.6179\n",
      "Iteration 262100, Train Loss: 2.0851, Validation Loss: 2.5667\n",
      "Iteration 262200, Train Loss: 2.4727, Validation Loss: 2.1736\n",
      "Iteration 262300, Train Loss: 2.1732, Validation Loss: 2.5237\n",
      "Iteration 262400, Train Loss: 2.2081, Validation Loss: 2.7737\n",
      "Iteration 262500, Train Loss: 2.2473, Validation Loss: 2.0341\n",
      "Iteration 262600, Train Loss: 2.4951, Validation Loss: 2.3067\n",
      "Iteration 262700, Train Loss: 1.8476, Validation Loss: 2.4323\n",
      "Iteration 262800, Train Loss: 2.4113, Validation Loss: 2.3175\n",
      "Iteration 262900, Train Loss: 2.4884, Validation Loss: 2.5308\n",
      "Iteration 263000, Train Loss: 2.1966, Validation Loss: 2.4656\n",
      "Iteration 263100, Train Loss: 2.5148, Validation Loss: 2.5290\n",
      "Iteration 263200, Train Loss: 2.3325, Validation Loss: 1.7819\n",
      "Iteration 263300, Train Loss: 2.0137, Validation Loss: 2.7561\n",
      "Iteration 263400, Train Loss: 2.4974, Validation Loss: 2.4770\n",
      "Iteration 263500, Train Loss: 2.3828, Validation Loss: 2.3847\n",
      "Iteration 263600, Train Loss: 2.4031, Validation Loss: 2.4409\n",
      "Iteration 263700, Train Loss: 2.2100, Validation Loss: 2.1242\n",
      "Iteration 263800, Train Loss: 2.0197, Validation Loss: 2.1774\n",
      "Iteration 263900, Train Loss: 2.2582, Validation Loss: 2.2858\n",
      "Iteration 264000, Train Loss: 2.3963, Validation Loss: 2.1582\n",
      "Iteration 264100, Train Loss: 2.8234, Validation Loss: 2.6345\n",
      "Iteration 264200, Train Loss: 2.3657, Validation Loss: 1.9435\n",
      "Iteration 264300, Train Loss: 2.3573, Validation Loss: 2.5916\n",
      "Iteration 264400, Train Loss: 2.4636, Validation Loss: 2.2435\n",
      "Iteration 264500, Train Loss: 2.2354, Validation Loss: 2.2354\n",
      "Iteration 264600, Train Loss: 2.1174, Validation Loss: 2.4149\n",
      "Iteration 264700, Train Loss: 2.2913, Validation Loss: 2.3038\n",
      "Iteration 264800, Train Loss: 2.1451, Validation Loss: 2.3328\n",
      "Iteration 264900, Train Loss: 2.2953, Validation Loss: 2.7689\n",
      "Iteration 265000, Train Loss: 2.2359, Validation Loss: 2.3741\n",
      "Iteration 265100, Train Loss: 2.2960, Validation Loss: 2.5365\n",
      "Iteration 265200, Train Loss: 2.2407, Validation Loss: 2.5804\n",
      "Iteration 265300, Train Loss: 2.3419, Validation Loss: 2.0484\n",
      "Iteration 265400, Train Loss: 2.3262, Validation Loss: 2.2403\n",
      "Iteration 265500, Train Loss: 2.5624, Validation Loss: 2.5071\n",
      "Iteration 265600, Train Loss: 2.3836, Validation Loss: 2.4795\n",
      "Iteration 265700, Train Loss: 2.2639, Validation Loss: 1.9893\n",
      "Iteration 265800, Train Loss: 2.0451, Validation Loss: 2.3122\n",
      "Iteration 265900, Train Loss: 3.0052, Validation Loss: 2.3124\n",
      "Iteration 266000, Train Loss: 2.2229, Validation Loss: 2.3201\n",
      "Iteration 266100, Train Loss: 2.2331, Validation Loss: 2.2615\n",
      "Iteration 266200, Train Loss: 2.5246, Validation Loss: 2.4035\n",
      "Iteration 266300, Train Loss: 1.9922, Validation Loss: 2.2800\n",
      "Iteration 266400, Train Loss: 2.3961, Validation Loss: 2.3442\n",
      "Iteration 266500, Train Loss: 2.1078, Validation Loss: 2.2733\n",
      "Iteration 266600, Train Loss: 1.8405, Validation Loss: 2.2542\n",
      "Iteration 266700, Train Loss: 1.9348, Validation Loss: 2.2566\n",
      "Iteration 266800, Train Loss: 2.1779, Validation Loss: 2.2428\n",
      "Iteration 266900, Train Loss: 2.6085, Validation Loss: 2.6745\n",
      "Iteration 267000, Train Loss: 2.2418, Validation Loss: 2.4691\n",
      "Iteration 267100, Train Loss: 2.4297, Validation Loss: 2.3133\n",
      "Iteration 267200, Train Loss: 2.2632, Validation Loss: 2.2055\n",
      "Iteration 267300, Train Loss: 2.2284, Validation Loss: 2.3149\n",
      "Iteration 267400, Train Loss: 2.3856, Validation Loss: 3.0952\n",
      "Iteration 267500, Train Loss: 2.1186, Validation Loss: 2.3060\n",
      "Iteration 267600, Train Loss: 2.3456, Validation Loss: 1.8035\n",
      "Iteration 267700, Train Loss: 2.3018, Validation Loss: 1.9916\n",
      "Iteration 267800, Train Loss: 2.5243, Validation Loss: 2.3191\n",
      "Iteration 267900, Train Loss: 2.0039, Validation Loss: 2.0743\n",
      "Iteration 268000, Train Loss: 2.5706, Validation Loss: 2.3266\n",
      "Iteration 268100, Train Loss: 2.6952, Validation Loss: 2.4088\n",
      "Iteration 268200, Train Loss: 2.1033, Validation Loss: 2.5816\n",
      "Iteration 268300, Train Loss: 2.4974, Validation Loss: 2.3557\n",
      "Iteration 268400, Train Loss: 2.3445, Validation Loss: 2.2377\n",
      "Iteration 268500, Train Loss: 2.5253, Validation Loss: 2.6622\n",
      "Iteration 268600, Train Loss: 2.4908, Validation Loss: 2.1116\n",
      "Iteration 268700, Train Loss: 2.4728, Validation Loss: 2.1649\n",
      "Iteration 268800, Train Loss: 2.3336, Validation Loss: 2.3971\n",
      "Iteration 268900, Train Loss: 2.6933, Validation Loss: 2.2645\n",
      "Iteration 269000, Train Loss: 2.7209, Validation Loss: 2.3952\n",
      "Iteration 269100, Train Loss: 2.0845, Validation Loss: 2.7855\n",
      "Iteration 269200, Train Loss: 2.1754, Validation Loss: 2.5844\n",
      "Iteration 269300, Train Loss: 2.3040, Validation Loss: 2.4687\n",
      "Iteration 269400, Train Loss: 2.4352, Validation Loss: 1.9506\n",
      "Iteration 269500, Train Loss: 2.3130, Validation Loss: 2.2023\n",
      "Iteration 269600, Train Loss: 2.6181, Validation Loss: 1.8471\n",
      "Iteration 269700, Train Loss: 1.9728, Validation Loss: 2.7410\n",
      "Iteration 269800, Train Loss: 2.0968, Validation Loss: 2.3952\n",
      "Iteration 269900, Train Loss: 2.3408, Validation Loss: 2.1969\n",
      "Iteration 270000, Train Loss: 2.2574, Validation Loss: 2.5317\n",
      "Iteration 270100, Train Loss: 2.2247, Validation Loss: 2.5364\n",
      "Iteration 270200, Train Loss: 2.6190, Validation Loss: 2.1374\n",
      "Iteration 270300, Train Loss: 2.0946, Validation Loss: 2.0404\n",
      "Iteration 270400, Train Loss: 2.4572, Validation Loss: 2.2521\n",
      "Iteration 270500, Train Loss: 2.1178, Validation Loss: 2.0888\n",
      "Iteration 270600, Train Loss: 2.4697, Validation Loss: 2.5020\n",
      "Iteration 270700, Train Loss: 2.1515, Validation Loss: 2.3730\n",
      "Iteration 270800, Train Loss: 2.3324, Validation Loss: 2.7711\n",
      "Iteration 270900, Train Loss: 1.8155, Validation Loss: 2.2693\n",
      "Iteration 271000, Train Loss: 2.4045, Validation Loss: 2.0071\n",
      "Iteration 271100, Train Loss: 2.6785, Validation Loss: 2.1339\n",
      "Iteration 271200, Train Loss: 2.2996, Validation Loss: 2.1164\n",
      "Iteration 271300, Train Loss: 2.3125, Validation Loss: 2.6594\n",
      "Iteration 271400, Train Loss: 2.3841, Validation Loss: 2.8764\n",
      "Iteration 271500, Train Loss: 2.0403, Validation Loss: 2.3751\n",
      "Iteration 271600, Train Loss: 2.2746, Validation Loss: 1.9480\n",
      "Iteration 271700, Train Loss: 1.9295, Validation Loss: 2.0707\n",
      "Iteration 271800, Train Loss: 2.2734, Validation Loss: 2.1827\n",
      "Iteration 271900, Train Loss: 2.0119, Validation Loss: 2.6956\n",
      "Iteration 272000, Train Loss: 2.4854, Validation Loss: 2.0053\n",
      "Iteration 272100, Train Loss: 2.2313, Validation Loss: 2.1453\n",
      "Iteration 272200, Train Loss: 2.3503, Validation Loss: 2.2477\n",
      "Iteration 272300, Train Loss: 2.1437, Validation Loss: 2.3809\n",
      "Iteration 272400, Train Loss: 2.6023, Validation Loss: 2.1073\n",
      "Iteration 272500, Train Loss: 2.0865, Validation Loss: 2.5460\n",
      "Iteration 272600, Train Loss: 2.4660, Validation Loss: 2.5222\n",
      "Iteration 272700, Train Loss: 2.6833, Validation Loss: 2.5867\n",
      "Iteration 272800, Train Loss: 2.1287, Validation Loss: 2.3173\n",
      "Iteration 272900, Train Loss: 2.2866, Validation Loss: 2.1185\n",
      "Iteration 273000, Train Loss: 2.6350, Validation Loss: 2.2317\n",
      "Iteration 273100, Train Loss: 2.1957, Validation Loss: 2.3406\n",
      "Iteration 273200, Train Loss: 2.4799, Validation Loss: 2.0712\n",
      "Iteration 273300, Train Loss: 2.1594, Validation Loss: 2.7272\n",
      "Iteration 273400, Train Loss: 2.4372, Validation Loss: 1.9628\n",
      "Iteration 273500, Train Loss: 2.2880, Validation Loss: 2.7354\n",
      "Iteration 273600, Train Loss: 2.0859, Validation Loss: 2.3162\n",
      "Iteration 273700, Train Loss: 2.5428, Validation Loss: 2.2046\n",
      "Iteration 273800, Train Loss: 1.9310, Validation Loss: 2.7289\n",
      "Iteration 273900, Train Loss: 2.3706, Validation Loss: 2.2809\n",
      "Iteration 274000, Train Loss: 2.5882, Validation Loss: 2.8068\n",
      "Iteration 274100, Train Loss: 2.3501, Validation Loss: 1.9796\n",
      "Iteration 274200, Train Loss: 2.1285, Validation Loss: 2.0557\n",
      "Iteration 274300, Train Loss: 2.1784, Validation Loss: 2.6740\n",
      "Iteration 274400, Train Loss: 1.9969, Validation Loss: 2.9226\n",
      "Iteration 274500, Train Loss: 2.1814, Validation Loss: 1.8879\n",
      "Iteration 274600, Train Loss: 2.0767, Validation Loss: 2.4031\n",
      "Iteration 274700, Train Loss: 2.6539, Validation Loss: 2.3155\n",
      "Iteration 274800, Train Loss: 1.9793, Validation Loss: 2.7282\n",
      "Iteration 274900, Train Loss: 2.2704, Validation Loss: 2.2637\n",
      "Iteration 275000, Train Loss: 2.4319, Validation Loss: 2.3852\n",
      "Iteration 275100, Train Loss: 2.0174, Validation Loss: 2.1008\n",
      "Iteration 275200, Train Loss: 1.9706, Validation Loss: 2.5779\n",
      "Iteration 275300, Train Loss: 2.2989, Validation Loss: 2.2322\n",
      "Iteration 275400, Train Loss: 2.5456, Validation Loss: 2.3093\n",
      "Iteration 275500, Train Loss: 2.1085, Validation Loss: 2.7763\n",
      "Iteration 275600, Train Loss: 2.2288, Validation Loss: 2.2620\n",
      "Iteration 275700, Train Loss: 2.4859, Validation Loss: 2.3650\n",
      "Iteration 275800, Train Loss: 2.2650, Validation Loss: 2.3304\n",
      "Iteration 275900, Train Loss: 2.0747, Validation Loss: 2.2986\n",
      "Iteration 276000, Train Loss: 2.0258, Validation Loss: 2.0166\n",
      "Iteration 276100, Train Loss: 2.3555, Validation Loss: 2.6853\n",
      "Iteration 276200, Train Loss: 2.4736, Validation Loss: 2.6462\n",
      "Iteration 276300, Train Loss: 2.1752, Validation Loss: 2.2109\n",
      "Iteration 276400, Train Loss: 2.0352, Validation Loss: 2.0016\n",
      "Iteration 276500, Train Loss: 2.6283, Validation Loss: 1.9306\n",
      "Iteration 276600, Train Loss: 2.6141, Validation Loss: 2.1927\n",
      "Iteration 276700, Train Loss: 2.7915, Validation Loss: 1.9199\n",
      "Iteration 276800, Train Loss: 2.2155, Validation Loss: 1.9493\n",
      "Iteration 276900, Train Loss: 2.0535, Validation Loss: 2.4884\n",
      "Iteration 277000, Train Loss: 2.2387, Validation Loss: 2.3186\n",
      "Iteration 277100, Train Loss: 2.3389, Validation Loss: 2.9407\n",
      "Iteration 277200, Train Loss: 2.1963, Validation Loss: 2.0310\n",
      "Iteration 277300, Train Loss: 2.3086, Validation Loss: 2.7478\n",
      "Iteration 277400, Train Loss: 2.4044, Validation Loss: 2.5747\n",
      "Iteration 277500, Train Loss: 2.4449, Validation Loss: 1.9932\n",
      "Iteration 277600, Train Loss: 2.3541, Validation Loss: 2.0083\n",
      "Iteration 277700, Train Loss: 2.4824, Validation Loss: 2.5407\n",
      "Iteration 277800, Train Loss: 2.3928, Validation Loss: 2.4491\n",
      "Iteration 277900, Train Loss: 2.7188, Validation Loss: 2.2165\n",
      "Iteration 278000, Train Loss: 2.5056, Validation Loss: 2.4088\n",
      "Iteration 278100, Train Loss: 1.9101, Validation Loss: 2.5421\n",
      "Iteration 278200, Train Loss: 1.9238, Validation Loss: 2.4487\n",
      "Iteration 278300, Train Loss: 2.3038, Validation Loss: 2.8596\n",
      "Iteration 278400, Train Loss: 2.1848, Validation Loss: 2.6344\n",
      "Iteration 278500, Train Loss: 2.3595, Validation Loss: 2.3832\n",
      "Iteration 278600, Train Loss: 2.1437, Validation Loss: 2.7607\n",
      "Iteration 278700, Train Loss: 2.1556, Validation Loss: 2.2753\n",
      "Iteration 278800, Train Loss: 2.8599, Validation Loss: 2.1384\n",
      "Iteration 278900, Train Loss: 2.1175, Validation Loss: 2.2682\n",
      "Iteration 279000, Train Loss: 2.8357, Validation Loss: 2.2234\n",
      "Iteration 279100, Train Loss: 1.8914, Validation Loss: 2.4190\n",
      "Iteration 279200, Train Loss: 2.6681, Validation Loss: 2.6624\n",
      "Iteration 279300, Train Loss: 2.4876, Validation Loss: 2.0619\n",
      "Iteration 279400, Train Loss: 2.1748, Validation Loss: 1.8769\n",
      "Iteration 279500, Train Loss: 2.1220, Validation Loss: 1.6620\n",
      "Iteration 279600, Train Loss: 2.5683, Validation Loss: 2.1238\n",
      "Iteration 279700, Train Loss: 1.9456, Validation Loss: 2.7661\n",
      "Iteration 279800, Train Loss: 2.6579, Validation Loss: 2.6328\n",
      "Iteration 279900, Train Loss: 2.2840, Validation Loss: 2.1592\n",
      "Iteration 280000, Train Loss: 2.3912, Validation Loss: 2.0730\n",
      "Iteration 280100, Train Loss: 2.2925, Validation Loss: 2.4063\n",
      "Iteration 280200, Train Loss: 2.3571, Validation Loss: 2.3335\n",
      "Iteration 280300, Train Loss: 2.6385, Validation Loss: 2.5018\n",
      "Iteration 280400, Train Loss: 2.3355, Validation Loss: 2.5144\n",
      "Iteration 280500, Train Loss: 2.6121, Validation Loss: 2.0879\n",
      "Iteration 280600, Train Loss: 2.0133, Validation Loss: 2.2771\n",
      "Iteration 280700, Train Loss: 2.4098, Validation Loss: 2.3836\n",
      "Iteration 280800, Train Loss: 2.5753, Validation Loss: 2.3268\n",
      "Iteration 280900, Train Loss: 2.0322, Validation Loss: 2.0323\n",
      "Iteration 281000, Train Loss: 2.1162, Validation Loss: 2.5999\n",
      "Iteration 281100, Train Loss: 2.1899, Validation Loss: 2.5899\n",
      "Iteration 281200, Train Loss: 2.1103, Validation Loss: 2.3448\n",
      "Iteration 281300, Train Loss: 2.7517, Validation Loss: 2.3231\n",
      "Iteration 281400, Train Loss: 2.3084, Validation Loss: 2.4004\n",
      "Iteration 281500, Train Loss: 1.8252, Validation Loss: 2.1336\n",
      "Iteration 281600, Train Loss: 2.1404, Validation Loss: 2.0478\n",
      "Iteration 281700, Train Loss: 2.6440, Validation Loss: 2.3026\n",
      "Iteration 281800, Train Loss: 2.1523, Validation Loss: 2.0951\n",
      "Iteration 281900, Train Loss: 2.6271, Validation Loss: 2.5013\n",
      "Iteration 282000, Train Loss: 2.1579, Validation Loss: 2.2288\n",
      "Iteration 282100, Train Loss: 2.3974, Validation Loss: 2.3951\n",
      "Iteration 282200, Train Loss: 2.6171, Validation Loss: 2.5170\n",
      "Iteration 282300, Train Loss: 2.2289, Validation Loss: 2.3758\n",
      "Iteration 282400, Train Loss: 2.4535, Validation Loss: 2.4990\n",
      "Iteration 282500, Train Loss: 2.1843, Validation Loss: 2.6415\n",
      "Iteration 282600, Train Loss: 1.8575, Validation Loss: 2.1314\n",
      "Iteration 282700, Train Loss: 2.3874, Validation Loss: 2.5822\n",
      "Iteration 282800, Train Loss: 2.2541, Validation Loss: 2.0900\n",
      "Iteration 282900, Train Loss: 2.2515, Validation Loss: 2.0493\n",
      "Iteration 283000, Train Loss: 2.4004, Validation Loss: 2.4032\n",
      "Iteration 283100, Train Loss: 2.3392, Validation Loss: 2.5925\n",
      "Iteration 283200, Train Loss: 2.1964, Validation Loss: 2.2892\n",
      "Iteration 283300, Train Loss: 2.1881, Validation Loss: 2.3230\n",
      "Iteration 283400, Train Loss: 2.5510, Validation Loss: 1.9901\n",
      "Iteration 283500, Train Loss: 2.1471, Validation Loss: 2.3519\n",
      "Iteration 283600, Train Loss: 2.2512, Validation Loss: 2.3169\n",
      "Iteration 283700, Train Loss: 2.1693, Validation Loss: 2.2758\n",
      "Iteration 283800, Train Loss: 2.1818, Validation Loss: 2.1051\n",
      "Iteration 283900, Train Loss: 1.9617, Validation Loss: 2.6949\n",
      "Iteration 284000, Train Loss: 2.3862, Validation Loss: 2.2554\n",
      "Iteration 284100, Train Loss: 2.5861, Validation Loss: 2.4816\n",
      "Iteration 284200, Train Loss: 1.9819, Validation Loss: 2.1562\n",
      "Iteration 284300, Train Loss: 2.3398, Validation Loss: 3.3244\n",
      "Iteration 284400, Train Loss: 2.3528, Validation Loss: 2.2176\n",
      "Iteration 284500, Train Loss: 2.3877, Validation Loss: 2.1408\n",
      "Iteration 284600, Train Loss: 1.8693, Validation Loss: 2.2318\n",
      "Iteration 284700, Train Loss: 2.5041, Validation Loss: 2.3077\n",
      "Iteration 284800, Train Loss: 2.1683, Validation Loss: 2.1526\n",
      "Iteration 284900, Train Loss: 2.5883, Validation Loss: 2.2408\n",
      "Iteration 285000, Train Loss: 2.1707, Validation Loss: 2.4571\n",
      "Iteration 285100, Train Loss: 2.3612, Validation Loss: 2.0976\n",
      "Iteration 285200, Train Loss: 2.1486, Validation Loss: 2.2282\n",
      "Iteration 285300, Train Loss: 2.1074, Validation Loss: 2.3939\n",
      "Iteration 285400, Train Loss: 2.3148, Validation Loss: 2.7455\n",
      "Iteration 285500, Train Loss: 2.4209, Validation Loss: 2.1812\n",
      "Iteration 285600, Train Loss: 2.0737, Validation Loss: 2.3493\n",
      "Iteration 285700, Train Loss: 2.1723, Validation Loss: 2.9577\n",
      "Iteration 285800, Train Loss: 2.4235, Validation Loss: 2.4211\n",
      "Iteration 285900, Train Loss: 2.0155, Validation Loss: 1.7029\n",
      "Iteration 286000, Train Loss: 2.5804, Validation Loss: 2.0582\n",
      "Iteration 286100, Train Loss: 2.1808, Validation Loss: 1.8663\n",
      "Iteration 286200, Train Loss: 2.0313, Validation Loss: 2.0293\n",
      "Iteration 286300, Train Loss: 2.1761, Validation Loss: 2.7449\n",
      "Iteration 286400, Train Loss: 2.6268, Validation Loss: 2.1273\n",
      "Iteration 286500, Train Loss: 2.3956, Validation Loss: 2.3797\n",
      "Iteration 286600, Train Loss: 2.3475, Validation Loss: 2.3400\n",
      "Iteration 286700, Train Loss: 2.3787, Validation Loss: 2.6854\n",
      "Iteration 286800, Train Loss: 1.7939, Validation Loss: 2.2599\n",
      "Iteration 286900, Train Loss: 2.2724, Validation Loss: 2.5013\n",
      "Iteration 287000, Train Loss: 2.4782, Validation Loss: 2.4420\n",
      "Iteration 287100, Train Loss: 1.9369, Validation Loss: 1.9596\n",
      "Iteration 287200, Train Loss: 2.0237, Validation Loss: 2.3043\n",
      "Iteration 287300, Train Loss: 2.7465, Validation Loss: 2.6044\n",
      "Iteration 287400, Train Loss: 2.0858, Validation Loss: 2.3170\n",
      "Iteration 287500, Train Loss: 2.4789, Validation Loss: 2.3025\n",
      "Iteration 287600, Train Loss: 1.8932, Validation Loss: 2.5736\n",
      "Iteration 287700, Train Loss: 2.2604, Validation Loss: 2.3089\n",
      "Iteration 287800, Train Loss: 2.8059, Validation Loss: 2.2456\n",
      "Iteration 287900, Train Loss: 2.4736, Validation Loss: 2.7957\n",
      "Iteration 288000, Train Loss: 2.1493, Validation Loss: 2.3841\n",
      "Iteration 288100, Train Loss: 2.0764, Validation Loss: 2.5182\n",
      "Iteration 288200, Train Loss: 2.8463, Validation Loss: 2.4708\n",
      "Iteration 288300, Train Loss: 2.3712, Validation Loss: 2.1675\n",
      "Iteration 288400, Train Loss: 2.4731, Validation Loss: 2.2000\n",
      "Iteration 288500, Train Loss: 2.1697, Validation Loss: 2.1428\n",
      "Iteration 288600, Train Loss: 2.1399, Validation Loss: 2.1799\n",
      "Iteration 288700, Train Loss: 2.1455, Validation Loss: 2.2882\n",
      "Iteration 288800, Train Loss: 2.3822, Validation Loss: 2.1550\n",
      "Iteration 288900, Train Loss: 2.3808, Validation Loss: 2.5735\n",
      "Iteration 289000, Train Loss: 2.3839, Validation Loss: 2.5682\n",
      "Iteration 289100, Train Loss: 1.9594, Validation Loss: 2.2875\n",
      "Iteration 289200, Train Loss: 2.4357, Validation Loss: 2.3895\n",
      "Iteration 289300, Train Loss: 2.5097, Validation Loss: 2.2255\n",
      "Iteration 289400, Train Loss: 2.2521, Validation Loss: 2.2887\n",
      "Iteration 289500, Train Loss: 2.4712, Validation Loss: 2.3762\n",
      "Iteration 289600, Train Loss: 2.6132, Validation Loss: 1.8922\n",
      "Iteration 289700, Train Loss: 2.2694, Validation Loss: 2.8697\n",
      "Iteration 289800, Train Loss: 2.0821, Validation Loss: 2.3124\n",
      "Iteration 289900, Train Loss: 2.4073, Validation Loss: 2.4058\n",
      "Iteration 290000, Train Loss: 2.3900, Validation Loss: 2.5856\n",
      "Iteration 290100, Train Loss: 2.5516, Validation Loss: 2.2740\n",
      "Iteration 290200, Train Loss: 2.4308, Validation Loss: 2.6035\n",
      "Iteration 290300, Train Loss: 2.3197, Validation Loss: 2.3405\n",
      "Iteration 290400, Train Loss: 2.4573, Validation Loss: 2.9024\n",
      "Iteration 290500, Train Loss: 2.7550, Validation Loss: 1.9405\n",
      "Iteration 290600, Train Loss: 2.5682, Validation Loss: 2.2093\n",
      "Iteration 290700, Train Loss: 2.4737, Validation Loss: 2.4248\n",
      "Iteration 290800, Train Loss: 2.2761, Validation Loss: 2.2395\n",
      "Iteration 290900, Train Loss: 2.5408, Validation Loss: 2.4206\n",
      "Iteration 291000, Train Loss: 2.3478, Validation Loss: 2.2810\n",
      "Iteration 291100, Train Loss: 2.6509, Validation Loss: 1.9988\n",
      "Iteration 291200, Train Loss: 2.3987, Validation Loss: 2.4306\n",
      "Iteration 291300, Train Loss: 2.3129, Validation Loss: 2.4011\n",
      "Iteration 291400, Train Loss: 2.2394, Validation Loss: 2.6673\n",
      "Iteration 291500, Train Loss: 2.6467, Validation Loss: 2.3891\n",
      "Iteration 291600, Train Loss: 2.3640, Validation Loss: 2.4459\n",
      "Iteration 291700, Train Loss: 2.4877, Validation Loss: 2.3744\n",
      "Iteration 291800, Train Loss: 2.4852, Validation Loss: 1.9764\n",
      "Iteration 291900, Train Loss: 2.2277, Validation Loss: 2.0599\n",
      "Iteration 292000, Train Loss: 2.1881, Validation Loss: 1.9155\n",
      "Iteration 292100, Train Loss: 2.3633, Validation Loss: 2.3552\n",
      "Iteration 292200, Train Loss: 2.2698, Validation Loss: 2.6359\n",
      "Iteration 292300, Train Loss: 2.3455, Validation Loss: 2.3531\n",
      "Iteration 292400, Train Loss: 2.5079, Validation Loss: 2.2464\n",
      "Iteration 292500, Train Loss: 2.0486, Validation Loss: 2.0783\n",
      "Iteration 292600, Train Loss: 2.5104, Validation Loss: 2.3019\n",
      "Iteration 292700, Train Loss: 2.0702, Validation Loss: 2.4936\n",
      "Iteration 292800, Train Loss: 2.4288, Validation Loss: 2.8330\n",
      "Iteration 292900, Train Loss: 2.3530, Validation Loss: 2.4233\n",
      "Iteration 293000, Train Loss: 1.9241, Validation Loss: 2.1425\n",
      "Iteration 293100, Train Loss: 1.6458, Validation Loss: 2.3604\n",
      "Iteration 293200, Train Loss: 1.7841, Validation Loss: 2.5774\n",
      "Iteration 293300, Train Loss: 2.0270, Validation Loss: 1.9807\n",
      "Iteration 293400, Train Loss: 2.4971, Validation Loss: 2.0931\n",
      "Iteration 293500, Train Loss: 2.4200, Validation Loss: 2.3045\n",
      "Iteration 293600, Train Loss: 2.1437, Validation Loss: 2.2162\n",
      "Iteration 293700, Train Loss: 2.2669, Validation Loss: 2.1082\n",
      "Iteration 293800, Train Loss: 2.3835, Validation Loss: 2.2531\n",
      "Iteration 293900, Train Loss: 2.1362, Validation Loss: 2.6589\n",
      "Iteration 294000, Train Loss: 2.7019, Validation Loss: 2.5021\n",
      "Iteration 294100, Train Loss: 2.4426, Validation Loss: 1.9999\n",
      "Iteration 294200, Train Loss: 2.4039, Validation Loss: 2.3468\n",
      "Iteration 294300, Train Loss: 2.5533, Validation Loss: 2.3075\n",
      "Iteration 294400, Train Loss: 2.2000, Validation Loss: 2.1632\n",
      "Iteration 294500, Train Loss: 2.8138, Validation Loss: 2.3239\n",
      "Iteration 294600, Train Loss: 2.4338, Validation Loss: 2.4475\n",
      "Iteration 294700, Train Loss: 2.2100, Validation Loss: 1.9389\n",
      "Iteration 294800, Train Loss: 1.9693, Validation Loss: 2.4096\n",
      "Iteration 294900, Train Loss: 2.1268, Validation Loss: 2.4030\n",
      "Iteration 295000, Train Loss: 2.3273, Validation Loss: 2.0661\n",
      "Iteration 295100, Train Loss: 2.1200, Validation Loss: 1.9348\n",
      "Iteration 295200, Train Loss: 2.1288, Validation Loss: 2.5148\n",
      "Iteration 295300, Train Loss: 2.1454, Validation Loss: 2.7327\n",
      "Iteration 295400, Train Loss: 2.1265, Validation Loss: 2.3503\n",
      "Iteration 295500, Train Loss: 2.1871, Validation Loss: 1.9849\n",
      "Iteration 295600, Train Loss: 1.9154, Validation Loss: 3.1493\n",
      "Iteration 295700, Train Loss: 2.2892, Validation Loss: 2.7161\n",
      "Iteration 295800, Train Loss: 2.2575, Validation Loss: 2.1589\n",
      "Iteration 295900, Train Loss: 2.5404, Validation Loss: 2.6569\n",
      "Iteration 296000, Train Loss: 2.2047, Validation Loss: 2.3957\n",
      "Iteration 296100, Train Loss: 2.7122, Validation Loss: 2.6154\n",
      "Iteration 296200, Train Loss: 2.1530, Validation Loss: 2.4749\n",
      "Iteration 296300, Train Loss: 2.1674, Validation Loss: 1.9440\n",
      "Iteration 296400, Train Loss: 2.0866, Validation Loss: 2.0437\n",
      "Iteration 296500, Train Loss: 2.4891, Validation Loss: 2.0255\n",
      "Iteration 296600, Train Loss: 2.6045, Validation Loss: 2.5925\n",
      "Iteration 296700, Train Loss: 2.0432, Validation Loss: 2.4540\n",
      "Iteration 296800, Train Loss: 2.0565, Validation Loss: 2.1225\n",
      "Iteration 296900, Train Loss: 1.8894, Validation Loss: 2.2516\n",
      "Iteration 297000, Train Loss: 2.4800, Validation Loss: 2.8325\n",
      "Iteration 297100, Train Loss: 2.2013, Validation Loss: 2.2883\n",
      "Iteration 297200, Train Loss: 2.7244, Validation Loss: 2.3268\n",
      "Iteration 297300, Train Loss: 2.0603, Validation Loss: 2.4907\n",
      "Iteration 297400, Train Loss: 2.3495, Validation Loss: 2.2099\n",
      "Iteration 297500, Train Loss: 2.1327, Validation Loss: 2.1232\n",
      "Iteration 297600, Train Loss: 2.1498, Validation Loss: 2.1245\n",
      "Iteration 297700, Train Loss: 2.4753, Validation Loss: 2.0480\n",
      "Iteration 297800, Train Loss: 2.5463, Validation Loss: 2.8247\n",
      "Iteration 297900, Train Loss: 2.4287, Validation Loss: 2.4007\n",
      "Iteration 298000, Train Loss: 2.2426, Validation Loss: 2.3437\n",
      "Iteration 298100, Train Loss: 2.4277, Validation Loss: 1.9420\n",
      "Iteration 298200, Train Loss: 2.5738, Validation Loss: 2.6671\n",
      "Iteration 298300, Train Loss: 2.6346, Validation Loss: 2.5158\n",
      "Iteration 298400, Train Loss: 2.4338, Validation Loss: 2.0301\n",
      "Iteration 298500, Train Loss: 2.5152, Validation Loss: 2.0899\n",
      "Iteration 298600, Train Loss: 2.1148, Validation Loss: 2.3948\n",
      "Iteration 298700, Train Loss: 2.4114, Validation Loss: 2.3628\n",
      "Iteration 298800, Train Loss: 2.5781, Validation Loss: 2.5387\n",
      "Iteration 298900, Train Loss: 2.7769, Validation Loss: 2.3329\n",
      "Iteration 299000, Train Loss: 2.2797, Validation Loss: 2.2562\n",
      "Iteration 299100, Train Loss: 2.5492, Validation Loss: 2.3117\n",
      "Iteration 299200, Train Loss: 1.9603, Validation Loss: 1.9799\n",
      "Iteration 299300, Train Loss: 2.0380, Validation Loss: 2.0445\n",
      "Iteration 299400, Train Loss: 2.3160, Validation Loss: 2.6908\n",
      "Iteration 299500, Train Loss: 2.3417, Validation Loss: 1.8310\n",
      "Iteration 299600, Train Loss: 2.8235, Validation Loss: 2.2026\n",
      "Iteration 299700, Train Loss: 2.5210, Validation Loss: 2.1740\n",
      "Iteration 299800, Train Loss: 2.1985, Validation Loss: 2.2923\n",
      "Iteration 299900, Train Loss: 1.9288, Validation Loss: 1.9291\n",
      "Iteration 300000, Train Loss: 2.2121, Validation Loss: 2.2309\n",
      "Iteration 300100, Train Loss: 2.1451, Validation Loss: 2.3670\n",
      "Iteration 300200, Train Loss: 1.9922, Validation Loss: 2.7326\n",
      "Iteration 300300, Train Loss: 2.0729, Validation Loss: 2.5119\n",
      "Iteration 300400, Train Loss: 1.9570, Validation Loss: 1.9807\n",
      "Iteration 300500, Train Loss: 2.8616, Validation Loss: 2.3407\n",
      "Iteration 300600, Train Loss: 2.2406, Validation Loss: 2.0695\n",
      "Iteration 300700, Train Loss: 2.1418, Validation Loss: 2.1810\n",
      "Iteration 300800, Train Loss: 2.3587, Validation Loss: 2.8939\n",
      "Iteration 300900, Train Loss: 2.2845, Validation Loss: 2.1616\n",
      "Iteration 301000, Train Loss: 2.3468, Validation Loss: 2.4326\n",
      "Iteration 301100, Train Loss: 2.4563, Validation Loss: 2.2269\n",
      "Iteration 301200, Train Loss: 2.5039, Validation Loss: 2.1121\n",
      "Iteration 301300, Train Loss: 2.2341, Validation Loss: 2.2659\n",
      "Iteration 301400, Train Loss: 2.2968, Validation Loss: 2.7569\n",
      "Iteration 301500, Train Loss: 2.2103, Validation Loss: 2.0735\n",
      "Iteration 301600, Train Loss: 2.6102, Validation Loss: 2.4235\n",
      "Iteration 301700, Train Loss: 2.1960, Validation Loss: 2.2805\n",
      "Iteration 301800, Train Loss: 2.3646, Validation Loss: 1.8225\n",
      "Iteration 301900, Train Loss: 2.5696, Validation Loss: 2.1945\n",
      "Iteration 302000, Train Loss: 2.6152, Validation Loss: 2.1971\n",
      "Iteration 302100, Train Loss: 1.9876, Validation Loss: 3.2404\n",
      "Iteration 302200, Train Loss: 2.4473, Validation Loss: 2.2926\n",
      "Iteration 302300, Train Loss: 2.0455, Validation Loss: 2.2193\n",
      "Iteration 302400, Train Loss: 2.2131, Validation Loss: 2.5532\n",
      "Iteration 302500, Train Loss: 2.6488, Validation Loss: 3.0139\n",
      "Iteration 302600, Train Loss: 2.4059, Validation Loss: 2.2146\n",
      "Iteration 302700, Train Loss: 2.3586, Validation Loss: 2.2314\n",
      "Iteration 302800, Train Loss: 2.2864, Validation Loss: 2.1635\n",
      "Iteration 302900, Train Loss: 1.9942, Validation Loss: 2.2249\n",
      "Iteration 303000, Train Loss: 2.0022, Validation Loss: 2.4919\n",
      "Iteration 303100, Train Loss: 1.8333, Validation Loss: 2.6615\n",
      "Iteration 303200, Train Loss: 2.4308, Validation Loss: 2.8795\n",
      "Iteration 303300, Train Loss: 2.3019, Validation Loss: 2.3406\n",
      "Iteration 303400, Train Loss: 2.5627, Validation Loss: 2.2549\n",
      "Iteration 303500, Train Loss: 2.3342, Validation Loss: 2.4507\n",
      "Iteration 303600, Train Loss: 1.8236, Validation Loss: 2.0152\n",
      "Iteration 303700, Train Loss: 2.3635, Validation Loss: 2.1342\n",
      "Iteration 303800, Train Loss: 2.2684, Validation Loss: 2.3365\n",
      "Iteration 303900, Train Loss: 2.3403, Validation Loss: 2.5281\n",
      "Iteration 304000, Train Loss: 2.2992, Validation Loss: 2.1487\n",
      "Iteration 304100, Train Loss: 2.4777, Validation Loss: 2.9582\n",
      "Iteration 304200, Train Loss: 2.3328, Validation Loss: 1.8415\n",
      "Iteration 304300, Train Loss: 2.1669, Validation Loss: 2.4517\n",
      "Iteration 304400, Train Loss: 2.1344, Validation Loss: 2.0729\n",
      "Iteration 304500, Train Loss: 2.5343, Validation Loss: 2.4445\n",
      "Iteration 304600, Train Loss: 2.3641, Validation Loss: 2.7750\n",
      "Iteration 304700, Train Loss: 2.2695, Validation Loss: 2.1015\n",
      "Iteration 304800, Train Loss: 1.8302, Validation Loss: 2.0639\n",
      "Iteration 304900, Train Loss: 2.3364, Validation Loss: 2.2104\n",
      "Iteration 305000, Train Loss: 1.6566, Validation Loss: 2.7136\n",
      "Iteration 305100, Train Loss: 2.4726, Validation Loss: 2.3046\n",
      "Iteration 305200, Train Loss: 2.3552, Validation Loss: 2.3413\n",
      "Iteration 305300, Train Loss: 2.2481, Validation Loss: 2.5032\n",
      "Iteration 305400, Train Loss: 1.8471, Validation Loss: 2.3265\n",
      "Iteration 305500, Train Loss: 2.0192, Validation Loss: 2.6469\n",
      "Iteration 305600, Train Loss: 2.4881, Validation Loss: 2.3683\n",
      "Iteration 305700, Train Loss: 2.2746, Validation Loss: 2.1664\n",
      "Iteration 305800, Train Loss: 2.2120, Validation Loss: 2.4343\n",
      "Iteration 305900, Train Loss: 2.3670, Validation Loss: 2.0170\n",
      "Iteration 306000, Train Loss: 2.2275, Validation Loss: 2.6044\n",
      "Iteration 306100, Train Loss: 2.4224, Validation Loss: 2.6559\n",
      "Iteration 306200, Train Loss: 2.5140, Validation Loss: 2.5481\n",
      "Iteration 306300, Train Loss: 2.3147, Validation Loss: 2.1163\n",
      "Iteration 306400, Train Loss: 2.3451, Validation Loss: 2.0611\n",
      "Iteration 306500, Train Loss: 2.1052, Validation Loss: 2.2523\n",
      "Iteration 306600, Train Loss: 2.5626, Validation Loss: 2.1501\n",
      "Iteration 306700, Train Loss: 2.3380, Validation Loss: 2.2816\n",
      "Iteration 306800, Train Loss: 2.5480, Validation Loss: 1.8821\n",
      "Iteration 306900, Train Loss: 1.8797, Validation Loss: 1.9125\n",
      "Iteration 307000, Train Loss: 2.1050, Validation Loss: 2.1542\n",
      "Iteration 307100, Train Loss: 2.4060, Validation Loss: 2.4523\n",
      "Iteration 307200, Train Loss: 2.2272, Validation Loss: 2.6023\n",
      "Iteration 307300, Train Loss: 2.3394, Validation Loss: 2.7555\n",
      "Iteration 307400, Train Loss: 2.1032, Validation Loss: 2.5941\n",
      "Iteration 307500, Train Loss: 2.5221, Validation Loss: 2.7531\n",
      "Iteration 307600, Train Loss: 2.6221, Validation Loss: 2.4452\n",
      "Iteration 307700, Train Loss: 2.3677, Validation Loss: 2.6702\n",
      "Iteration 307800, Train Loss: 2.0593, Validation Loss: 2.6694\n",
      "Iteration 307900, Train Loss: 2.5002, Validation Loss: 2.5008\n",
      "Iteration 308000, Train Loss: 2.0511, Validation Loss: 2.3513\n",
      "Iteration 308100, Train Loss: 2.5565, Validation Loss: 2.4539\n",
      "Iteration 308200, Train Loss: 2.1384, Validation Loss: 2.5057\n",
      "Iteration 308300, Train Loss: 2.1409, Validation Loss: 2.3521\n",
      "Iteration 308400, Train Loss: 2.5182, Validation Loss: 2.7262\n",
      "Iteration 308500, Train Loss: 2.4596, Validation Loss: 2.1491\n",
      "Iteration 308600, Train Loss: 2.2769, Validation Loss: 3.1545\n",
      "Iteration 308700, Train Loss: 2.6388, Validation Loss: 2.7957\n",
      "Iteration 308800, Train Loss: 2.6661, Validation Loss: 2.3399\n",
      "Iteration 308900, Train Loss: 2.3647, Validation Loss: 2.4355\n",
      "Iteration 309000, Train Loss: 2.1593, Validation Loss: 2.4382\n",
      "Iteration 309100, Train Loss: 1.9306, Validation Loss: 2.3834\n",
      "Iteration 309200, Train Loss: 2.1725, Validation Loss: 1.9546\n",
      "Iteration 309300, Train Loss: 2.2830, Validation Loss: 2.5228\n",
      "Iteration 309400, Train Loss: 2.2823, Validation Loss: 1.9416\n",
      "Iteration 309500, Train Loss: 2.3868, Validation Loss: 2.6014\n",
      "Iteration 309600, Train Loss: 2.2102, Validation Loss: 2.1034\n",
      "Iteration 309700, Train Loss: 2.1319, Validation Loss: 2.2855\n",
      "Iteration 309800, Train Loss: 2.3822, Validation Loss: 2.4789\n",
      "Iteration 309900, Train Loss: 2.2572, Validation Loss: 2.3774\n",
      "Iteration 310000, Train Loss: 1.9126, Validation Loss: 2.0034\n",
      "Iteration 310100, Train Loss: 2.4806, Validation Loss: 1.9946\n",
      "Iteration 310200, Train Loss: 2.2478, Validation Loss: 2.3059\n",
      "Iteration 310300, Train Loss: 2.2531, Validation Loss: 2.2175\n",
      "Iteration 310400, Train Loss: 2.6847, Validation Loss: 2.3723\n",
      "Iteration 310500, Train Loss: 2.2429, Validation Loss: 2.2784\n",
      "Iteration 310600, Train Loss: 2.3040, Validation Loss: 2.3254\n",
      "Iteration 310700, Train Loss: 2.3008, Validation Loss: 2.5470\n",
      "Iteration 310800, Train Loss: 2.3183, Validation Loss: 2.3402\n",
      "Iteration 310900, Train Loss: 2.3954, Validation Loss: 2.3583\n",
      "Iteration 311000, Train Loss: 2.2537, Validation Loss: 1.9682\n",
      "Iteration 311100, Train Loss: 2.4529, Validation Loss: 2.2431\n",
      "Iteration 311200, Train Loss: 2.4596, Validation Loss: 2.0351\n",
      "Iteration 311300, Train Loss: 2.0503, Validation Loss: 2.1266\n",
      "Iteration 311400, Train Loss: 2.1110, Validation Loss: 2.2015\n",
      "Iteration 311500, Train Loss: 2.1352, Validation Loss: 2.0792\n",
      "Iteration 311600, Train Loss: 2.8473, Validation Loss: 1.8656\n",
      "Iteration 311700, Train Loss: 2.3659, Validation Loss: 2.6746\n",
      "Iteration 311800, Train Loss: 2.5107, Validation Loss: 2.5268\n",
      "Iteration 311900, Train Loss: 1.9801, Validation Loss: 2.5431\n",
      "Iteration 312000, Train Loss: 1.7500, Validation Loss: 2.2232\n",
      "Iteration 312100, Train Loss: 2.3368, Validation Loss: 2.0972\n",
      "Iteration 312200, Train Loss: 2.4225, Validation Loss: 2.2556\n",
      "Iteration 312300, Train Loss: 2.3649, Validation Loss: 2.2638\n",
      "Iteration 312400, Train Loss: 2.1278, Validation Loss: 2.4927\n",
      "Iteration 312500, Train Loss: 2.2889, Validation Loss: 2.2709\n",
      "Iteration 312600, Train Loss: 2.3436, Validation Loss: 2.3157\n",
      "Iteration 312700, Train Loss: 2.4320, Validation Loss: 2.0958\n",
      "Iteration 312800, Train Loss: 2.1168, Validation Loss: 1.7169\n",
      "Iteration 312900, Train Loss: 2.5807, Validation Loss: 2.2113\n",
      "Iteration 313000, Train Loss: 2.1191, Validation Loss: 2.0821\n",
      "Iteration 313100, Train Loss: 2.3416, Validation Loss: 2.0539\n",
      "Iteration 313200, Train Loss: 2.0181, Validation Loss: 2.3299\n",
      "Iteration 313300, Train Loss: 2.0492, Validation Loss: 2.8392\n",
      "Iteration 313400, Train Loss: 2.0762, Validation Loss: 1.9215\n",
      "Iteration 313500, Train Loss: 2.2184, Validation Loss: 2.1041\n",
      "Iteration 313600, Train Loss: 2.1752, Validation Loss: 2.6694\n",
      "Iteration 313700, Train Loss: 2.0901, Validation Loss: 2.2537\n",
      "Iteration 313800, Train Loss: 2.6788, Validation Loss: 2.4040\n",
      "Iteration 313900, Train Loss: 2.4563, Validation Loss: 2.1236\n",
      "Iteration 314000, Train Loss: 2.4870, Validation Loss: 2.2605\n",
      "Iteration 314100, Train Loss: 2.8829, Validation Loss: 1.8696\n",
      "Iteration 314200, Train Loss: 2.2224, Validation Loss: 1.9781\n",
      "Iteration 314300, Train Loss: 2.3919, Validation Loss: 1.9852\n",
      "Iteration 314400, Train Loss: 2.3106, Validation Loss: 2.2033\n",
      "Iteration 314500, Train Loss: 2.0646, Validation Loss: 2.3418\n",
      "Iteration 314600, Train Loss: 2.5191, Validation Loss: 2.2717\n",
      "Iteration 314700, Train Loss: 2.5293, Validation Loss: 1.9356\n",
      "Iteration 314800, Train Loss: 1.8688, Validation Loss: 2.6165\n",
      "Iteration 314900, Train Loss: 2.3606, Validation Loss: 2.2112\n",
      "Iteration 315000, Train Loss: 2.1404, Validation Loss: 2.6848\n",
      "Iteration 315100, Train Loss: 2.7092, Validation Loss: 1.8340\n",
      "Iteration 315200, Train Loss: 2.4074, Validation Loss: 2.5819\n",
      "Iteration 315300, Train Loss: 2.4994, Validation Loss: 1.9878\n",
      "Iteration 315400, Train Loss: 2.1178, Validation Loss: 2.0997\n",
      "Iteration 315500, Train Loss: 2.1517, Validation Loss: 2.4886\n",
      "Iteration 315600, Train Loss: 2.0957, Validation Loss: 2.3803\n",
      "Iteration 315700, Train Loss: 2.2140, Validation Loss: 2.0940\n",
      "Iteration 315800, Train Loss: 2.1608, Validation Loss: 1.9838\n",
      "Iteration 315900, Train Loss: 2.0601, Validation Loss: 2.6076\n",
      "Iteration 316000, Train Loss: 2.2116, Validation Loss: 2.1983\n",
      "Iteration 316100, Train Loss: 2.2087, Validation Loss: 2.7367\n",
      "Iteration 316200, Train Loss: 1.7540, Validation Loss: 2.7800\n",
      "Iteration 316300, Train Loss: 2.1954, Validation Loss: 2.3257\n",
      "Iteration 316400, Train Loss: 2.0901, Validation Loss: 2.2185\n",
      "Iteration 316500, Train Loss: 2.2291, Validation Loss: 2.2477\n",
      "Iteration 316600, Train Loss: 2.5477, Validation Loss: 2.4824\n",
      "Iteration 316700, Train Loss: 1.9888, Validation Loss: 2.8072\n",
      "Iteration 316800, Train Loss: 2.1786, Validation Loss: 2.1552\n",
      "Iteration 316900, Train Loss: 1.9666, Validation Loss: 2.7629\n",
      "Iteration 317000, Train Loss: 2.4409, Validation Loss: 2.2332\n",
      "Iteration 317100, Train Loss: 2.4513, Validation Loss: 2.2251\n",
      "Iteration 317200, Train Loss: 2.3115, Validation Loss: 2.2771\n",
      "Iteration 317300, Train Loss: 2.3481, Validation Loss: 2.9455\n",
      "Iteration 317400, Train Loss: 2.0847, Validation Loss: 1.9181\n",
      "Iteration 317500, Train Loss: 2.0483, Validation Loss: 2.7669\n",
      "Iteration 317600, Train Loss: 2.4673, Validation Loss: 2.1919\n",
      "Iteration 317700, Train Loss: 2.0990, Validation Loss: 2.5583\n",
      "Iteration 317800, Train Loss: 2.4694, Validation Loss: 1.7565\n",
      "Iteration 317900, Train Loss: 2.4614, Validation Loss: 2.4129\n",
      "Iteration 318000, Train Loss: 2.1986, Validation Loss: 2.2244\n",
      "Iteration 318100, Train Loss: 2.0578, Validation Loss: 2.5098\n",
      "Iteration 318200, Train Loss: 2.5252, Validation Loss: 2.4355\n",
      "Iteration 318300, Train Loss: 2.3092, Validation Loss: 2.3423\n",
      "Iteration 318400, Train Loss: 1.8054, Validation Loss: 2.3100\n",
      "Iteration 318500, Train Loss: 2.0898, Validation Loss: 2.2966\n",
      "Iteration 318600, Train Loss: 1.9138, Validation Loss: 2.4229\n",
      "Iteration 318700, Train Loss: 2.3249, Validation Loss: 2.0793\n",
      "Iteration 318800, Train Loss: 2.3411, Validation Loss: 2.5126\n",
      "Iteration 318900, Train Loss: 2.4728, Validation Loss: 2.1260\n",
      "Iteration 319000, Train Loss: 2.7443, Validation Loss: 2.2343\n",
      "Iteration 319100, Train Loss: 2.4815, Validation Loss: 2.5830\n",
      "Iteration 319200, Train Loss: 2.4404, Validation Loss: 2.5674\n",
      "Iteration 319300, Train Loss: 2.1147, Validation Loss: 2.7643\n",
      "Iteration 319400, Train Loss: 2.7331, Validation Loss: 2.5086\n",
      "Iteration 319500, Train Loss: 2.4486, Validation Loss: 1.8274\n",
      "Iteration 319600, Train Loss: 2.1603, Validation Loss: 2.4723\n",
      "Iteration 319700, Train Loss: 2.6856, Validation Loss: 2.9393\n",
      "Iteration 319800, Train Loss: 2.7556, Validation Loss: 2.1964\n",
      "Iteration 319900, Train Loss: 1.9776, Validation Loss: 1.8537\n",
      "Iteration 320000, Train Loss: 2.6864, Validation Loss: 2.3355\n",
      "Iteration 320100, Train Loss: 2.4560, Validation Loss: 2.1505\n",
      "Iteration 320200, Train Loss: 2.3929, Validation Loss: 2.2374\n",
      "Iteration 320300, Train Loss: 2.5392, Validation Loss: 2.5668\n",
      "Iteration 320400, Train Loss: 2.8158, Validation Loss: 2.2238\n",
      "Iteration 320500, Train Loss: 1.8347, Validation Loss: 2.6388\n",
      "Iteration 320600, Train Loss: 2.1896, Validation Loss: 2.2093\n",
      "Iteration 320700, Train Loss: 2.0631, Validation Loss: 2.3169\n",
      "Iteration 320800, Train Loss: 2.2886, Validation Loss: 2.5348\n",
      "Iteration 320900, Train Loss: 2.1420, Validation Loss: 2.3365\n",
      "Iteration 321000, Train Loss: 1.9267, Validation Loss: 2.3875\n",
      "Iteration 321100, Train Loss: 2.3494, Validation Loss: 2.5721\n",
      "Iteration 321200, Train Loss: 2.3988, Validation Loss: 2.3469\n",
      "Iteration 321300, Train Loss: 2.3510, Validation Loss: 2.7956\n",
      "Iteration 321400, Train Loss: 2.3166, Validation Loss: 2.0877\n",
      "Iteration 321500, Train Loss: 2.1414, Validation Loss: 2.2188\n",
      "Iteration 321600, Train Loss: 2.4376, Validation Loss: 2.3474\n",
      "Iteration 321700, Train Loss: 2.3102, Validation Loss: 2.1277\n",
      "Iteration 321800, Train Loss: 2.1754, Validation Loss: 2.3429\n",
      "Iteration 321900, Train Loss: 2.1976, Validation Loss: 2.6138\n",
      "Iteration 322000, Train Loss: 2.2677, Validation Loss: 2.5491\n",
      "Iteration 322100, Train Loss: 2.1933, Validation Loss: 2.6548\n",
      "Iteration 322200, Train Loss: 1.9924, Validation Loss: 2.5060\n",
      "Iteration 322300, Train Loss: 2.2510, Validation Loss: 2.4561\n",
      "Iteration 322400, Train Loss: 2.1827, Validation Loss: 2.1604\n",
      "Iteration 322500, Train Loss: 2.0961, Validation Loss: 2.4725\n",
      "Iteration 322600, Train Loss: 2.8416, Validation Loss: 2.5940\n",
      "Iteration 322700, Train Loss: 2.0476, Validation Loss: 2.4713\n",
      "Iteration 322800, Train Loss: 2.3721, Validation Loss: 2.5784\n",
      "Iteration 322900, Train Loss: 2.2855, Validation Loss: 2.3134\n",
      "Iteration 323000, Train Loss: 2.7700, Validation Loss: 2.3927\n",
      "Iteration 323100, Train Loss: 2.5196, Validation Loss: 2.0303\n",
      "Iteration 323200, Train Loss: 2.6669, Validation Loss: 2.8573\n",
      "Iteration 323300, Train Loss: 2.1961, Validation Loss: 2.3193\n",
      "Iteration 323400, Train Loss: 2.2263, Validation Loss: 2.4511\n",
      "Iteration 323500, Train Loss: 2.4493, Validation Loss: 2.4691\n",
      "Iteration 323600, Train Loss: 2.1209, Validation Loss: 2.6750\n",
      "Iteration 323700, Train Loss: 2.0737, Validation Loss: 2.4273\n",
      "Iteration 323800, Train Loss: 2.4517, Validation Loss: 2.3972\n",
      "Iteration 323900, Train Loss: 2.4741, Validation Loss: 2.5632\n",
      "Iteration 324000, Train Loss: 1.9677, Validation Loss: 2.2046\n",
      "Iteration 324100, Train Loss: 1.9092, Validation Loss: 2.2641\n",
      "Iteration 324200, Train Loss: 2.5391, Validation Loss: 2.2796\n",
      "Iteration 324300, Train Loss: 2.1612, Validation Loss: 2.4733\n",
      "Iteration 324400, Train Loss: 2.4340, Validation Loss: 2.6206\n",
      "Iteration 324500, Train Loss: 2.0606, Validation Loss: 2.9245\n",
      "Iteration 324600, Train Loss: 2.6191, Validation Loss: 2.2517\n",
      "Iteration 324700, Train Loss: 1.8843, Validation Loss: 2.0568\n",
      "Iteration 324800, Train Loss: 2.1485, Validation Loss: 2.3111\n",
      "Iteration 324900, Train Loss: 2.4392, Validation Loss: 2.1073\n",
      "Iteration 325000, Train Loss: 2.4984, Validation Loss: 2.4196\n",
      "Iteration 325100, Train Loss: 2.3237, Validation Loss: 2.5321\n",
      "Iteration 325200, Train Loss: 2.2082, Validation Loss: 2.5644\n",
      "Iteration 325300, Train Loss: 2.0353, Validation Loss: 2.8847\n",
      "Iteration 325400, Train Loss: 2.7875, Validation Loss: 2.2324\n",
      "Iteration 325500, Train Loss: 2.0905, Validation Loss: 2.1497\n",
      "Iteration 325600, Train Loss: 2.3849, Validation Loss: 1.9007\n",
      "Iteration 325700, Train Loss: 1.9150, Validation Loss: 2.7419\n",
      "Iteration 325800, Train Loss: 2.0008, Validation Loss: 2.2141\n",
      "Iteration 325900, Train Loss: 2.1546, Validation Loss: 2.2573\n",
      "Iteration 326000, Train Loss: 2.7052, Validation Loss: 2.3902\n",
      "Iteration 326100, Train Loss: 2.1237, Validation Loss: 2.2392\n",
      "Iteration 326200, Train Loss: 2.5291, Validation Loss: 2.0716\n",
      "Iteration 326300, Train Loss: 2.5520, Validation Loss: 2.3349\n",
      "Iteration 326400, Train Loss: 2.4078, Validation Loss: 2.5159\n",
      "Iteration 326500, Train Loss: 3.2278, Validation Loss: 2.4985\n",
      "Iteration 326600, Train Loss: 2.0989, Validation Loss: 2.3832\n",
      "Iteration 326700, Train Loss: 2.3890, Validation Loss: 2.7303\n",
      "Iteration 326800, Train Loss: 2.4177, Validation Loss: 2.2725\n",
      "Iteration 326900, Train Loss: 1.8740, Validation Loss: 1.9050\n",
      "Iteration 327000, Train Loss: 2.5213, Validation Loss: 2.2086\n",
      "Iteration 327100, Train Loss: 2.1970, Validation Loss: 2.6818\n",
      "Iteration 327200, Train Loss: 2.4342, Validation Loss: 2.4484\n",
      "Iteration 327300, Train Loss: 2.0596, Validation Loss: 2.1602\n",
      "Iteration 327400, Train Loss: 2.2874, Validation Loss: 2.0670\n",
      "Iteration 327500, Train Loss: 2.9162, Validation Loss: 1.9888\n",
      "Iteration 327600, Train Loss: 2.2596, Validation Loss: 2.5336\n",
      "Iteration 327700, Train Loss: 2.0456, Validation Loss: 2.0955\n",
      "Iteration 327800, Train Loss: 2.1426, Validation Loss: 2.0807\n",
      "Iteration 327900, Train Loss: 2.3310, Validation Loss: 2.2594\n",
      "Iteration 328000, Train Loss: 1.8631, Validation Loss: 2.0457\n",
      "Iteration 328100, Train Loss: 2.3036, Validation Loss: 2.4312\n",
      "Iteration 328200, Train Loss: 2.1446, Validation Loss: 2.3105\n",
      "Iteration 328300, Train Loss: 2.0413, Validation Loss: 2.2341\n",
      "Iteration 328400, Train Loss: 2.5673, Validation Loss: 2.2272\n",
      "Iteration 328500, Train Loss: 2.2817, Validation Loss: 2.1765\n",
      "Iteration 328600, Train Loss: 2.3845, Validation Loss: 2.4567\n",
      "Iteration 328700, Train Loss: 2.1309, Validation Loss: 2.2008\n",
      "Iteration 328800, Train Loss: 2.6820, Validation Loss: 2.2727\n",
      "Iteration 328900, Train Loss: 2.3064, Validation Loss: 2.3436\n",
      "Iteration 329000, Train Loss: 2.1266, Validation Loss: 2.2945\n",
      "Iteration 329100, Train Loss: 2.6016, Validation Loss: 2.1107\n",
      "Iteration 329200, Train Loss: 2.5094, Validation Loss: 2.3120\n",
      "Iteration 329300, Train Loss: 2.6706, Validation Loss: 2.0123\n",
      "Iteration 329400, Train Loss: 2.6701, Validation Loss: 2.0948\n",
      "Iteration 329500, Train Loss: 2.3889, Validation Loss: 2.3192\n",
      "Iteration 329600, Train Loss: 2.0531, Validation Loss: 2.5426\n",
      "Iteration 329700, Train Loss: 2.0855, Validation Loss: 2.1184\n",
      "Iteration 329800, Train Loss: 2.4715, Validation Loss: 2.3268\n",
      "Iteration 329900, Train Loss: 2.0410, Validation Loss: 1.9099\n",
      "Iteration 330000, Train Loss: 2.6966, Validation Loss: 2.3395\n",
      "Iteration 330100, Train Loss: 2.4828, Validation Loss: 2.5979\n",
      "Iteration 330200, Train Loss: 2.6387, Validation Loss: 2.3349\n",
      "Iteration 330300, Train Loss: 2.3555, Validation Loss: 2.2170\n",
      "Iteration 330400, Train Loss: 2.2336, Validation Loss: 2.0581\n",
      "Iteration 330500, Train Loss: 2.2942, Validation Loss: 2.3994\n",
      "Iteration 330600, Train Loss: 2.1047, Validation Loss: 2.3773\n",
      "Iteration 330700, Train Loss: 2.4321, Validation Loss: 2.4755\n",
      "Iteration 330800, Train Loss: 2.1620, Validation Loss: 2.1217\n",
      "Iteration 330900, Train Loss: 2.3445, Validation Loss: 2.1491\n",
      "Iteration 331000, Train Loss: 2.3480, Validation Loss: 2.1713\n",
      "Iteration 331100, Train Loss: 2.1093, Validation Loss: 2.4746\n",
      "Iteration 331200, Train Loss: 2.6268, Validation Loss: 2.3017\n",
      "Iteration 331300, Train Loss: 2.5650, Validation Loss: 2.1870\n",
      "Iteration 331400, Train Loss: 2.5453, Validation Loss: 2.3673\n",
      "Iteration 331500, Train Loss: 2.3293, Validation Loss: 2.0794\n",
      "Iteration 331600, Train Loss: 2.0470, Validation Loss: 2.5602\n",
      "Iteration 331700, Train Loss: 2.5523, Validation Loss: 2.2303\n",
      "Iteration 331800, Train Loss: 2.3791, Validation Loss: 2.5119\n",
      "Iteration 331900, Train Loss: 2.6163, Validation Loss: 2.0865\n",
      "Iteration 332000, Train Loss: 2.2585, Validation Loss: 2.2219\n",
      "Iteration 332100, Train Loss: 2.0406, Validation Loss: 2.5642\n",
      "Iteration 332200, Train Loss: 1.8909, Validation Loss: 2.1236\n",
      "Iteration 332300, Train Loss: 2.6163, Validation Loss: 2.4964\n",
      "Iteration 332400, Train Loss: 2.2595, Validation Loss: 2.4957\n",
      "Iteration 332500, Train Loss: 2.5297, Validation Loss: 2.5772\n",
      "Iteration 332600, Train Loss: 2.3067, Validation Loss: 2.2733\n",
      "Iteration 332700, Train Loss: 1.8994, Validation Loss: 2.3304\n",
      "Iteration 332800, Train Loss: 2.4005, Validation Loss: 2.4099\n",
      "Iteration 332900, Train Loss: 2.3580, Validation Loss: 2.0993\n",
      "Iteration 333000, Train Loss: 2.2329, Validation Loss: 2.6639\n",
      "Iteration 333100, Train Loss: 2.4555, Validation Loss: 2.0370\n",
      "Iteration 333200, Train Loss: 2.0352, Validation Loss: 2.5262\n",
      "Iteration 333300, Train Loss: 1.8680, Validation Loss: 2.8903\n",
      "Iteration 333400, Train Loss: 2.3372, Validation Loss: 2.6201\n",
      "Iteration 333500, Train Loss: 2.3587, Validation Loss: 2.6248\n",
      "Iteration 333600, Train Loss: 2.3242, Validation Loss: 2.3735\n",
      "Iteration 333700, Train Loss: 2.1195, Validation Loss: 2.1561\n",
      "Iteration 333800, Train Loss: 2.1932, Validation Loss: 2.4531\n",
      "Iteration 333900, Train Loss: 2.2598, Validation Loss: 1.6691\n",
      "Iteration 334000, Train Loss: 2.5190, Validation Loss: 2.4943\n",
      "Iteration 334100, Train Loss: 2.1466, Validation Loss: 2.6806\n",
      "Iteration 334200, Train Loss: 2.1431, Validation Loss: 2.5728\n",
      "Iteration 334300, Train Loss: 2.2809, Validation Loss: 2.4059\n",
      "Iteration 334400, Train Loss: 2.0963, Validation Loss: 2.1719\n",
      "Iteration 334500, Train Loss: 2.1232, Validation Loss: 2.1958\n",
      "Iteration 334600, Train Loss: 2.2954, Validation Loss: 2.3867\n",
      "Iteration 334700, Train Loss: 2.1364, Validation Loss: 2.1502\n",
      "Iteration 334800, Train Loss: 2.4527, Validation Loss: 2.9215\n",
      "Iteration 334900, Train Loss: 1.7015, Validation Loss: 2.2620\n",
      "Iteration 335000, Train Loss: 2.4647, Validation Loss: 1.6386\n",
      "Iteration 335100, Train Loss: 1.8435, Validation Loss: 2.5296\n",
      "Iteration 335200, Train Loss: 1.9466, Validation Loss: 2.4760\n",
      "Iteration 335300, Train Loss: 2.4332, Validation Loss: 2.3891\n",
      "Iteration 335400, Train Loss: 2.0406, Validation Loss: 2.1636\n",
      "Iteration 335500, Train Loss: 2.3794, Validation Loss: 2.4204\n",
      "Iteration 335600, Train Loss: 2.2946, Validation Loss: 2.0186\n",
      "Iteration 335700, Train Loss: 2.2997, Validation Loss: 2.2980\n",
      "Iteration 335800, Train Loss: 2.3051, Validation Loss: 2.1351\n",
      "Iteration 335900, Train Loss: 2.2492, Validation Loss: 2.1446\n",
      "Iteration 336000, Train Loss: 2.6182, Validation Loss: 2.7118\n",
      "Iteration 336100, Train Loss: 2.3378, Validation Loss: 2.5250\n",
      "Iteration 336200, Train Loss: 2.3081, Validation Loss: 2.2773\n",
      "Iteration 336300, Train Loss: 2.2262, Validation Loss: 2.6576\n",
      "Iteration 336400, Train Loss: 2.3132, Validation Loss: 2.1843\n",
      "Iteration 336500, Train Loss: 2.4591, Validation Loss: 2.0364\n",
      "Iteration 336600, Train Loss: 2.5621, Validation Loss: 2.6222\n",
      "Iteration 336700, Train Loss: 2.3876, Validation Loss: 1.8292\n",
      "Iteration 336800, Train Loss: 2.4173, Validation Loss: 2.2256\n",
      "Iteration 336900, Train Loss: 2.0360, Validation Loss: 2.1151\n",
      "Iteration 337000, Train Loss: 2.2663, Validation Loss: 2.4456\n",
      "Iteration 337100, Train Loss: 2.7681, Validation Loss: 2.4274\n",
      "Iteration 337200, Train Loss: 2.3302, Validation Loss: 2.2597\n",
      "Iteration 337300, Train Loss: 2.0137, Validation Loss: 2.3612\n",
      "Iteration 337400, Train Loss: 2.2889, Validation Loss: 1.8913\n",
      "Iteration 337500, Train Loss: 2.9647, Validation Loss: 2.4754\n",
      "Iteration 337600, Train Loss: 2.4101, Validation Loss: 2.3716\n",
      "Iteration 337700, Train Loss: 2.3908, Validation Loss: 2.1575\n",
      "Iteration 337800, Train Loss: 1.8516, Validation Loss: 2.3697\n",
      "Iteration 337900, Train Loss: 2.0576, Validation Loss: 2.3602\n",
      "Iteration 338000, Train Loss: 2.1111, Validation Loss: 2.5720\n",
      "Iteration 338100, Train Loss: 2.5835, Validation Loss: 2.3623\n",
      "Iteration 338200, Train Loss: 1.8429, Validation Loss: 2.6383\n",
      "Iteration 338300, Train Loss: 2.5644, Validation Loss: 2.3713\n",
      "Iteration 338400, Train Loss: 2.3922, Validation Loss: 2.4019\n",
      "Iteration 338500, Train Loss: 2.0451, Validation Loss: 2.7044\n",
      "Iteration 338600, Train Loss: 1.9895, Validation Loss: 1.9888\n",
      "Iteration 338700, Train Loss: 2.3691, Validation Loss: 1.9886\n",
      "Iteration 338800, Train Loss: 2.4866, Validation Loss: 2.0740\n",
      "Iteration 338900, Train Loss: 2.5498, Validation Loss: 2.0095\n",
      "Iteration 339000, Train Loss: 2.4398, Validation Loss: 2.2043\n",
      "Iteration 339100, Train Loss: 2.0742, Validation Loss: 2.3376\n",
      "Iteration 339200, Train Loss: 2.1282, Validation Loss: 2.2065\n",
      "Iteration 339300, Train Loss: 2.2007, Validation Loss: 2.0183\n",
      "Iteration 339400, Train Loss: 2.2955, Validation Loss: 2.0207\n",
      "Iteration 339500, Train Loss: 2.3910, Validation Loss: 2.4546\n",
      "Iteration 339600, Train Loss: 2.1984, Validation Loss: 2.2661\n",
      "Iteration 339700, Train Loss: 2.2057, Validation Loss: 2.2565\n",
      "Iteration 339800, Train Loss: 2.1648, Validation Loss: 2.5499\n",
      "Iteration 339900, Train Loss: 1.9911, Validation Loss: 2.3984\n",
      "Iteration 340000, Train Loss: 2.1346, Validation Loss: 2.3625\n",
      "Iteration 340100, Train Loss: 1.9952, Validation Loss: 2.4268\n",
      "Iteration 340200, Train Loss: 2.1593, Validation Loss: 2.1982\n",
      "Iteration 340300, Train Loss: 1.9847, Validation Loss: 2.0690\n",
      "Iteration 340400, Train Loss: 2.1936, Validation Loss: 2.0299\n",
      "Iteration 340500, Train Loss: 2.3212, Validation Loss: 2.3697\n",
      "Iteration 340600, Train Loss: 2.4192, Validation Loss: 2.1947\n",
      "Iteration 340700, Train Loss: 2.2392, Validation Loss: 2.2012\n",
      "Iteration 340800, Train Loss: 1.7539, Validation Loss: 2.1448\n",
      "Iteration 340900, Train Loss: 2.5168, Validation Loss: 2.5606\n",
      "Iteration 341000, Train Loss: 2.1989, Validation Loss: 2.2876\n",
      "Iteration 341100, Train Loss: 2.4291, Validation Loss: 2.5506\n",
      "Iteration 341200, Train Loss: 2.3250, Validation Loss: 2.5437\n",
      "Iteration 341300, Train Loss: 2.5299, Validation Loss: 2.2494\n",
      "Iteration 341400, Train Loss: 2.0942, Validation Loss: 2.5157\n",
      "Iteration 341500, Train Loss: 2.1683, Validation Loss: 2.3505\n",
      "Iteration 341600, Train Loss: 2.1056, Validation Loss: 1.9632\n",
      "Iteration 341700, Train Loss: 2.4077, Validation Loss: 2.1916\n",
      "Iteration 341800, Train Loss: 2.7389, Validation Loss: 2.5976\n",
      "Iteration 341900, Train Loss: 2.5491, Validation Loss: 2.0286\n",
      "Iteration 342000, Train Loss: 2.5097, Validation Loss: 2.4564\n",
      "Iteration 342100, Train Loss: 2.2837, Validation Loss: 1.9204\n",
      "Iteration 342200, Train Loss: 2.1081, Validation Loss: 2.0496\n",
      "Iteration 342300, Train Loss: 2.3072, Validation Loss: 2.7763\n",
      "Iteration 342400, Train Loss: 2.6009, Validation Loss: 2.2287\n",
      "Iteration 342500, Train Loss: 2.2459, Validation Loss: 2.0334\n",
      "Iteration 342600, Train Loss: 2.4163, Validation Loss: 2.3176\n",
      "Iteration 342700, Train Loss: 2.8189, Validation Loss: 2.2208\n",
      "Iteration 342800, Train Loss: 2.3237, Validation Loss: 2.5887\n",
      "Iteration 342900, Train Loss: 2.3343, Validation Loss: 2.4551\n",
      "Iteration 343000, Train Loss: 2.3344, Validation Loss: 2.0801\n",
      "Iteration 343100, Train Loss: 1.6997, Validation Loss: 2.1007\n",
      "Iteration 343200, Train Loss: 2.4778, Validation Loss: 2.3071\n",
      "Iteration 343300, Train Loss: 2.4298, Validation Loss: 1.9289\n",
      "Iteration 343400, Train Loss: 2.5189, Validation Loss: 2.7939\n",
      "Iteration 343500, Train Loss: 2.4732, Validation Loss: 2.4546\n",
      "Iteration 343600, Train Loss: 1.7883, Validation Loss: 2.5182\n",
      "Iteration 343700, Train Loss: 1.8228, Validation Loss: 2.6915\n",
      "Iteration 343800, Train Loss: 2.0947, Validation Loss: 2.4174\n",
      "Iteration 343900, Train Loss: 2.8410, Validation Loss: 2.2257\n",
      "Iteration 344000, Train Loss: 2.0285, Validation Loss: 2.3312\n",
      "Iteration 344100, Train Loss: 2.1661, Validation Loss: 2.3389\n",
      "Iteration 344200, Train Loss: 2.3713, Validation Loss: 2.4286\n",
      "Iteration 344300, Train Loss: 1.9613, Validation Loss: 2.3231\n",
      "Iteration 344400, Train Loss: 2.3897, Validation Loss: 2.5665\n",
      "Iteration 344500, Train Loss: 3.0784, Validation Loss: 1.8172\n",
      "Iteration 344600, Train Loss: 2.4132, Validation Loss: 2.3111\n",
      "Iteration 344700, Train Loss: 2.8411, Validation Loss: 2.2781\n",
      "Iteration 344800, Train Loss: 2.3935, Validation Loss: 2.3629\n",
      "Iteration 344900, Train Loss: 1.9176, Validation Loss: 2.4929\n",
      "Iteration 345000, Train Loss: 2.0588, Validation Loss: 1.9205\n",
      "Iteration 345100, Train Loss: 3.1806, Validation Loss: 2.5827\n",
      "Iteration 345200, Train Loss: 2.4340, Validation Loss: 2.0515\n",
      "Iteration 345300, Train Loss: 2.2666, Validation Loss: 2.5066\n",
      "Iteration 345400, Train Loss: 2.3305, Validation Loss: 2.1051\n",
      "Iteration 345500, Train Loss: 2.3567, Validation Loss: 2.3236\n",
      "Iteration 345600, Train Loss: 2.4750, Validation Loss: 2.0671\n",
      "Iteration 345700, Train Loss: 2.7718, Validation Loss: 2.3905\n",
      "Iteration 345800, Train Loss: 2.5230, Validation Loss: 2.2095\n",
      "Iteration 345900, Train Loss: 2.6634, Validation Loss: 2.2707\n",
      "Iteration 346000, Train Loss: 2.0308, Validation Loss: 2.1641\n",
      "Iteration 346100, Train Loss: 2.3064, Validation Loss: 2.6001\n",
      "Iteration 346200, Train Loss: 2.4859, Validation Loss: 2.4986\n",
      "Iteration 346300, Train Loss: 2.2483, Validation Loss: 2.7116\n",
      "Iteration 346400, Train Loss: 2.3083, Validation Loss: 1.8678\n",
      "Iteration 346500, Train Loss: 2.3512, Validation Loss: 2.6345\n",
      "Iteration 346600, Train Loss: 2.2430, Validation Loss: 2.3676\n",
      "Iteration 346700, Train Loss: 2.4487, Validation Loss: 1.5931\n",
      "Iteration 346800, Train Loss: 2.6414, Validation Loss: 2.8737\n",
      "Iteration 346900, Train Loss: 2.0135, Validation Loss: 2.6375\n",
      "Iteration 347000, Train Loss: 2.0954, Validation Loss: 2.4119\n",
      "Iteration 347100, Train Loss: 2.0488, Validation Loss: 2.8156\n",
      "Iteration 347200, Train Loss: 2.1254, Validation Loss: 2.8496\n",
      "Iteration 347300, Train Loss: 2.4104, Validation Loss: 2.5097\n",
      "Iteration 347400, Train Loss: 2.2318, Validation Loss: 1.9504\n",
      "Iteration 347500, Train Loss: 2.3928, Validation Loss: 2.5310\n",
      "Iteration 347600, Train Loss: 2.6128, Validation Loss: 2.0652\n",
      "Iteration 347700, Train Loss: 2.3845, Validation Loss: 2.3581\n",
      "Iteration 347800, Train Loss: 2.5298, Validation Loss: 2.3993\n",
      "Iteration 347900, Train Loss: 2.2683, Validation Loss: 2.4039\n",
      "Iteration 348000, Train Loss: 2.6549, Validation Loss: 2.3146\n",
      "Iteration 348100, Train Loss: 1.9932, Validation Loss: 2.0127\n",
      "Iteration 348200, Train Loss: 2.3431, Validation Loss: 2.3321\n",
      "Iteration 348300, Train Loss: 2.1283, Validation Loss: 1.7616\n",
      "Iteration 348400, Train Loss: 1.9822, Validation Loss: 2.0893\n",
      "Iteration 348500, Train Loss: 2.7152, Validation Loss: 2.4263\n",
      "Iteration 348600, Train Loss: 2.5218, Validation Loss: 2.0695\n",
      "Iteration 348700, Train Loss: 1.9857, Validation Loss: 2.2998\n",
      "Iteration 348800, Train Loss: 2.0458, Validation Loss: 2.4911\n",
      "Iteration 348900, Train Loss: 2.1884, Validation Loss: 2.5126\n",
      "Iteration 349000, Train Loss: 2.3350, Validation Loss: 1.9495\n",
      "Iteration 349100, Train Loss: 2.1428, Validation Loss: 2.4297\n",
      "Iteration 349200, Train Loss: 2.3184, Validation Loss: 2.0437\n",
      "Iteration 349300, Train Loss: 1.9200, Validation Loss: 2.3317\n",
      "Iteration 349400, Train Loss: 2.4785, Validation Loss: 2.2408\n",
      "Iteration 349500, Train Loss: 2.2501, Validation Loss: 2.2355\n",
      "Iteration 349600, Train Loss: 2.3167, Validation Loss: 1.6802\n",
      "Iteration 349700, Train Loss: 2.0004, Validation Loss: 2.5076\n",
      "Iteration 349800, Train Loss: 1.8512, Validation Loss: 2.1950\n",
      "Iteration 349900, Train Loss: 2.3984, Validation Loss: 2.1795\n",
      "Iteration 350000, Train Loss: 2.2997, Validation Loss: 2.5582\n",
      "Iteration 350100, Train Loss: 1.8584, Validation Loss: 2.1517\n",
      "Iteration 350200, Train Loss: 2.8285, Validation Loss: 2.7886\n",
      "Iteration 350300, Train Loss: 2.4758, Validation Loss: 2.1349\n",
      "Iteration 350400, Train Loss: 1.8797, Validation Loss: 1.6724\n",
      "Iteration 350500, Train Loss: 2.2968, Validation Loss: 1.8469\n",
      "Iteration 350600, Train Loss: 2.5342, Validation Loss: 2.7531\n",
      "Iteration 350700, Train Loss: 2.0577, Validation Loss: 2.3515\n",
      "Iteration 350800, Train Loss: 2.5137, Validation Loss: 2.4278\n",
      "Iteration 350900, Train Loss: 2.1128, Validation Loss: 2.1508\n",
      "Iteration 351000, Train Loss: 2.6047, Validation Loss: 2.2566\n",
      "Iteration 351100, Train Loss: 2.2983, Validation Loss: 2.9081\n",
      "Iteration 351200, Train Loss: 1.9979, Validation Loss: 2.0905\n",
      "Iteration 351300, Train Loss: 2.8184, Validation Loss: 2.1064\n",
      "Iteration 351400, Train Loss: 2.8992, Validation Loss: 2.4390\n",
      "Iteration 351500, Train Loss: 2.4385, Validation Loss: 2.5979\n",
      "Iteration 351600, Train Loss: 2.5788, Validation Loss: 2.1261\n",
      "Iteration 351700, Train Loss: 2.3666, Validation Loss: 2.5960\n",
      "Iteration 351800, Train Loss: 1.9719, Validation Loss: 2.1633\n",
      "Iteration 351900, Train Loss: 2.2701, Validation Loss: 2.5543\n",
      "Iteration 352000, Train Loss: 2.1542, Validation Loss: 2.1661\n",
      "Iteration 352100, Train Loss: 2.4079, Validation Loss: 2.0878\n",
      "Iteration 352200, Train Loss: 2.4725, Validation Loss: 2.3291\n",
      "Iteration 352300, Train Loss: 2.1519, Validation Loss: 2.2487\n",
      "Iteration 352400, Train Loss: 2.2541, Validation Loss: 2.3158\n",
      "Iteration 352500, Train Loss: 2.3447, Validation Loss: 2.3478\n",
      "Iteration 352600, Train Loss: 2.3471, Validation Loss: 2.3292\n",
      "Iteration 352700, Train Loss: 2.7588, Validation Loss: 2.3410\n",
      "Iteration 352800, Train Loss: 2.3297, Validation Loss: 2.0096\n",
      "Iteration 352900, Train Loss: 2.0616, Validation Loss: 2.4961\n",
      "Iteration 353000, Train Loss: 2.2282, Validation Loss: 2.7011\n",
      "Iteration 353100, Train Loss: 1.9786, Validation Loss: 2.4589\n",
      "Iteration 353200, Train Loss: 1.9678, Validation Loss: 2.0981\n",
      "Iteration 353300, Train Loss: 2.2563, Validation Loss: 2.3500\n",
      "Iteration 353400, Train Loss: 2.5251, Validation Loss: 2.3162\n",
      "Iteration 353500, Train Loss: 2.4662, Validation Loss: 1.9503\n",
      "Iteration 353600, Train Loss: 2.3494, Validation Loss: 2.3835\n",
      "Iteration 353700, Train Loss: 2.3342, Validation Loss: 2.1566\n",
      "Iteration 353800, Train Loss: 2.1011, Validation Loss: 2.5290\n",
      "Iteration 353900, Train Loss: 2.3376, Validation Loss: 2.9203\n",
      "Iteration 354000, Train Loss: 1.9876, Validation Loss: 2.4643\n",
      "Iteration 354100, Train Loss: 2.2449, Validation Loss: 2.7472\n",
      "Iteration 354200, Train Loss: 2.2230, Validation Loss: 2.3938\n",
      "Iteration 354300, Train Loss: 2.1289, Validation Loss: 2.1890\n",
      "Iteration 354400, Train Loss: 2.2710, Validation Loss: 2.3061\n",
      "Iteration 354500, Train Loss: 2.2750, Validation Loss: 1.9649\n",
      "Iteration 354600, Train Loss: 2.4675, Validation Loss: 2.5055\n",
      "Iteration 354700, Train Loss: 2.3314, Validation Loss: 2.0908\n",
      "Iteration 354800, Train Loss: 1.8400, Validation Loss: 2.3999\n",
      "Iteration 354900, Train Loss: 2.1858, Validation Loss: 2.4607\n",
      "Iteration 355000, Train Loss: 2.1301, Validation Loss: 2.2051\n",
      "Iteration 355100, Train Loss: 1.8665, Validation Loss: 2.0619\n",
      "Iteration 355200, Train Loss: 2.1312, Validation Loss: 1.9292\n",
      "Iteration 355300, Train Loss: 2.4058, Validation Loss: 2.0526\n",
      "Iteration 355400, Train Loss: 1.9951, Validation Loss: 2.1558\n",
      "Iteration 355500, Train Loss: 1.9141, Validation Loss: 2.2716\n",
      "Iteration 355600, Train Loss: 2.4559, Validation Loss: 2.0324\n",
      "Iteration 355700, Train Loss: 2.1873, Validation Loss: 2.1989\n",
      "Iteration 355800, Train Loss: 2.3349, Validation Loss: 2.3008\n",
      "Iteration 355900, Train Loss: 2.3890, Validation Loss: 2.0717\n",
      "Iteration 356000, Train Loss: 2.0369, Validation Loss: 2.2690\n",
      "Iteration 356100, Train Loss: 2.3940, Validation Loss: 2.2429\n",
      "Iteration 356200, Train Loss: 2.6152, Validation Loss: 2.4115\n",
      "Iteration 356300, Train Loss: 2.5297, Validation Loss: 2.0788\n",
      "Iteration 356400, Train Loss: 2.2152, Validation Loss: 1.9004\n",
      "Iteration 356500, Train Loss: 2.3535, Validation Loss: 2.7672\n",
      "Iteration 356600, Train Loss: 2.4056, Validation Loss: 2.3581\n",
      "Iteration 356700, Train Loss: 2.2211, Validation Loss: 2.2537\n",
      "Iteration 356800, Train Loss: 1.9068, Validation Loss: 2.4800\n",
      "Iteration 356900, Train Loss: 2.2807, Validation Loss: 2.3529\n",
      "Iteration 357000, Train Loss: 2.2899, Validation Loss: 2.2370\n",
      "Iteration 357100, Train Loss: 2.4563, Validation Loss: 2.3872\n",
      "Iteration 357200, Train Loss: 2.3528, Validation Loss: 2.4208\n",
      "Iteration 357300, Train Loss: 2.4085, Validation Loss: 2.6294\n",
      "Iteration 357400, Train Loss: 2.2553, Validation Loss: 2.1201\n",
      "Iteration 357500, Train Loss: 2.2034, Validation Loss: 3.1640\n",
      "Iteration 357600, Train Loss: 3.1903, Validation Loss: 2.0300\n",
      "Iteration 357700, Train Loss: 2.4463, Validation Loss: 2.1825\n",
      "Iteration 357800, Train Loss: 1.8256, Validation Loss: 2.5646\n",
      "Iteration 357900, Train Loss: 2.3671, Validation Loss: 2.2443\n",
      "Iteration 358000, Train Loss: 2.5541, Validation Loss: 2.2864\n",
      "Iteration 358100, Train Loss: 1.9152, Validation Loss: 2.0986\n",
      "Iteration 358200, Train Loss: 2.3185, Validation Loss: 2.2902\n",
      "Iteration 358300, Train Loss: 2.3530, Validation Loss: 2.2912\n",
      "Iteration 358400, Train Loss: 2.1198, Validation Loss: 2.2467\n",
      "Iteration 358500, Train Loss: 2.3851, Validation Loss: 2.3735\n",
      "Iteration 358600, Train Loss: 2.9456, Validation Loss: 2.4088\n",
      "Iteration 358700, Train Loss: 2.3957, Validation Loss: 2.1368\n",
      "Iteration 358800, Train Loss: 2.6435, Validation Loss: 2.3078\n",
      "Iteration 358900, Train Loss: 2.3541, Validation Loss: 2.4253\n",
      "Iteration 359000, Train Loss: 2.5260, Validation Loss: 2.1674\n",
      "Iteration 359100, Train Loss: 2.3896, Validation Loss: 2.3884\n",
      "Iteration 359200, Train Loss: 2.2231, Validation Loss: 2.0801\n",
      "Iteration 359300, Train Loss: 2.3656, Validation Loss: 1.7574\n",
      "Iteration 359400, Train Loss: 2.3694, Validation Loss: 2.1193\n",
      "Iteration 359500, Train Loss: 2.2185, Validation Loss: 2.4483\n",
      "Iteration 359600, Train Loss: 2.5080, Validation Loss: 1.9529\n",
      "Iteration 359700, Train Loss: 2.1429, Validation Loss: 2.2319\n",
      "Iteration 359800, Train Loss: 2.5715, Validation Loss: 2.3420\n",
      "Iteration 359900, Train Loss: 2.4262, Validation Loss: 2.0836\n",
      "Iteration 360000, Train Loss: 1.9750, Validation Loss: 2.0998\n",
      "Iteration 360100, Train Loss: 2.1371, Validation Loss: 2.5375\n",
      "Iteration 360200, Train Loss: 2.5531, Validation Loss: 2.5619\n",
      "Iteration 360300, Train Loss: 2.2923, Validation Loss: 1.7451\n",
      "Iteration 360400, Train Loss: 1.9062, Validation Loss: 2.1689\n",
      "Iteration 360500, Train Loss: 2.1435, Validation Loss: 2.3570\n",
      "Iteration 360600, Train Loss: 2.3469, Validation Loss: 1.9767\n",
      "Iteration 360700, Train Loss: 2.4283, Validation Loss: 2.4168\n",
      "Iteration 360800, Train Loss: 1.7173, Validation Loss: 2.6680\n",
      "Iteration 360900, Train Loss: 2.2558, Validation Loss: 1.8102\n",
      "Iteration 361000, Train Loss: 2.1843, Validation Loss: 2.1813\n",
      "Iteration 361100, Train Loss: 2.4312, Validation Loss: 2.5752\n",
      "Iteration 361200, Train Loss: 2.1004, Validation Loss: 2.3300\n",
      "Iteration 361300, Train Loss: 2.1520, Validation Loss: 2.3136\n",
      "Iteration 361400, Train Loss: 2.7982, Validation Loss: 2.0961\n",
      "Iteration 361500, Train Loss: 2.3486, Validation Loss: 2.3612\n",
      "Iteration 361600, Train Loss: 2.0866, Validation Loss: 2.5559\n",
      "Iteration 361700, Train Loss: 2.1938, Validation Loss: 2.2945\n",
      "Iteration 361800, Train Loss: 2.5332, Validation Loss: 2.3557\n",
      "Iteration 361900, Train Loss: 2.1310, Validation Loss: 2.4727\n",
      "Iteration 362000, Train Loss: 2.3211, Validation Loss: 2.2404\n",
      "Iteration 362100, Train Loss: 2.6077, Validation Loss: 2.1574\n",
      "Iteration 362200, Train Loss: 2.1163, Validation Loss: 2.5335\n",
      "Iteration 362300, Train Loss: 2.3284, Validation Loss: 2.0264\n",
      "Iteration 362400, Train Loss: 2.0497, Validation Loss: 2.1871\n",
      "Iteration 362500, Train Loss: 2.9442, Validation Loss: 2.5263\n",
      "Iteration 362600, Train Loss: 2.0126, Validation Loss: 2.7203\n",
      "Iteration 362700, Train Loss: 2.3409, Validation Loss: 3.0851\n",
      "Iteration 362800, Train Loss: 2.2414, Validation Loss: 2.5803\n",
      "Iteration 362900, Train Loss: 1.9797, Validation Loss: 2.6901\n",
      "Iteration 363000, Train Loss: 2.3611, Validation Loss: 2.4274\n",
      "Iteration 363100, Train Loss: 2.5844, Validation Loss: 2.4848\n",
      "Iteration 363200, Train Loss: 2.5039, Validation Loss: 1.9563\n",
      "Iteration 363300, Train Loss: 2.3647, Validation Loss: 2.0830\n",
      "Iteration 363400, Train Loss: 2.3835, Validation Loss: 2.4366\n",
      "Iteration 363500, Train Loss: 2.5191, Validation Loss: 1.6601\n",
      "Iteration 363600, Train Loss: 1.9446, Validation Loss: 2.3318\n",
      "Iteration 363700, Train Loss: 2.4127, Validation Loss: 2.7353\n",
      "Iteration 363800, Train Loss: 2.4030, Validation Loss: 2.4151\n",
      "Iteration 363900, Train Loss: 2.0673, Validation Loss: 2.3085\n",
      "Iteration 364000, Train Loss: 2.3669, Validation Loss: 1.9431\n",
      "Iteration 364100, Train Loss: 2.2906, Validation Loss: 2.5929\n",
      "Iteration 364200, Train Loss: 2.3187, Validation Loss: 2.8296\n",
      "Iteration 364300, Train Loss: 2.2265, Validation Loss: 2.6171\n",
      "Iteration 364400, Train Loss: 2.3247, Validation Loss: 2.2693\n",
      "Iteration 364500, Train Loss: 2.4854, Validation Loss: 2.5937\n",
      "Iteration 364600, Train Loss: 2.4093, Validation Loss: 2.6475\n",
      "Iteration 364700, Train Loss: 2.5671, Validation Loss: 2.3839\n",
      "Iteration 364800, Train Loss: 2.2157, Validation Loss: 2.4061\n",
      "Iteration 364900, Train Loss: 2.3269, Validation Loss: 2.7273\n",
      "Iteration 365000, Train Loss: 2.4086, Validation Loss: 2.4361\n",
      "Iteration 365100, Train Loss: 2.3933, Validation Loss: 2.3401\n",
      "Iteration 365200, Train Loss: 2.2737, Validation Loss: 2.3323\n",
      "Iteration 365300, Train Loss: 2.7600, Validation Loss: 2.3831\n",
      "Iteration 365400, Train Loss: 1.9024, Validation Loss: 2.6140\n",
      "Iteration 365500, Train Loss: 2.2043, Validation Loss: 2.4342\n",
      "Iteration 365600, Train Loss: 2.7259, Validation Loss: 2.1085\n",
      "Iteration 365700, Train Loss: 2.1736, Validation Loss: 2.4786\n",
      "Iteration 365800, Train Loss: 2.5985, Validation Loss: 2.2351\n",
      "Iteration 365900, Train Loss: 1.9257, Validation Loss: 2.6346\n",
      "Iteration 366000, Train Loss: 2.3269, Validation Loss: 2.4835\n",
      "Iteration 366100, Train Loss: 2.5651, Validation Loss: 2.3462\n",
      "Iteration 366200, Train Loss: 2.3832, Validation Loss: 2.6903\n",
      "Iteration 366300, Train Loss: 2.4709, Validation Loss: 2.2209\n",
      "Iteration 366400, Train Loss: 2.4084, Validation Loss: 1.9503\n",
      "Iteration 366500, Train Loss: 2.0659, Validation Loss: 2.0450\n",
      "Iteration 366600, Train Loss: 2.2369, Validation Loss: 2.1639\n",
      "Iteration 366700, Train Loss: 2.3409, Validation Loss: 2.4738\n",
      "Iteration 366800, Train Loss: 2.0596, Validation Loss: 2.2764\n",
      "Iteration 366900, Train Loss: 2.2338, Validation Loss: 2.3632\n",
      "Iteration 367000, Train Loss: 2.5444, Validation Loss: 2.7742\n",
      "Iteration 367100, Train Loss: 2.0103, Validation Loss: 2.3589\n",
      "Iteration 367200, Train Loss: 2.7453, Validation Loss: 2.5419\n",
      "Iteration 367300, Train Loss: 2.3108, Validation Loss: 2.0165\n",
      "Iteration 367400, Train Loss: 2.1703, Validation Loss: 1.9793\n",
      "Iteration 367500, Train Loss: 1.9556, Validation Loss: 1.8473\n",
      "Iteration 367600, Train Loss: 2.5341, Validation Loss: 2.4716\n",
      "Iteration 367700, Train Loss: 2.4858, Validation Loss: 2.6891\n",
      "Iteration 367800, Train Loss: 2.4861, Validation Loss: 2.6849\n",
      "Iteration 367900, Train Loss: 2.3256, Validation Loss: 2.6155\n",
      "Iteration 368000, Train Loss: 2.5620, Validation Loss: 1.8877\n",
      "Iteration 368100, Train Loss: 2.7139, Validation Loss: 2.5853\n",
      "Iteration 368200, Train Loss: 1.9095, Validation Loss: 2.3034\n",
      "Iteration 368300, Train Loss: 2.4150, Validation Loss: 2.1555\n",
      "Iteration 368400, Train Loss: 2.2302, Validation Loss: 2.2311\n",
      "Iteration 368500, Train Loss: 2.7444, Validation Loss: 2.1189\n",
      "Iteration 368600, Train Loss: 2.3562, Validation Loss: 2.2584\n",
      "Iteration 368700, Train Loss: 2.5697, Validation Loss: 1.9427\n",
      "Iteration 368800, Train Loss: 2.4232, Validation Loss: 2.7622\n",
      "Iteration 368900, Train Loss: 2.2225, Validation Loss: 1.9318\n",
      "Iteration 369000, Train Loss: 2.1985, Validation Loss: 2.2106\n",
      "Iteration 369100, Train Loss: 2.6071, Validation Loss: 3.0295\n",
      "Iteration 369200, Train Loss: 2.4523, Validation Loss: 2.3603\n",
      "Iteration 369300, Train Loss: 1.7960, Validation Loss: 2.3101\n",
      "Iteration 369400, Train Loss: 2.4812, Validation Loss: 2.1372\n",
      "Iteration 369500, Train Loss: 2.3113, Validation Loss: 2.2592\n",
      "Iteration 369600, Train Loss: 2.4082, Validation Loss: 2.2068\n",
      "Iteration 369700, Train Loss: 2.5182, Validation Loss: 2.1251\n",
      "Iteration 369800, Train Loss: 2.5219, Validation Loss: 2.1533\n",
      "Iteration 369900, Train Loss: 2.0543, Validation Loss: 2.6619\n",
      "Iteration 370000, Train Loss: 2.3920, Validation Loss: 2.4098\n",
      "Iteration 370100, Train Loss: 2.3353, Validation Loss: 2.5356\n",
      "Iteration 370200, Train Loss: 1.9539, Validation Loss: 2.5942\n",
      "Iteration 370300, Train Loss: 2.2327, Validation Loss: 2.3194\n",
      "Iteration 370400, Train Loss: 2.1886, Validation Loss: 2.0098\n",
      "Iteration 370500, Train Loss: 2.0963, Validation Loss: 1.8214\n",
      "Iteration 370600, Train Loss: 2.5479, Validation Loss: 2.3773\n",
      "Iteration 370700, Train Loss: 2.0719, Validation Loss: 2.3287\n",
      "Iteration 370800, Train Loss: 2.3452, Validation Loss: 2.4490\n",
      "Iteration 370900, Train Loss: 2.3112, Validation Loss: 1.7349\n",
      "Iteration 371000, Train Loss: 2.2672, Validation Loss: 2.2518\n",
      "Iteration 371100, Train Loss: 1.9324, Validation Loss: 2.2039\n",
      "Iteration 371200, Train Loss: 2.4320, Validation Loss: 2.3278\n",
      "Iteration 371300, Train Loss: 2.5964, Validation Loss: 2.1785\n",
      "Iteration 371400, Train Loss: 2.0332, Validation Loss: 2.3908\n",
      "Iteration 371500, Train Loss: 2.0320, Validation Loss: 2.0794\n",
      "Iteration 371600, Train Loss: 2.1770, Validation Loss: 2.1397\n",
      "Iteration 371700, Train Loss: 2.5059, Validation Loss: 2.3446\n",
      "Iteration 371800, Train Loss: 2.3329, Validation Loss: 2.3604\n",
      "Iteration 371900, Train Loss: 2.4837, Validation Loss: 2.1909\n",
      "Iteration 372000, Train Loss: 2.3844, Validation Loss: 2.1317\n",
      "Iteration 372100, Train Loss: 2.1775, Validation Loss: 2.2478\n",
      "Iteration 372200, Train Loss: 2.1706, Validation Loss: 2.3337\n",
      "Iteration 372300, Train Loss: 1.9107, Validation Loss: 2.0663\n",
      "Iteration 372400, Train Loss: 2.3718, Validation Loss: 2.1428\n",
      "Iteration 372500, Train Loss: 2.4387, Validation Loss: 2.5042\n",
      "Iteration 372600, Train Loss: 2.0136, Validation Loss: 2.4685\n",
      "Iteration 372700, Train Loss: 2.7692, Validation Loss: 2.1490\n",
      "Iteration 372800, Train Loss: 2.3396, Validation Loss: 2.3307\n",
      "Iteration 372900, Train Loss: 2.0177, Validation Loss: 2.3906\n",
      "Iteration 373000, Train Loss: 2.3980, Validation Loss: 2.3675\n",
      "Iteration 373100, Train Loss: 2.1176, Validation Loss: 2.5840\n",
      "Iteration 373200, Train Loss: 1.9930, Validation Loss: 2.2536\n",
      "Iteration 373300, Train Loss: 2.2639, Validation Loss: 2.0905\n",
      "Iteration 373400, Train Loss: 2.7479, Validation Loss: 2.0232\n",
      "Iteration 373500, Train Loss: 2.2772, Validation Loss: 2.4097\n",
      "Iteration 373600, Train Loss: 2.3438, Validation Loss: 2.1465\n",
      "Iteration 373700, Train Loss: 2.7237, Validation Loss: 2.5864\n",
      "Iteration 373800, Train Loss: 2.0873, Validation Loss: 2.2861\n",
      "Iteration 373900, Train Loss: 2.4419, Validation Loss: 2.2595\n",
      "Iteration 374000, Train Loss: 2.3534, Validation Loss: 2.3212\n",
      "Iteration 374100, Train Loss: 2.2287, Validation Loss: 2.3780\n",
      "Iteration 374200, Train Loss: 1.9254, Validation Loss: 2.6860\n",
      "Iteration 374300, Train Loss: 2.5445, Validation Loss: 1.8362\n",
      "Iteration 374400, Train Loss: 2.9793, Validation Loss: 1.8292\n",
      "Iteration 374500, Train Loss: 1.5583, Validation Loss: 2.2856\n",
      "Iteration 374600, Train Loss: 2.0958, Validation Loss: 2.5932\n",
      "Iteration 374700, Train Loss: 2.3392, Validation Loss: 2.1400\n",
      "Iteration 374800, Train Loss: 2.2794, Validation Loss: 2.2638\n",
      "Iteration 374900, Train Loss: 1.9141, Validation Loss: 2.2635\n",
      "Iteration 375000, Train Loss: 2.7589, Validation Loss: 2.1652\n",
      "Iteration 375100, Train Loss: 2.0974, Validation Loss: 2.2301\n",
      "Iteration 375200, Train Loss: 1.9368, Validation Loss: 2.3547\n",
      "Iteration 375300, Train Loss: 2.1788, Validation Loss: 2.4985\n",
      "Iteration 375400, Train Loss: 2.0625, Validation Loss: 2.1250\n",
      "Iteration 375500, Train Loss: 2.1693, Validation Loss: 2.5108\n",
      "Iteration 375600, Train Loss: 2.2983, Validation Loss: 2.2527\n",
      "Iteration 375700, Train Loss: 2.5475, Validation Loss: 2.1363\n",
      "Iteration 375800, Train Loss: 1.9217, Validation Loss: 2.2549\n",
      "Iteration 375900, Train Loss: 2.2066, Validation Loss: 2.4455\n",
      "Iteration 376000, Train Loss: 2.3093, Validation Loss: 1.9482\n",
      "Iteration 376100, Train Loss: 2.2944, Validation Loss: 2.7880\n",
      "Iteration 376200, Train Loss: 2.2110, Validation Loss: 2.4054\n",
      "Iteration 376300, Train Loss: 2.7761, Validation Loss: 2.3795\n",
      "Iteration 376400, Train Loss: 2.4549, Validation Loss: 2.5209\n",
      "Iteration 376500, Train Loss: 2.6415, Validation Loss: 2.4635\n",
      "Iteration 376600, Train Loss: 2.5374, Validation Loss: 2.1861\n",
      "Iteration 376700, Train Loss: 2.1527, Validation Loss: 2.1392\n",
      "Iteration 376800, Train Loss: 2.4237, Validation Loss: 2.2305\n",
      "Iteration 376900, Train Loss: 1.9509, Validation Loss: 2.0585\n",
      "Iteration 377000, Train Loss: 1.9819, Validation Loss: 2.1375\n",
      "Iteration 377100, Train Loss: 2.7205, Validation Loss: 2.0675\n",
      "Iteration 377200, Train Loss: 2.4551, Validation Loss: 2.1775\n",
      "Iteration 377300, Train Loss: 2.5499, Validation Loss: 2.2787\n",
      "Iteration 377400, Train Loss: 2.5464, Validation Loss: 2.4958\n",
      "Iteration 377500, Train Loss: 2.2723, Validation Loss: 2.9186\n",
      "Iteration 377600, Train Loss: 2.1506, Validation Loss: 2.4890\n",
      "Iteration 377700, Train Loss: 2.8302, Validation Loss: 2.3562\n",
      "Iteration 377800, Train Loss: 2.2667, Validation Loss: 2.2777\n",
      "Iteration 377900, Train Loss: 2.6631, Validation Loss: 2.4122\n",
      "Iteration 378000, Train Loss: 2.4139, Validation Loss: 2.3513\n",
      "Iteration 378100, Train Loss: 2.3121, Validation Loss: 2.1050\n",
      "Iteration 378200, Train Loss: 2.5393, Validation Loss: 2.6578\n",
      "Iteration 378300, Train Loss: 2.1012, Validation Loss: 2.3697\n",
      "Iteration 378400, Train Loss: 2.0617, Validation Loss: 2.2058\n",
      "Iteration 378500, Train Loss: 1.9895, Validation Loss: 2.1086\n",
      "Iteration 378600, Train Loss: 2.6784, Validation Loss: 2.5639\n",
      "Iteration 378700, Train Loss: 2.0358, Validation Loss: 3.0529\n",
      "Iteration 378800, Train Loss: 2.1846, Validation Loss: 2.6113\n",
      "Iteration 378900, Train Loss: 2.4996, Validation Loss: 2.0077\n",
      "Iteration 379000, Train Loss: 2.6478, Validation Loss: 2.1314\n",
      "Iteration 379100, Train Loss: 2.3585, Validation Loss: 2.0979\n",
      "Iteration 379200, Train Loss: 2.1132, Validation Loss: 2.2750\n",
      "Iteration 379300, Train Loss: 1.8967, Validation Loss: 2.0495\n",
      "Iteration 379400, Train Loss: 2.2392, Validation Loss: 2.4724\n",
      "Iteration 379500, Train Loss: 2.2722, Validation Loss: 2.0923\n",
      "Iteration 379600, Train Loss: 2.5034, Validation Loss: 2.2165\n",
      "Iteration 379700, Train Loss: 2.0874, Validation Loss: 1.8964\n",
      "Iteration 379800, Train Loss: 2.6833, Validation Loss: 2.2904\n",
      "Iteration 379900, Train Loss: 2.2039, Validation Loss: 2.7215\n",
      "Iteration 380000, Train Loss: 2.5755, Validation Loss: 2.2306\n",
      "Iteration 380100, Train Loss: 2.3865, Validation Loss: 2.3617\n",
      "Iteration 380200, Train Loss: 2.2901, Validation Loss: 2.4324\n",
      "Iteration 380300, Train Loss: 2.4545, Validation Loss: 2.3686\n",
      "Iteration 380400, Train Loss: 2.6768, Validation Loss: 1.8789\n",
      "Iteration 380500, Train Loss: 2.1004, Validation Loss: 2.5203\n",
      "Iteration 380600, Train Loss: 2.2443, Validation Loss: 1.9526\n",
      "Iteration 380700, Train Loss: 2.3337, Validation Loss: 2.4111\n",
      "Iteration 380800, Train Loss: 2.2702, Validation Loss: 2.5619\n",
      "Iteration 380900, Train Loss: 2.0345, Validation Loss: 2.4910\n",
      "Iteration 381000, Train Loss: 2.4455, Validation Loss: 2.3426\n",
      "Iteration 381100, Train Loss: 1.9532, Validation Loss: 2.0246\n",
      "Iteration 381200, Train Loss: 2.2963, Validation Loss: 2.3972\n",
      "Iteration 381300, Train Loss: 2.3748, Validation Loss: 2.7276\n",
      "Iteration 381400, Train Loss: 2.1212, Validation Loss: 2.2862\n",
      "Iteration 381500, Train Loss: 2.4104, Validation Loss: 2.3730\n",
      "Iteration 381600, Train Loss: 2.5734, Validation Loss: 2.6556\n",
      "Iteration 381700, Train Loss: 2.7411, Validation Loss: 2.6362\n",
      "Iteration 381800, Train Loss: 2.2011, Validation Loss: 2.4927\n",
      "Iteration 381900, Train Loss: 2.1456, Validation Loss: 1.8582\n",
      "Iteration 382000, Train Loss: 2.4857, Validation Loss: 2.0740\n",
      "Iteration 382100, Train Loss: 2.3180, Validation Loss: 2.4890\n",
      "Iteration 382200, Train Loss: 2.5633, Validation Loss: 2.2601\n",
      "Iteration 382300, Train Loss: 2.2449, Validation Loss: 2.0217\n",
      "Iteration 382400, Train Loss: 2.1357, Validation Loss: 2.4519\n",
      "Iteration 382500, Train Loss: 2.3238, Validation Loss: 1.9365\n",
      "Iteration 382600, Train Loss: 2.5754, Validation Loss: 2.3208\n",
      "Iteration 382700, Train Loss: 2.1873, Validation Loss: 2.2709\n",
      "Iteration 382800, Train Loss: 2.5375, Validation Loss: 2.1605\n",
      "Iteration 382900, Train Loss: 2.6452, Validation Loss: 2.3773\n",
      "Iteration 383000, Train Loss: 2.4459, Validation Loss: 2.3088\n",
      "Iteration 383100, Train Loss: 2.0139, Validation Loss: 2.2221\n",
      "Iteration 383200, Train Loss: 2.3602, Validation Loss: 2.2182\n",
      "Iteration 383300, Train Loss: 2.1870, Validation Loss: 2.4122\n",
      "Iteration 383400, Train Loss: 2.4038, Validation Loss: 2.2398\n",
      "Iteration 383500, Train Loss: 2.1675, Validation Loss: 2.4401\n",
      "Iteration 383600, Train Loss: 2.2696, Validation Loss: 2.4231\n",
      "Iteration 383700, Train Loss: 2.1140, Validation Loss: 1.9587\n",
      "Iteration 383800, Train Loss: 2.2306, Validation Loss: 2.6187\n",
      "Iteration 383900, Train Loss: 2.4535, Validation Loss: 2.0578\n",
      "Iteration 384000, Train Loss: 2.9361, Validation Loss: 2.1753\n",
      "Iteration 384100, Train Loss: 2.2847, Validation Loss: 2.1618\n",
      "Iteration 384200, Train Loss: 2.2169, Validation Loss: 2.3721\n",
      "Iteration 384300, Train Loss: 2.1285, Validation Loss: 2.6182\n",
      "Iteration 384400, Train Loss: 1.9234, Validation Loss: 1.9848\n",
      "Iteration 384500, Train Loss: 2.2116, Validation Loss: 1.9863\n",
      "Iteration 384600, Train Loss: 2.5361, Validation Loss: 2.2953\n",
      "Iteration 384700, Train Loss: 2.4092, Validation Loss: 2.1711\n",
      "Iteration 384800, Train Loss: 2.2303, Validation Loss: 2.5662\n",
      "Iteration 384900, Train Loss: 2.5594, Validation Loss: 2.1442\n",
      "Iteration 385000, Train Loss: 2.2737, Validation Loss: 2.1612\n",
      "Iteration 385100, Train Loss: 2.1713, Validation Loss: 2.3643\n",
      "Iteration 385200, Train Loss: 2.4972, Validation Loss: 2.1779\n",
      "Iteration 385300, Train Loss: 2.0440, Validation Loss: 2.3987\n",
      "Iteration 385400, Train Loss: 2.1342, Validation Loss: 2.0286\n",
      "Iteration 385500, Train Loss: 2.3353, Validation Loss: 2.2002\n",
      "Iteration 385600, Train Loss: 2.2152, Validation Loss: 2.4445\n",
      "Iteration 385700, Train Loss: 2.5799, Validation Loss: 2.2357\n",
      "Iteration 385800, Train Loss: 2.2570, Validation Loss: 2.3663\n",
      "Iteration 385900, Train Loss: 2.5911, Validation Loss: 2.2826\n",
      "Iteration 386000, Train Loss: 2.1288, Validation Loss: 1.9340\n",
      "Iteration 386100, Train Loss: 2.5957, Validation Loss: 2.3833\n",
      "Iteration 386200, Train Loss: 2.2046, Validation Loss: 2.5782\n",
      "Iteration 386300, Train Loss: 2.0866, Validation Loss: 2.2194\n",
      "Iteration 386400, Train Loss: 2.0274, Validation Loss: 2.6083\n",
      "Iteration 386500, Train Loss: 2.2648, Validation Loss: 2.2574\n",
      "Iteration 386600, Train Loss: 2.3668, Validation Loss: 2.2272\n",
      "Iteration 386700, Train Loss: 2.0425, Validation Loss: 2.6159\n",
      "Iteration 386800, Train Loss: 2.6451, Validation Loss: 2.1898\n",
      "Iteration 386900, Train Loss: 2.6774, Validation Loss: 2.3659\n",
      "Iteration 387000, Train Loss: 2.2922, Validation Loss: 2.4153\n",
      "Iteration 387100, Train Loss: 2.1620, Validation Loss: 2.1585\n",
      "Iteration 387200, Train Loss: 2.1725, Validation Loss: 2.2649\n",
      "Iteration 387300, Train Loss: 2.2061, Validation Loss: 2.7063\n",
      "Iteration 387400, Train Loss: 2.4033, Validation Loss: 2.3605\n",
      "Iteration 387500, Train Loss: 2.2685, Validation Loss: 2.1596\n",
      "Iteration 387600, Train Loss: 2.3008, Validation Loss: 1.9958\n",
      "Iteration 387700, Train Loss: 2.4482, Validation Loss: 2.1465\n",
      "Iteration 387800, Train Loss: 2.5991, Validation Loss: 2.3240\n",
      "Iteration 387900, Train Loss: 1.9756, Validation Loss: 2.2874\n",
      "Iteration 388000, Train Loss: 2.2546, Validation Loss: 2.5744\n",
      "Iteration 388100, Train Loss: 2.2382, Validation Loss: 2.5747\n",
      "Iteration 388200, Train Loss: 2.3751, Validation Loss: 2.3378\n",
      "Iteration 388300, Train Loss: 2.0314, Validation Loss: 2.4464\n",
      "Iteration 388400, Train Loss: 2.3037, Validation Loss: 2.1350\n",
      "Iteration 388500, Train Loss: 2.4411, Validation Loss: 2.3751\n",
      "Iteration 388600, Train Loss: 2.0479, Validation Loss: 2.2795\n",
      "Iteration 388700, Train Loss: 2.3597, Validation Loss: 2.5588\n",
      "Iteration 388800, Train Loss: 1.8116, Validation Loss: 2.8029\n",
      "Iteration 388900, Train Loss: 2.2948, Validation Loss: 2.5196\n",
      "Iteration 389000, Train Loss: 2.1684, Validation Loss: 2.2405\n",
      "Iteration 389100, Train Loss: 2.6373, Validation Loss: 1.9429\n",
      "Iteration 389200, Train Loss: 2.4122, Validation Loss: 1.7182\n",
      "Iteration 389300, Train Loss: 1.9567, Validation Loss: 2.2714\n",
      "Iteration 389400, Train Loss: 2.4467, Validation Loss: 2.0431\n",
      "Iteration 389500, Train Loss: 2.2333, Validation Loss: 2.4362\n",
      "Iteration 389600, Train Loss: 2.0991, Validation Loss: 2.3097\n",
      "Iteration 389700, Train Loss: 2.1905, Validation Loss: 2.3435\n",
      "Iteration 389800, Train Loss: 2.5319, Validation Loss: 2.3481\n",
      "Iteration 389900, Train Loss: 2.0978, Validation Loss: 2.1167\n",
      "Iteration 390000, Train Loss: 2.3039, Validation Loss: 2.7297\n",
      "Iteration 390100, Train Loss: 2.3461, Validation Loss: 2.4532\n",
      "Iteration 390200, Train Loss: 2.2480, Validation Loss: 1.9605\n",
      "Iteration 390300, Train Loss: 1.9755, Validation Loss: 2.4866\n",
      "Iteration 390400, Train Loss: 2.3571, Validation Loss: 2.4686\n",
      "Iteration 390500, Train Loss: 1.9262, Validation Loss: 2.4740\n",
      "Iteration 390600, Train Loss: 2.2382, Validation Loss: 2.5432\n",
      "Iteration 390700, Train Loss: 2.0273, Validation Loss: 2.3042\n",
      "Iteration 390800, Train Loss: 2.5397, Validation Loss: 2.0777\n",
      "Iteration 390900, Train Loss: 2.2001, Validation Loss: 2.3682\n",
      "Iteration 391000, Train Loss: 2.2757, Validation Loss: 2.5654\n",
      "Iteration 391100, Train Loss: 2.4816, Validation Loss: 2.8691\n",
      "Iteration 391200, Train Loss: 1.6883, Validation Loss: 2.0609\n",
      "Iteration 391300, Train Loss: 2.1516, Validation Loss: 2.1347\n",
      "Iteration 391400, Train Loss: 2.6071, Validation Loss: 2.0761\n",
      "Iteration 391500, Train Loss: 2.3937, Validation Loss: 2.0745\n",
      "Iteration 391600, Train Loss: 2.2607, Validation Loss: 2.0903\n",
      "Iteration 391700, Train Loss: 2.2854, Validation Loss: 2.2728\n",
      "Iteration 391800, Train Loss: 2.3225, Validation Loss: 2.0688\n",
      "Iteration 391900, Train Loss: 2.0516, Validation Loss: 2.4270\n",
      "Iteration 392000, Train Loss: 2.2732, Validation Loss: 2.2813\n",
      "Iteration 392100, Train Loss: 2.4916, Validation Loss: 2.4388\n",
      "Iteration 392200, Train Loss: 2.6870, Validation Loss: 2.3282\n",
      "Iteration 392300, Train Loss: 1.9751, Validation Loss: 2.0807\n",
      "Iteration 392400, Train Loss: 2.0836, Validation Loss: 2.6093\n",
      "Iteration 392500, Train Loss: 2.0802, Validation Loss: 2.7073\n",
      "Iteration 392600, Train Loss: 2.4192, Validation Loss: 2.4950\n",
      "Iteration 392700, Train Loss: 2.1760, Validation Loss: 2.0550\n",
      "Iteration 392800, Train Loss: 2.4134, Validation Loss: 1.8614\n",
      "Iteration 392900, Train Loss: 2.7153, Validation Loss: 1.9311\n",
      "Iteration 393000, Train Loss: 2.2348, Validation Loss: 2.2262\n",
      "Iteration 393100, Train Loss: 2.3839, Validation Loss: 2.1701\n",
      "Iteration 393200, Train Loss: 2.0657, Validation Loss: 2.4652\n",
      "Iteration 393300, Train Loss: 2.6803, Validation Loss: 2.4273\n",
      "Iteration 393400, Train Loss: 2.0883, Validation Loss: 2.4097\n",
      "Iteration 393500, Train Loss: 1.8278, Validation Loss: 2.3332\n",
      "Iteration 393600, Train Loss: 2.2141, Validation Loss: 2.1835\n",
      "Iteration 393700, Train Loss: 2.2650, Validation Loss: 2.2675\n",
      "Iteration 393800, Train Loss: 2.4203, Validation Loss: 2.1306\n",
      "Iteration 393900, Train Loss: 1.9137, Validation Loss: 2.7358\n",
      "Iteration 394000, Train Loss: 1.8010, Validation Loss: 2.0321\n",
      "Iteration 394100, Train Loss: 2.8343, Validation Loss: 2.4843\n",
      "Iteration 394200, Train Loss: 2.0366, Validation Loss: 1.9535\n",
      "Iteration 394300, Train Loss: 2.1968, Validation Loss: 2.4476\n",
      "Iteration 394400, Train Loss: 2.7605, Validation Loss: 1.9454\n",
      "Iteration 394500, Train Loss: 1.9502, Validation Loss: 2.8358\n",
      "Iteration 394600, Train Loss: 2.5587, Validation Loss: 2.3245\n",
      "Iteration 394700, Train Loss: 2.4658, Validation Loss: 2.5938\n",
      "Iteration 394800, Train Loss: 2.6311, Validation Loss: 2.7794\n",
      "Iteration 394900, Train Loss: 2.7456, Validation Loss: 2.5177\n",
      "Iteration 395000, Train Loss: 2.0378, Validation Loss: 2.0395\n",
      "Iteration 395100, Train Loss: 2.5111, Validation Loss: 2.1169\n",
      "Iteration 395200, Train Loss: 2.1346, Validation Loss: 2.6348\n",
      "Iteration 395300, Train Loss: 2.5291, Validation Loss: 2.3956\n",
      "Iteration 395400, Train Loss: 2.4050, Validation Loss: 2.2928\n",
      "Iteration 395500, Train Loss: 2.2675, Validation Loss: 2.6601\n",
      "Iteration 395600, Train Loss: 2.1267, Validation Loss: 2.1148\n",
      "Iteration 395700, Train Loss: 2.7505, Validation Loss: 2.7871\n",
      "Iteration 395800, Train Loss: 2.2900, Validation Loss: 2.7052\n",
      "Iteration 395900, Train Loss: 2.1033, Validation Loss: 2.3038\n",
      "Iteration 396000, Train Loss: 2.2997, Validation Loss: 2.2835\n",
      "Iteration 396100, Train Loss: 2.1494, Validation Loss: 2.2548\n",
      "Iteration 396200, Train Loss: 2.2515, Validation Loss: 2.4098\n",
      "Iteration 396300, Train Loss: 2.2478, Validation Loss: 2.4854\n",
      "Iteration 396400, Train Loss: 2.4425, Validation Loss: 2.0880\n",
      "Iteration 396500, Train Loss: 2.3276, Validation Loss: 2.4415\n",
      "Iteration 396600, Train Loss: 1.9676, Validation Loss: 2.1792\n",
      "Iteration 396700, Train Loss: 2.3894, Validation Loss: 2.0342\n",
      "Iteration 396800, Train Loss: 2.0522, Validation Loss: 2.4593\n",
      "Iteration 396900, Train Loss: 2.3165, Validation Loss: 2.3286\n",
      "Iteration 397000, Train Loss: 2.2454, Validation Loss: 2.5319\n",
      "Iteration 397100, Train Loss: 2.1321, Validation Loss: 1.9639\n",
      "Iteration 397200, Train Loss: 2.2041, Validation Loss: 2.4857\n",
      "Iteration 397300, Train Loss: 2.4946, Validation Loss: 2.4772\n",
      "Iteration 397400, Train Loss: 1.8436, Validation Loss: 2.2864\n",
      "Iteration 397500, Train Loss: 2.2257, Validation Loss: 2.2934\n",
      "Iteration 397600, Train Loss: 2.7198, Validation Loss: 2.2285\n",
      "Iteration 397700, Train Loss: 2.4559, Validation Loss: 2.4283\n",
      "Iteration 397800, Train Loss: 2.2156, Validation Loss: 2.3131\n",
      "Iteration 397900, Train Loss: 2.0674, Validation Loss: 2.3285\n",
      "Iteration 398000, Train Loss: 2.1543, Validation Loss: 2.2049\n",
      "Iteration 398100, Train Loss: 2.3900, Validation Loss: 2.6590\n",
      "Iteration 398200, Train Loss: 2.2720, Validation Loss: 2.1474\n",
      "Iteration 398300, Train Loss: 2.2700, Validation Loss: 2.4038\n",
      "Iteration 398400, Train Loss: 2.4478, Validation Loss: 2.2200\n",
      "Iteration 398500, Train Loss: 2.5501, Validation Loss: 2.4079\n",
      "Iteration 398600, Train Loss: 2.4024, Validation Loss: 2.3964\n",
      "Iteration 398700, Train Loss: 2.4566, Validation Loss: 1.8303\n",
      "Iteration 398800, Train Loss: 2.1486, Validation Loss: 2.5453\n",
      "Iteration 398900, Train Loss: 2.0171, Validation Loss: 2.0516\n",
      "Iteration 399000, Train Loss: 2.0665, Validation Loss: 2.0265\n",
      "Iteration 399100, Train Loss: 2.4434, Validation Loss: 2.3549\n",
      "Iteration 399200, Train Loss: 2.5290, Validation Loss: 1.9004\n",
      "Iteration 399300, Train Loss: 2.5352, Validation Loss: 2.3674\n",
      "Iteration 399400, Train Loss: 2.1127, Validation Loss: 2.4896\n",
      "Iteration 399500, Train Loss: 2.3611, Validation Loss: 2.7333\n",
      "Iteration 399600, Train Loss: 2.6520, Validation Loss: 2.3498\n",
      "Iteration 399700, Train Loss: 2.3360, Validation Loss: 1.9285\n",
      "Iteration 399800, Train Loss: 2.0080, Validation Loss: 1.8326\n",
      "Iteration 399900, Train Loss: 2.3612, Validation Loss: 2.4257\n",
      "Iteration 400000, Train Loss: 2.3820, Validation Loss: 2.4340\n",
      "Iteration 400100, Train Loss: 2.0611, Validation Loss: 1.8985\n",
      "Iteration 400200, Train Loss: 2.0684, Validation Loss: 2.6626\n",
      "Iteration 400300, Train Loss: 2.3564, Validation Loss: 1.9721\n",
      "Iteration 400400, Train Loss: 2.8263, Validation Loss: 2.4230\n",
      "Iteration 400500, Train Loss: 2.2013, Validation Loss: 2.2387\n",
      "Iteration 400600, Train Loss: 2.3924, Validation Loss: 2.3700\n",
      "Iteration 400700, Train Loss: 2.6749, Validation Loss: 2.3447\n",
      "Iteration 400800, Train Loss: 1.9594, Validation Loss: 2.7040\n",
      "Iteration 400900, Train Loss: 2.4517, Validation Loss: 2.3053\n",
      "Iteration 401000, Train Loss: 2.4980, Validation Loss: 2.3615\n",
      "Iteration 401100, Train Loss: 2.4266, Validation Loss: 2.5687\n",
      "Iteration 401200, Train Loss: 2.1091, Validation Loss: 2.1898\n",
      "Iteration 401300, Train Loss: 2.0659, Validation Loss: 2.2030\n",
      "Iteration 401400, Train Loss: 2.3333, Validation Loss: 2.1268\n",
      "Iteration 401500, Train Loss: 2.4044, Validation Loss: 2.5771\n",
      "Iteration 401600, Train Loss: 2.3337, Validation Loss: 2.1614\n",
      "Iteration 401700, Train Loss: 2.1132, Validation Loss: 2.5029\n",
      "Iteration 401800, Train Loss: 2.1977, Validation Loss: 2.5464\n",
      "Iteration 401900, Train Loss: 2.2324, Validation Loss: 2.6850\n",
      "Iteration 402000, Train Loss: 2.1683, Validation Loss: 2.1834\n",
      "Iteration 402100, Train Loss: 2.2316, Validation Loss: 2.1348\n",
      "Iteration 402200, Train Loss: 2.3091, Validation Loss: 2.5492\n",
      "Iteration 402300, Train Loss: 2.4930, Validation Loss: 1.9957\n",
      "Iteration 402400, Train Loss: 2.1327, Validation Loss: 2.0415\n",
      "Iteration 402500, Train Loss: 2.3810, Validation Loss: 2.4300\n",
      "Iteration 402600, Train Loss: 2.1943, Validation Loss: 2.2740\n",
      "Iteration 402700, Train Loss: 2.1165, Validation Loss: 1.8064\n",
      "Iteration 402800, Train Loss: 2.3386, Validation Loss: 2.0310\n",
      "Iteration 402900, Train Loss: 2.8661, Validation Loss: 2.4756\n",
      "Iteration 403000, Train Loss: 2.6770, Validation Loss: 2.1934\n",
      "Iteration 403100, Train Loss: 2.1822, Validation Loss: 2.5675\n",
      "Iteration 403200, Train Loss: 2.4273, Validation Loss: 2.6697\n",
      "Iteration 403300, Train Loss: 2.3484, Validation Loss: 2.3697\n",
      "Iteration 403400, Train Loss: 2.3835, Validation Loss: 2.7376\n",
      "Iteration 403500, Train Loss: 2.2857, Validation Loss: 2.2265\n",
      "Iteration 403600, Train Loss: 1.9743, Validation Loss: 2.4624\n",
      "Iteration 403700, Train Loss: 2.2748, Validation Loss: 2.2142\n",
      "Iteration 403800, Train Loss: 2.3185, Validation Loss: 2.2008\n",
      "Iteration 403900, Train Loss: 2.0633, Validation Loss: 2.4701\n",
      "Iteration 404000, Train Loss: 1.9137, Validation Loss: 2.0747\n",
      "Iteration 404100, Train Loss: 2.3709, Validation Loss: 2.3341\n",
      "Iteration 404200, Train Loss: 1.9835, Validation Loss: 2.1830\n",
      "Iteration 404300, Train Loss: 1.9456, Validation Loss: 2.5562\n",
      "Iteration 404400, Train Loss: 1.9568, Validation Loss: 2.4545\n",
      "Iteration 404500, Train Loss: 1.8150, Validation Loss: 2.1444\n",
      "Iteration 404600, Train Loss: 2.2704, Validation Loss: 2.1633\n",
      "Iteration 404700, Train Loss: 2.0410, Validation Loss: 1.8102\n",
      "Iteration 404800, Train Loss: 2.3212, Validation Loss: 2.2895\n",
      "Iteration 404900, Train Loss: 2.4538, Validation Loss: 2.1868\n",
      "Iteration 405000, Train Loss: 2.3123, Validation Loss: 2.2860\n",
      "Iteration 405100, Train Loss: 2.1771, Validation Loss: 2.2171\n",
      "Iteration 405200, Train Loss: 1.9661, Validation Loss: 2.2525\n",
      "Iteration 405300, Train Loss: 2.5513, Validation Loss: 2.2153\n",
      "Iteration 405400, Train Loss: 2.0605, Validation Loss: 2.3039\n",
      "Iteration 405500, Train Loss: 2.2797, Validation Loss: 2.3680\n",
      "Iteration 405600, Train Loss: 2.2652, Validation Loss: 2.6183\n",
      "Iteration 405700, Train Loss: 2.2850, Validation Loss: 2.4189\n",
      "Iteration 405800, Train Loss: 1.8566, Validation Loss: 2.0954\n",
      "Iteration 405900, Train Loss: 2.1385, Validation Loss: 1.9199\n",
      "Iteration 406000, Train Loss: 2.1005, Validation Loss: 2.1938\n",
      "Iteration 406100, Train Loss: 1.9533, Validation Loss: 2.1461\n",
      "Iteration 406200, Train Loss: 2.4931, Validation Loss: 2.3982\n",
      "Iteration 406300, Train Loss: 2.2093, Validation Loss: 2.1728\n",
      "Iteration 406400, Train Loss: 2.6406, Validation Loss: 1.9764\n",
      "Iteration 406500, Train Loss: 2.2195, Validation Loss: 1.8336\n",
      "Iteration 406600, Train Loss: 2.2794, Validation Loss: 1.9084\n",
      "Iteration 406700, Train Loss: 2.7224, Validation Loss: 2.2482\n",
      "Iteration 406800, Train Loss: 1.8664, Validation Loss: 2.3373\n",
      "Iteration 406900, Train Loss: 2.3783, Validation Loss: 2.0150\n",
      "Iteration 407000, Train Loss: 2.7244, Validation Loss: 2.0752\n",
      "Iteration 407100, Train Loss: 2.1680, Validation Loss: 2.1111\n",
      "Iteration 407200, Train Loss: 2.0947, Validation Loss: 2.1979\n",
      "Iteration 407300, Train Loss: 2.3781, Validation Loss: 2.0514\n",
      "Iteration 407400, Train Loss: 2.2048, Validation Loss: 2.9449\n",
      "Iteration 407500, Train Loss: 2.0396, Validation Loss: 2.6811\n",
      "Iteration 407600, Train Loss: 2.0702, Validation Loss: 2.3587\n",
      "Iteration 407700, Train Loss: 2.4870, Validation Loss: 2.4942\n",
      "Iteration 407800, Train Loss: 2.5527, Validation Loss: 2.1754\n",
      "Iteration 407900, Train Loss: 2.5604, Validation Loss: 2.5040\n",
      "Iteration 408000, Train Loss: 2.5007, Validation Loss: 2.3949\n",
      "Iteration 408100, Train Loss: 2.4696, Validation Loss: 2.2812\n",
      "Iteration 408200, Train Loss: 3.1946, Validation Loss: 1.8728\n",
      "Iteration 408300, Train Loss: 2.0228, Validation Loss: 1.9705\n",
      "Iteration 408400, Train Loss: 2.5267, Validation Loss: 2.3049\n",
      "Iteration 408500, Train Loss: 2.1379, Validation Loss: 1.9407\n",
      "Iteration 408600, Train Loss: 2.6646, Validation Loss: 2.6026\n",
      "Iteration 408700, Train Loss: 1.9499, Validation Loss: 2.7664\n",
      "Iteration 408800, Train Loss: 2.1364, Validation Loss: 2.0521\n",
      "Iteration 408900, Train Loss: 1.9788, Validation Loss: 2.5034\n",
      "Iteration 409000, Train Loss: 2.4130, Validation Loss: 2.0747\n",
      "Iteration 409100, Train Loss: 2.0783, Validation Loss: 2.3334\n",
      "Iteration 409200, Train Loss: 2.9337, Validation Loss: 2.1233\n",
      "Iteration 409300, Train Loss: 1.9457, Validation Loss: 1.8502\n",
      "Iteration 409400, Train Loss: 2.2124, Validation Loss: 2.0006\n",
      "Iteration 409500, Train Loss: 2.1088, Validation Loss: 2.2045\n",
      "Iteration 409600, Train Loss: 2.5591, Validation Loss: 2.1888\n",
      "Iteration 409700, Train Loss: 1.9524, Validation Loss: 2.2208\n",
      "Iteration 409800, Train Loss: 2.4009, Validation Loss: 2.1480\n",
      "Iteration 409900, Train Loss: 2.3091, Validation Loss: 2.0404\n",
      "Iteration 410000, Train Loss: 1.8971, Validation Loss: 2.2913\n",
      "Iteration 410100, Train Loss: 2.3291, Validation Loss: 2.0542\n",
      "Iteration 410200, Train Loss: 2.3282, Validation Loss: 2.1960\n",
      "Iteration 410300, Train Loss: 2.3182, Validation Loss: 2.1454\n",
      "Iteration 410400, Train Loss: 2.3801, Validation Loss: 2.2501\n",
      "Iteration 410500, Train Loss: 1.9733, Validation Loss: 2.3609\n",
      "Iteration 410600, Train Loss: 2.5475, Validation Loss: 2.3599\n",
      "Iteration 410700, Train Loss: 2.0260, Validation Loss: 2.2112\n",
      "Iteration 410800, Train Loss: 2.4800, Validation Loss: 2.5079\n",
      "Iteration 410900, Train Loss: 2.4345, Validation Loss: 2.6554\n",
      "Iteration 411000, Train Loss: 2.2864, Validation Loss: 2.1636\n",
      "Iteration 411100, Train Loss: 2.3473, Validation Loss: 2.6848\n",
      "Iteration 411200, Train Loss: 2.4833, Validation Loss: 2.3243\n",
      "Iteration 411300, Train Loss: 2.5762, Validation Loss: 2.1016\n",
      "Iteration 411400, Train Loss: 2.8294, Validation Loss: 2.6831\n",
      "Iteration 411500, Train Loss: 2.3022, Validation Loss: 2.2199\n",
      "Iteration 411600, Train Loss: 2.1855, Validation Loss: 2.6408\n",
      "Iteration 411700, Train Loss: 1.9240, Validation Loss: 2.6633\n",
      "Iteration 411800, Train Loss: 2.3988, Validation Loss: 2.6853\n",
      "Iteration 411900, Train Loss: 2.4493, Validation Loss: 2.7758\n",
      "Iteration 412000, Train Loss: 2.0746, Validation Loss: 2.3917\n",
      "Iteration 412100, Train Loss: 2.4876, Validation Loss: 2.0122\n",
      "Iteration 412200, Train Loss: 2.7998, Validation Loss: 2.3991\n",
      "Iteration 412300, Train Loss: 2.3423, Validation Loss: 2.2003\n",
      "Iteration 412400, Train Loss: 2.0412, Validation Loss: 2.6039\n",
      "Iteration 412500, Train Loss: 2.1171, Validation Loss: 2.5278\n",
      "Iteration 412600, Train Loss: 2.2912, Validation Loss: 2.0732\n",
      "Iteration 412700, Train Loss: 2.5807, Validation Loss: 2.3356\n",
      "Iteration 412800, Train Loss: 1.9274, Validation Loss: 2.2047\n",
      "Iteration 412900, Train Loss: 2.7842, Validation Loss: 2.4534\n",
      "Iteration 413000, Train Loss: 2.5590, Validation Loss: 2.6044\n",
      "Iteration 413100, Train Loss: 1.9347, Validation Loss: 2.3665\n",
      "Iteration 413200, Train Loss: 2.6242, Validation Loss: 2.7268\n",
      "Iteration 413300, Train Loss: 2.1028, Validation Loss: 2.2956\n",
      "Iteration 413400, Train Loss: 2.0873, Validation Loss: 2.0360\n",
      "Iteration 413500, Train Loss: 2.0091, Validation Loss: 2.0219\n",
      "Iteration 413600, Train Loss: 2.2079, Validation Loss: 2.2476\n",
      "Iteration 413700, Train Loss: 2.3188, Validation Loss: 2.3842\n",
      "Iteration 413800, Train Loss: 2.4120, Validation Loss: 2.1228\n",
      "Iteration 413900, Train Loss: 2.5235, Validation Loss: 2.2863\n",
      "Iteration 414000, Train Loss: 2.2686, Validation Loss: 2.2509\n",
      "Iteration 414100, Train Loss: 2.4125, Validation Loss: 2.3445\n",
      "Iteration 414200, Train Loss: 1.9776, Validation Loss: 2.7039\n",
      "Iteration 414300, Train Loss: 1.7956, Validation Loss: 1.9680\n",
      "Iteration 414400, Train Loss: 2.4398, Validation Loss: 2.5308\n",
      "Iteration 414500, Train Loss: 2.1869, Validation Loss: 2.0512\n",
      "Iteration 414600, Train Loss: 2.1452, Validation Loss: 2.5855\n",
      "Iteration 414700, Train Loss: 2.0737, Validation Loss: 2.0775\n",
      "Iteration 414800, Train Loss: 2.3178, Validation Loss: 2.2813\n",
      "Iteration 414900, Train Loss: 1.8028, Validation Loss: 2.1846\n",
      "Iteration 415000, Train Loss: 1.9148, Validation Loss: 2.1618\n",
      "Iteration 415100, Train Loss: 1.9770, Validation Loss: 2.4234\n",
      "Iteration 415200, Train Loss: 2.2144, Validation Loss: 2.5131\n",
      "Iteration 415300, Train Loss: 2.0456, Validation Loss: 2.5529\n",
      "Iteration 415400, Train Loss: 2.3368, Validation Loss: 2.3434\n",
      "Iteration 415500, Train Loss: 2.1546, Validation Loss: 2.2740\n",
      "Iteration 415600, Train Loss: 2.3102, Validation Loss: 2.1178\n",
      "Iteration 415700, Train Loss: 2.3720, Validation Loss: 2.4418\n",
      "Iteration 415800, Train Loss: 1.9432, Validation Loss: 1.9791\n",
      "Iteration 415900, Train Loss: 2.2184, Validation Loss: 2.6383\n",
      "Iteration 416000, Train Loss: 2.2964, Validation Loss: 2.7443\n",
      "Iteration 416100, Train Loss: 2.4687, Validation Loss: 2.3847\n",
      "Iteration 416200, Train Loss: 2.0316, Validation Loss: 2.3864\n",
      "Iteration 416300, Train Loss: 3.1179, Validation Loss: 2.3381\n",
      "Iteration 416400, Train Loss: 2.5248, Validation Loss: 2.4275\n",
      "Iteration 416500, Train Loss: 1.8193, Validation Loss: 2.0862\n",
      "Iteration 416600, Train Loss: 1.9662, Validation Loss: 2.0722\n",
      "Iteration 416700, Train Loss: 2.1064, Validation Loss: 2.5389\n",
      "Iteration 416800, Train Loss: 2.1574, Validation Loss: 2.2769\n",
      "Iteration 416900, Train Loss: 2.2929, Validation Loss: 2.5291\n",
      "Iteration 417000, Train Loss: 2.2421, Validation Loss: 1.7659\n",
      "Iteration 417100, Train Loss: 2.2666, Validation Loss: 2.2168\n",
      "Iteration 417200, Train Loss: 2.4234, Validation Loss: 2.7298\n",
      "Iteration 417300, Train Loss: 2.3308, Validation Loss: 2.4073\n",
      "Iteration 417400, Train Loss: 2.6888, Validation Loss: 2.1132\n",
      "Iteration 417500, Train Loss: 2.6219, Validation Loss: 2.3301\n",
      "Iteration 417600, Train Loss: 2.7127, Validation Loss: 1.6996\n",
      "Iteration 417700, Train Loss: 2.4788, Validation Loss: 2.2863\n",
      "Iteration 417800, Train Loss: 2.6563, Validation Loss: 2.3081\n",
      "Iteration 417900, Train Loss: 2.1852, Validation Loss: 2.3001\n",
      "Iteration 418000, Train Loss: 2.1167, Validation Loss: 2.2541\n",
      "Iteration 418100, Train Loss: 2.4569, Validation Loss: 2.2660\n",
      "Iteration 418200, Train Loss: 2.0913, Validation Loss: 1.9433\n",
      "Iteration 418300, Train Loss: 2.3240, Validation Loss: 2.5574\n",
      "Iteration 418400, Train Loss: 2.4028, Validation Loss: 2.2462\n",
      "Iteration 418500, Train Loss: 1.8146, Validation Loss: 2.0798\n",
      "Iteration 418600, Train Loss: 2.2698, Validation Loss: 2.1182\n",
      "Iteration 418700, Train Loss: 2.3112, Validation Loss: 2.0653\n",
      "Iteration 418800, Train Loss: 2.2345, Validation Loss: 2.2217\n",
      "Iteration 418900, Train Loss: 2.6590, Validation Loss: 2.0967\n",
      "Iteration 419000, Train Loss: 2.3796, Validation Loss: 2.3871\n",
      "Iteration 419100, Train Loss: 2.0049, Validation Loss: 2.3673\n",
      "Iteration 419200, Train Loss: 2.2023, Validation Loss: 2.2334\n",
      "Iteration 419300, Train Loss: 2.3495, Validation Loss: 2.6078\n",
      "Iteration 419400, Train Loss: 2.6697, Validation Loss: 2.2219\n",
      "Iteration 419500, Train Loss: 2.4206, Validation Loss: 2.9173\n",
      "Iteration 419600, Train Loss: 1.6865, Validation Loss: 2.1799\n",
      "Iteration 419700, Train Loss: 2.2585, Validation Loss: 2.5199\n",
      "Iteration 419800, Train Loss: 2.0748, Validation Loss: 2.4188\n",
      "Iteration 419900, Train Loss: 2.3795, Validation Loss: 2.3591\n",
      "Iteration 420000, Train Loss: 2.1070, Validation Loss: 2.4797\n",
      "Iteration 420100, Train Loss: 1.8269, Validation Loss: 1.8468\n",
      "Iteration 420200, Train Loss: 2.1894, Validation Loss: 1.7015\n",
      "Iteration 420300, Train Loss: 2.5490, Validation Loss: 2.2453\n",
      "Iteration 420400, Train Loss: 2.3619, Validation Loss: 2.2383\n",
      "Iteration 420500, Train Loss: 2.2408, Validation Loss: 2.3160\n",
      "Iteration 420600, Train Loss: 2.3545, Validation Loss: 2.4680\n",
      "Iteration 420700, Train Loss: 2.4285, Validation Loss: 1.8266\n",
      "Iteration 420800, Train Loss: 2.0630, Validation Loss: 2.3411\n",
      "Iteration 420900, Train Loss: 2.3770, Validation Loss: 2.1230\n",
      "Iteration 421000, Train Loss: 2.1556, Validation Loss: 2.4825\n",
      "Iteration 421100, Train Loss: 1.9787, Validation Loss: 2.0625\n",
      "Iteration 421200, Train Loss: 2.5012, Validation Loss: 2.1345\n",
      "Iteration 421300, Train Loss: 2.3094, Validation Loss: 2.4105\n",
      "Iteration 421400, Train Loss: 2.1191, Validation Loss: 1.8787\n",
      "Iteration 421500, Train Loss: 2.3688, Validation Loss: 2.2912\n",
      "Iteration 421600, Train Loss: 2.5622, Validation Loss: 2.2416\n",
      "Iteration 421700, Train Loss: 2.2615, Validation Loss: 2.7321\n",
      "Iteration 421800, Train Loss: 1.9773, Validation Loss: 2.1219\n",
      "Iteration 421900, Train Loss: 2.4540, Validation Loss: 1.9981\n",
      "Iteration 422000, Train Loss: 2.6226, Validation Loss: 1.8348\n",
      "Iteration 422100, Train Loss: 2.0039, Validation Loss: 2.6577\n",
      "Iteration 422200, Train Loss: 2.4885, Validation Loss: 2.5604\n",
      "Iteration 422300, Train Loss: 2.1442, Validation Loss: 2.1735\n",
      "Iteration 422400, Train Loss: 2.0755, Validation Loss: 2.5223\n",
      "Iteration 422500, Train Loss: 2.4490, Validation Loss: 2.0535\n",
      "Iteration 422600, Train Loss: 2.2684, Validation Loss: 2.1254\n",
      "Iteration 422700, Train Loss: 2.1387, Validation Loss: 2.2277\n",
      "Iteration 422800, Train Loss: 2.3461, Validation Loss: 2.3366\n",
      "Iteration 422900, Train Loss: 2.0073, Validation Loss: 2.6531\n",
      "Iteration 423000, Train Loss: 2.2562, Validation Loss: 2.6963\n",
      "Iteration 423100, Train Loss: 2.3669, Validation Loss: 2.2296\n",
      "Iteration 423200, Train Loss: 1.9498, Validation Loss: 2.1362\n",
      "Iteration 423300, Train Loss: 2.5609, Validation Loss: 2.4509\n",
      "Iteration 423400, Train Loss: 2.3646, Validation Loss: 2.1534\n",
      "Iteration 423500, Train Loss: 2.2618, Validation Loss: 2.0094\n",
      "Iteration 423600, Train Loss: 2.1936, Validation Loss: 2.1355\n",
      "Iteration 423700, Train Loss: 2.6063, Validation Loss: 2.1254\n",
      "Iteration 423800, Train Loss: 1.8951, Validation Loss: 2.0677\n",
      "Iteration 423900, Train Loss: 2.2513, Validation Loss: 2.2204\n",
      "Iteration 424000, Train Loss: 2.4115, Validation Loss: 2.1321\n",
      "Iteration 424100, Train Loss: 2.2354, Validation Loss: 2.3758\n",
      "Iteration 424200, Train Loss: 2.0987, Validation Loss: 2.2509\n",
      "Iteration 424300, Train Loss: 2.8703, Validation Loss: 2.5367\n",
      "Iteration 424400, Train Loss: 2.4447, Validation Loss: 2.2420\n",
      "Iteration 424500, Train Loss: 2.6044, Validation Loss: 2.8800\n",
      "Iteration 424600, Train Loss: 2.3806, Validation Loss: 2.1789\n",
      "Iteration 424700, Train Loss: 2.1297, Validation Loss: 2.3638\n",
      "Iteration 424800, Train Loss: 2.1494, Validation Loss: 2.8179\n",
      "Iteration 424900, Train Loss: 2.1786, Validation Loss: 2.4887\n",
      "Iteration 425000, Train Loss: 2.3019, Validation Loss: 2.3499\n",
      "Iteration 425100, Train Loss: 2.5316, Validation Loss: 1.9352\n",
      "Iteration 425200, Train Loss: 2.4792, Validation Loss: 1.8857\n",
      "Iteration 425300, Train Loss: 2.5011, Validation Loss: 2.4013\n",
      "Iteration 425400, Train Loss: 2.5362, Validation Loss: 2.0460\n",
      "Iteration 425500, Train Loss: 1.8753, Validation Loss: 1.9751\n",
      "Iteration 425600, Train Loss: 2.3379, Validation Loss: 2.3997\n",
      "Iteration 425700, Train Loss: 2.1101, Validation Loss: 2.4319\n",
      "Iteration 425800, Train Loss: 1.9301, Validation Loss: 2.3664\n",
      "Iteration 425900, Train Loss: 2.6171, Validation Loss: 2.6846\n",
      "Iteration 426000, Train Loss: 2.1889, Validation Loss: 2.3912\n",
      "Iteration 426100, Train Loss: 2.1134, Validation Loss: 1.9336\n",
      "Iteration 426200, Train Loss: 2.0608, Validation Loss: 2.1077\n",
      "Iteration 426300, Train Loss: 2.8674, Validation Loss: 2.5319\n",
      "Iteration 426400, Train Loss: 2.1160, Validation Loss: 2.4752\n",
      "Iteration 426500, Train Loss: 2.0739, Validation Loss: 2.3850\n",
      "Iteration 426600, Train Loss: 2.0731, Validation Loss: 2.4751\n",
      "Iteration 426700, Train Loss: 2.0797, Validation Loss: 2.2699\n",
      "Iteration 426800, Train Loss: 2.3636, Validation Loss: 2.5293\n",
      "Iteration 426900, Train Loss: 2.0297, Validation Loss: 2.2240\n",
      "Iteration 427000, Train Loss: 2.6518, Validation Loss: 2.5655\n",
      "Iteration 427100, Train Loss: 2.2312, Validation Loss: 2.6241\n",
      "Iteration 427200, Train Loss: 2.2713, Validation Loss: 2.6160\n",
      "Iteration 427300, Train Loss: 2.3655, Validation Loss: 2.1983\n",
      "Iteration 427400, Train Loss: 2.3542, Validation Loss: 2.3879\n",
      "Iteration 427500, Train Loss: 2.0366, Validation Loss: 1.9510\n",
      "Iteration 427600, Train Loss: 2.8693, Validation Loss: 2.2358\n",
      "Iteration 427700, Train Loss: 1.9734, Validation Loss: 2.0613\n",
      "Iteration 427800, Train Loss: 2.3929, Validation Loss: 1.8059\n",
      "Iteration 427900, Train Loss: 2.4019, Validation Loss: 2.3401\n",
      "Iteration 428000, Train Loss: 2.2515, Validation Loss: 2.1388\n",
      "Iteration 428100, Train Loss: 2.1789, Validation Loss: 2.7090\n",
      "Iteration 428200, Train Loss: 2.5736, Validation Loss: 2.3021\n",
      "Iteration 428300, Train Loss: 2.7489, Validation Loss: 2.6521\n",
      "Iteration 428400, Train Loss: 1.9267, Validation Loss: 2.2825\n",
      "Iteration 428500, Train Loss: 2.0623, Validation Loss: 2.2336\n",
      "Iteration 428600, Train Loss: 2.0363, Validation Loss: 2.1588\n",
      "Iteration 428700, Train Loss: 2.3078, Validation Loss: 2.1124\n",
      "Iteration 428800, Train Loss: 2.2301, Validation Loss: 2.4464\n",
      "Iteration 428900, Train Loss: 2.2613, Validation Loss: 2.7208\n",
      "Iteration 429000, Train Loss: 2.3275, Validation Loss: 2.4766\n",
      "Iteration 429100, Train Loss: 2.0543, Validation Loss: 2.5371\n",
      "Iteration 429200, Train Loss: 2.5252, Validation Loss: 2.0735\n",
      "Iteration 429300, Train Loss: 2.3946, Validation Loss: 2.2937\n",
      "Iteration 429400, Train Loss: 2.4009, Validation Loss: 2.2662\n",
      "Iteration 429500, Train Loss: 2.9854, Validation Loss: 1.9065\n",
      "Iteration 429600, Train Loss: 2.6620, Validation Loss: 2.1977\n",
      "Iteration 429700, Train Loss: 2.4231, Validation Loss: 2.3596\n",
      "Iteration 429800, Train Loss: 2.2697, Validation Loss: 2.2997\n",
      "Iteration 429900, Train Loss: 2.4036, Validation Loss: 2.4933\n",
      "Iteration 430000, Train Loss: 2.7485, Validation Loss: 2.4868\n",
      "Iteration 430100, Train Loss: 2.2089, Validation Loss: 1.8629\n",
      "Iteration 430200, Train Loss: 2.2288, Validation Loss: 2.6400\n",
      "Iteration 430300, Train Loss: 2.0529, Validation Loss: 2.2147\n",
      "Iteration 430400, Train Loss: 2.2348, Validation Loss: 2.2025\n",
      "Iteration 430500, Train Loss: 2.4172, Validation Loss: 2.6524\n",
      "Iteration 430600, Train Loss: 2.1258, Validation Loss: 2.3600\n",
      "Iteration 430700, Train Loss: 2.2799, Validation Loss: 2.5493\n",
      "Iteration 430800, Train Loss: 2.3285, Validation Loss: 2.0136\n",
      "Iteration 430900, Train Loss: 2.5021, Validation Loss: 2.4327\n",
      "Iteration 431000, Train Loss: 2.3533, Validation Loss: 2.1556\n",
      "Iteration 431100, Train Loss: 2.3665, Validation Loss: 2.3245\n",
      "Iteration 431200, Train Loss: 2.2843, Validation Loss: 2.0629\n",
      "Iteration 431300, Train Loss: 2.3186, Validation Loss: 2.3364\n",
      "Iteration 431400, Train Loss: 2.3358, Validation Loss: 2.2040\n",
      "Iteration 431500, Train Loss: 2.5130, Validation Loss: 2.2675\n",
      "Iteration 431600, Train Loss: 2.0987, Validation Loss: 2.4647\n",
      "Iteration 431700, Train Loss: 2.5583, Validation Loss: 2.2428\n",
      "Iteration 431800, Train Loss: 2.0679, Validation Loss: 2.4328\n",
      "Iteration 431900, Train Loss: 2.3370, Validation Loss: 2.2176\n",
      "Iteration 432000, Train Loss: 2.1684, Validation Loss: 2.5744\n",
      "Iteration 432100, Train Loss: 2.1996, Validation Loss: 2.7971\n",
      "Iteration 432200, Train Loss: 2.3465, Validation Loss: 2.5374\n",
      "Iteration 432300, Train Loss: 2.3991, Validation Loss: 2.4543\n",
      "Iteration 432400, Train Loss: 2.3912, Validation Loss: 2.0047\n",
      "Iteration 432500, Train Loss: 2.1120, Validation Loss: 2.1496\n",
      "Iteration 432600, Train Loss: 2.4288, Validation Loss: 2.3757\n",
      "Iteration 432700, Train Loss: 1.8685, Validation Loss: 2.3865\n",
      "Iteration 432800, Train Loss: 2.5917, Validation Loss: 2.2796\n",
      "Iteration 432900, Train Loss: 2.3892, Validation Loss: 2.4687\n",
      "Iteration 433000, Train Loss: 1.8824, Validation Loss: 2.1917\n",
      "Iteration 433100, Train Loss: 2.1200, Validation Loss: 2.2220\n",
      "Iteration 433200, Train Loss: 2.2273, Validation Loss: 2.5113\n",
      "Iteration 433300, Train Loss: 2.3193, Validation Loss: 2.3414\n",
      "Iteration 433400, Train Loss: 2.1846, Validation Loss: 2.2656\n",
      "Iteration 433500, Train Loss: 2.4629, Validation Loss: 2.1482\n",
      "Iteration 433600, Train Loss: 1.9058, Validation Loss: 2.5401\n",
      "Iteration 433700, Train Loss: 2.1531, Validation Loss: 2.0812\n",
      "Iteration 433800, Train Loss: 2.2577, Validation Loss: 2.2753\n",
      "Iteration 433900, Train Loss: 2.2685, Validation Loss: 2.4224\n",
      "Iteration 434000, Train Loss: 2.2508, Validation Loss: 2.3233\n",
      "Iteration 434100, Train Loss: 2.5803, Validation Loss: 2.3497\n",
      "Iteration 434200, Train Loss: 2.2841, Validation Loss: 2.5793\n",
      "Iteration 434300, Train Loss: 2.2097, Validation Loss: 1.9008\n",
      "Iteration 434400, Train Loss: 2.2133, Validation Loss: 1.8846\n",
      "Iteration 434500, Train Loss: 2.2719, Validation Loss: 2.1990\n",
      "Iteration 434600, Train Loss: 1.6704, Validation Loss: 2.0840\n",
      "Iteration 434700, Train Loss: 2.5024, Validation Loss: 2.4077\n",
      "Iteration 434800, Train Loss: 2.0903, Validation Loss: 2.8074\n",
      "Iteration 434900, Train Loss: 2.2899, Validation Loss: 2.2389\n",
      "Iteration 435000, Train Loss: 1.9948, Validation Loss: 2.3376\n",
      "Iteration 435100, Train Loss: 2.1653, Validation Loss: 2.2109\n",
      "Iteration 435200, Train Loss: 2.3941, Validation Loss: 2.1339\n",
      "Iteration 435300, Train Loss: 2.5666, Validation Loss: 2.0264\n",
      "Iteration 435400, Train Loss: 2.3642, Validation Loss: 2.8834\n",
      "Iteration 435500, Train Loss: 2.2135, Validation Loss: 1.8961\n",
      "Iteration 435600, Train Loss: 2.2080, Validation Loss: 2.4921\n",
      "Iteration 435700, Train Loss: 2.5762, Validation Loss: 2.1420\n",
      "Iteration 435800, Train Loss: 2.0864, Validation Loss: 1.9411\n",
      "Iteration 435900, Train Loss: 2.4975, Validation Loss: 3.3127\n",
      "Iteration 436000, Train Loss: 2.2806, Validation Loss: 2.4187\n",
      "Iteration 436100, Train Loss: 2.6864, Validation Loss: 2.0595\n",
      "Iteration 436200, Train Loss: 2.0644, Validation Loss: 2.1993\n",
      "Iteration 436300, Train Loss: 2.2175, Validation Loss: 2.2208\n",
      "Iteration 436400, Train Loss: 2.2185, Validation Loss: 2.1524\n",
      "Iteration 436500, Train Loss: 1.8334, Validation Loss: 2.4427\n",
      "Iteration 436600, Train Loss: 2.0389, Validation Loss: 2.7654\n",
      "Iteration 436700, Train Loss: 2.4149, Validation Loss: 2.2414\n",
      "Iteration 436800, Train Loss: 2.1209, Validation Loss: 2.5710\n",
      "Iteration 436900, Train Loss: 2.4496, Validation Loss: 2.7199\n",
      "Iteration 437000, Train Loss: 2.2411, Validation Loss: 3.0726\n",
      "Iteration 437100, Train Loss: 2.6637, Validation Loss: 2.2975\n",
      "Iteration 437200, Train Loss: 2.3814, Validation Loss: 2.9405\n",
      "Iteration 437300, Train Loss: 1.9080, Validation Loss: 2.6081\n",
      "Iteration 437400, Train Loss: 2.1178, Validation Loss: 2.2927\n",
      "Iteration 437500, Train Loss: 2.0212, Validation Loss: 2.4364\n",
      "Iteration 437600, Train Loss: 2.5675, Validation Loss: 2.2291\n",
      "Iteration 437700, Train Loss: 2.3410, Validation Loss: 2.5248\n",
      "Iteration 437800, Train Loss: 2.4961, Validation Loss: 2.4438\n",
      "Iteration 437900, Train Loss: 2.4040, Validation Loss: 2.2713\n",
      "Iteration 438000, Train Loss: 2.2876, Validation Loss: 1.9228\n",
      "Iteration 438100, Train Loss: 2.5345, Validation Loss: 1.9997\n",
      "Iteration 438200, Train Loss: 2.0906, Validation Loss: 2.6776\n",
      "Iteration 438300, Train Loss: 2.2883, Validation Loss: 2.5332\n",
      "Iteration 438400, Train Loss: 2.0556, Validation Loss: 2.3362\n",
      "Iteration 438500, Train Loss: 2.4518, Validation Loss: 2.2837\n",
      "Iteration 438600, Train Loss: 1.8874, Validation Loss: 2.6406\n",
      "Iteration 438700, Train Loss: 2.4256, Validation Loss: 2.5498\n",
      "Iteration 438800, Train Loss: 2.4071, Validation Loss: 1.9916\n",
      "Iteration 438900, Train Loss: 1.9518, Validation Loss: 2.1997\n",
      "Iteration 439000, Train Loss: 2.7449, Validation Loss: 2.0233\n",
      "Iteration 439100, Train Loss: 2.2483, Validation Loss: 2.1144\n",
      "Iteration 439200, Train Loss: 2.0546, Validation Loss: 2.0570\n",
      "Iteration 439300, Train Loss: 2.4016, Validation Loss: 2.2743\n",
      "Iteration 439400, Train Loss: 1.8967, Validation Loss: 2.5367\n",
      "Iteration 439500, Train Loss: 1.9023, Validation Loss: 2.2480\n",
      "Iteration 439600, Train Loss: 2.1465, Validation Loss: 1.7700\n",
      "Iteration 439700, Train Loss: 2.2485, Validation Loss: 2.7198\n",
      "Iteration 439800, Train Loss: 2.5588, Validation Loss: 2.1457\n",
      "Iteration 439900, Train Loss: 2.3548, Validation Loss: 2.4124\n",
      "Iteration 440000, Train Loss: 2.3900, Validation Loss: 2.3401\n",
      "Iteration 440100, Train Loss: 2.2590, Validation Loss: 1.8262\n",
      "Iteration 440200, Train Loss: 2.2115, Validation Loss: 2.4675\n",
      "Iteration 440300, Train Loss: 2.4571, Validation Loss: 1.8608\n",
      "Iteration 440400, Train Loss: 2.2203, Validation Loss: 1.9908\n",
      "Iteration 440500, Train Loss: 2.5055, Validation Loss: 1.9679\n",
      "Iteration 440600, Train Loss: 2.1175, Validation Loss: 2.5734\n",
      "Iteration 440700, Train Loss: 2.3002, Validation Loss: 2.1642\n",
      "Iteration 440800, Train Loss: 2.2288, Validation Loss: 2.0438\n",
      "Iteration 440900, Train Loss: 2.1496, Validation Loss: 2.3862\n",
      "Iteration 441000, Train Loss: 2.8926, Validation Loss: 2.3470\n",
      "Iteration 441100, Train Loss: 2.3691, Validation Loss: 2.3193\n",
      "Iteration 441200, Train Loss: 2.2596, Validation Loss: 2.4175\n",
      "Iteration 441300, Train Loss: 2.1761, Validation Loss: 2.3183\n",
      "Iteration 441400, Train Loss: 2.6570, Validation Loss: 1.8942\n",
      "Iteration 441500, Train Loss: 1.8743, Validation Loss: 2.1159\n",
      "Iteration 441600, Train Loss: 2.2397, Validation Loss: 2.3122\n",
      "Iteration 441700, Train Loss: 2.9464, Validation Loss: 2.3902\n",
      "Iteration 441800, Train Loss: 2.1905, Validation Loss: 2.2668\n",
      "Iteration 441900, Train Loss: 2.1687, Validation Loss: 2.2440\n",
      "Iteration 442000, Train Loss: 2.1697, Validation Loss: 2.3819\n",
      "Iteration 442100, Train Loss: 2.4605, Validation Loss: 1.7729\n",
      "Iteration 442200, Train Loss: 2.4032, Validation Loss: 2.5073\n",
      "Iteration 442300, Train Loss: 2.3050, Validation Loss: 1.9857\n",
      "Iteration 442400, Train Loss: 2.4968, Validation Loss: 3.0314\n",
      "Iteration 442500, Train Loss: 2.3937, Validation Loss: 2.4482\n",
      "Iteration 442600, Train Loss: 2.2093, Validation Loss: 2.1571\n",
      "Iteration 442700, Train Loss: 2.0654, Validation Loss: 2.2503\n",
      "Iteration 442800, Train Loss: 2.1985, Validation Loss: 2.3479\n",
      "Iteration 442900, Train Loss: 1.9432, Validation Loss: 2.2983\n",
      "Iteration 443000, Train Loss: 2.5355, Validation Loss: 2.2400\n",
      "Iteration 443100, Train Loss: 2.3772, Validation Loss: 2.4348\n",
      "Iteration 443200, Train Loss: 2.6889, Validation Loss: 2.5458\n",
      "Iteration 443300, Train Loss: 2.3140, Validation Loss: 2.0688\n",
      "Iteration 443400, Train Loss: 1.9023, Validation Loss: 2.0449\n",
      "Iteration 443500, Train Loss: 2.3446, Validation Loss: 2.2513\n",
      "Iteration 443600, Train Loss: 2.7421, Validation Loss: 2.1892\n",
      "Iteration 443700, Train Loss: 2.3322, Validation Loss: 2.1041\n",
      "Iteration 443800, Train Loss: 2.4105, Validation Loss: 2.1695\n",
      "Iteration 443900, Train Loss: 2.1845, Validation Loss: 2.1752\n",
      "Iteration 444000, Train Loss: 2.0792, Validation Loss: 2.5786\n",
      "Iteration 444100, Train Loss: 2.2479, Validation Loss: 2.7053\n",
      "Iteration 444200, Train Loss: 2.3521, Validation Loss: 2.1426\n",
      "Iteration 444300, Train Loss: 2.0744, Validation Loss: 2.1474\n",
      "Iteration 444400, Train Loss: 2.5641, Validation Loss: 2.4326\n",
      "Iteration 444500, Train Loss: 1.9196, Validation Loss: 2.2920\n",
      "Iteration 444600, Train Loss: 2.6144, Validation Loss: 2.1122\n",
      "Iteration 444700, Train Loss: 2.2241, Validation Loss: 2.2981\n",
      "Iteration 444800, Train Loss: 1.6187, Validation Loss: 1.8696\n",
      "Iteration 444900, Train Loss: 2.0416, Validation Loss: 2.3096\n",
      "Iteration 445000, Train Loss: 2.0852, Validation Loss: 2.4700\n",
      "Iteration 445100, Train Loss: 2.0655, Validation Loss: 2.1245\n",
      "Iteration 445200, Train Loss: 2.2377, Validation Loss: 2.9301\n",
      "Iteration 445300, Train Loss: 2.0395, Validation Loss: 2.5005\n",
      "Iteration 445400, Train Loss: 2.4898, Validation Loss: 2.1543\n",
      "Iteration 445500, Train Loss: 2.0195, Validation Loss: 2.4341\n",
      "Iteration 445600, Train Loss: 2.3075, Validation Loss: 2.4841\n",
      "Iteration 445700, Train Loss: 1.9460, Validation Loss: 2.4674\n",
      "Iteration 445800, Train Loss: 2.0510, Validation Loss: 2.4305\n",
      "Iteration 445900, Train Loss: 2.2230, Validation Loss: 2.8335\n",
      "Iteration 446000, Train Loss: 2.1709, Validation Loss: 2.5053\n",
      "Iteration 446100, Train Loss: 2.0470, Validation Loss: 2.2320\n",
      "Iteration 446200, Train Loss: 1.9766, Validation Loss: 2.2027\n",
      "Iteration 446300, Train Loss: 1.6726, Validation Loss: 2.3480\n",
      "Iteration 446400, Train Loss: 2.0825, Validation Loss: 2.1800\n",
      "Iteration 446500, Train Loss: 2.5379, Validation Loss: 2.1065\n",
      "Iteration 446600, Train Loss: 2.5187, Validation Loss: 2.4418\n",
      "Iteration 446700, Train Loss: 2.3656, Validation Loss: 1.9974\n",
      "Iteration 446800, Train Loss: 2.9303, Validation Loss: 2.6359\n",
      "Iteration 446900, Train Loss: 2.2222, Validation Loss: 2.7731\n",
      "Iteration 447000, Train Loss: 2.7355, Validation Loss: 2.2866\n",
      "Iteration 447100, Train Loss: 2.5504, Validation Loss: 2.2092\n",
      "Iteration 447200, Train Loss: 2.2699, Validation Loss: 2.0228\n",
      "Iteration 447300, Train Loss: 2.3072, Validation Loss: 2.5571\n",
      "Iteration 447400, Train Loss: 2.6487, Validation Loss: 2.3099\n",
      "Iteration 447500, Train Loss: 2.5216, Validation Loss: 2.2936\n",
      "Iteration 447600, Train Loss: 2.3610, Validation Loss: 2.1490\n",
      "Iteration 447700, Train Loss: 2.1488, Validation Loss: 2.3041\n",
      "Iteration 447800, Train Loss: 2.2995, Validation Loss: 2.2638\n",
      "Iteration 447900, Train Loss: 2.0345, Validation Loss: 1.9894\n",
      "Iteration 448000, Train Loss: 2.4156, Validation Loss: 2.4376\n",
      "Iteration 448100, Train Loss: 2.2170, Validation Loss: 2.8971\n",
      "Iteration 448200, Train Loss: 1.7901, Validation Loss: 1.8739\n",
      "Iteration 448300, Train Loss: 2.5295, Validation Loss: 2.4090\n",
      "Iteration 448400, Train Loss: 2.3916, Validation Loss: 2.1534\n",
      "Iteration 448500, Train Loss: 2.5842, Validation Loss: 2.1473\n",
      "Iteration 448600, Train Loss: 2.9037, Validation Loss: 2.0903\n",
      "Iteration 448700, Train Loss: 2.2268, Validation Loss: 2.2327\n",
      "Iteration 448800, Train Loss: 2.1356, Validation Loss: 2.3593\n",
      "Iteration 448900, Train Loss: 2.0838, Validation Loss: 2.2237\n",
      "Iteration 449000, Train Loss: 2.1514, Validation Loss: 2.1227\n",
      "Iteration 449100, Train Loss: 1.7466, Validation Loss: 2.1436\n",
      "Iteration 449200, Train Loss: 2.2553, Validation Loss: 2.5221\n",
      "Iteration 449300, Train Loss: 1.7210, Validation Loss: 2.2724\n",
      "Iteration 449400, Train Loss: 2.0125, Validation Loss: 2.6479\n",
      "Iteration 449500, Train Loss: 2.2044, Validation Loss: 2.1576\n",
      "Iteration 449600, Train Loss: 2.4014, Validation Loss: 2.4366\n",
      "Iteration 449700, Train Loss: 1.8646, Validation Loss: 1.9423\n",
      "Iteration 449800, Train Loss: 2.2148, Validation Loss: 2.4769\n",
      "Iteration 449900, Train Loss: 2.0371, Validation Loss: 2.3915\n",
      "Iteration 450000, Train Loss: 1.7681, Validation Loss: 2.4695\n",
      "Iteration 450100, Train Loss: 2.5440, Validation Loss: 1.9547\n",
      "Iteration 450200, Train Loss: 2.6871, Validation Loss: 1.9676\n",
      "Iteration 450300, Train Loss: 2.6180, Validation Loss: 1.9398\n",
      "Iteration 450400, Train Loss: 2.4570, Validation Loss: 2.3310\n",
      "Iteration 450500, Train Loss: 2.3226, Validation Loss: 1.7316\n",
      "Iteration 450600, Train Loss: 2.5219, Validation Loss: 2.6545\n",
      "Iteration 450700, Train Loss: 1.9928, Validation Loss: 2.5464\n",
      "Iteration 450800, Train Loss: 2.3581, Validation Loss: 1.9739\n",
      "Iteration 450900, Train Loss: 2.2256, Validation Loss: 2.5390\n",
      "Iteration 451000, Train Loss: 2.4116, Validation Loss: 1.7658\n",
      "Iteration 451100, Train Loss: 2.3486, Validation Loss: 2.3500\n",
      "Iteration 451200, Train Loss: 2.4526, Validation Loss: 2.1611\n",
      "Iteration 451300, Train Loss: 2.3105, Validation Loss: 2.4743\n",
      "Iteration 451400, Train Loss: 2.3420, Validation Loss: 2.3040\n",
      "Iteration 451500, Train Loss: 2.2582, Validation Loss: 2.5206\n",
      "Iteration 451600, Train Loss: 2.4078, Validation Loss: 2.2540\n",
      "Iteration 451700, Train Loss: 2.7375, Validation Loss: 2.1454\n",
      "Iteration 451800, Train Loss: 2.0735, Validation Loss: 2.6653\n",
      "Iteration 451900, Train Loss: 2.1013, Validation Loss: 2.7193\n",
      "Iteration 452000, Train Loss: 2.2795, Validation Loss: 2.1619\n",
      "Iteration 452100, Train Loss: 2.0354, Validation Loss: 2.2758\n",
      "Iteration 452200, Train Loss: 2.4019, Validation Loss: 2.6219\n",
      "Iteration 452300, Train Loss: 2.0271, Validation Loss: 2.0049\n",
      "Iteration 452400, Train Loss: 2.2059, Validation Loss: 2.6198\n",
      "Iteration 452500, Train Loss: 1.9368, Validation Loss: 2.4333\n",
      "Iteration 452600, Train Loss: 2.4294, Validation Loss: 2.2598\n",
      "Iteration 452700, Train Loss: 2.3696, Validation Loss: 2.4869\n",
      "Iteration 452800, Train Loss: 1.8033, Validation Loss: 2.2911\n",
      "Iteration 452900, Train Loss: 2.6701, Validation Loss: 1.9150\n",
      "Iteration 453000, Train Loss: 1.9656, Validation Loss: 2.1055\n",
      "Iteration 453100, Train Loss: 2.2143, Validation Loss: 1.7010\n",
      "Iteration 453200, Train Loss: 2.5788, Validation Loss: 2.1706\n",
      "Iteration 453300, Train Loss: 2.1380, Validation Loss: 2.3143\n",
      "Iteration 453400, Train Loss: 1.7863, Validation Loss: 2.5387\n",
      "Iteration 453500, Train Loss: 2.0268, Validation Loss: 2.5933\n",
      "Iteration 453600, Train Loss: 2.4742, Validation Loss: 2.4237\n",
      "Iteration 453700, Train Loss: 2.2664, Validation Loss: 2.0525\n",
      "Iteration 453800, Train Loss: 2.2496, Validation Loss: 1.8570\n",
      "Iteration 453900, Train Loss: 2.1301, Validation Loss: 2.3358\n",
      "Iteration 454000, Train Loss: 2.5162, Validation Loss: 2.3863\n",
      "Iteration 454100, Train Loss: 2.0316, Validation Loss: 2.3740\n",
      "Iteration 454200, Train Loss: 1.8819, Validation Loss: 1.7041\n",
      "Iteration 454300, Train Loss: 2.1242, Validation Loss: 2.3756\n",
      "Iteration 454400, Train Loss: 2.2536, Validation Loss: 2.3971\n",
      "Iteration 454500, Train Loss: 2.1438, Validation Loss: 2.1473\n",
      "Iteration 454600, Train Loss: 2.6681, Validation Loss: 2.2079\n",
      "Iteration 454700, Train Loss: 2.2346, Validation Loss: 1.8977\n",
      "Iteration 454800, Train Loss: 2.0491, Validation Loss: 1.8723\n",
      "Iteration 454900, Train Loss: 2.5304, Validation Loss: 2.5738\n",
      "Iteration 455000, Train Loss: 2.1295, Validation Loss: 2.3205\n",
      "Iteration 455100, Train Loss: 2.5594, Validation Loss: 2.3945\n",
      "Iteration 455200, Train Loss: 2.5955, Validation Loss: 2.1711\n",
      "Iteration 455300, Train Loss: 1.8634, Validation Loss: 2.8119\n",
      "Iteration 455400, Train Loss: 1.8687, Validation Loss: 2.1049\n",
      "Iteration 455500, Train Loss: 2.2387, Validation Loss: 2.1566\n",
      "Iteration 455600, Train Loss: 2.2037, Validation Loss: 2.3430\n",
      "Iteration 455700, Train Loss: 2.9675, Validation Loss: 2.0879\n",
      "Iteration 455800, Train Loss: 2.2829, Validation Loss: 2.2795\n",
      "Iteration 455900, Train Loss: 2.1958, Validation Loss: 2.1861\n",
      "Iteration 456000, Train Loss: 2.1858, Validation Loss: 2.4871\n",
      "Iteration 456100, Train Loss: 2.3076, Validation Loss: 2.2188\n",
      "Iteration 456200, Train Loss: 2.5106, Validation Loss: 2.4388\n",
      "Iteration 456300, Train Loss: 2.2611, Validation Loss: 2.3202\n",
      "Iteration 456400, Train Loss: 2.3118, Validation Loss: 2.1455\n",
      "Iteration 456500, Train Loss: 2.3687, Validation Loss: 2.1787\n",
      "Iteration 456600, Train Loss: 2.4173, Validation Loss: 2.3925\n",
      "Iteration 456700, Train Loss: 2.0658, Validation Loss: 1.9623\n",
      "Iteration 456800, Train Loss: 2.0186, Validation Loss: 2.4435\n",
      "Iteration 456900, Train Loss: 2.6284, Validation Loss: 1.8917\n",
      "Iteration 457000, Train Loss: 1.8058, Validation Loss: 2.1178\n",
      "Iteration 457100, Train Loss: 2.3921, Validation Loss: 2.0823\n",
      "Iteration 457200, Train Loss: 2.2336, Validation Loss: 2.3619\n",
      "Iteration 457300, Train Loss: 2.9616, Validation Loss: 2.0618\n",
      "Iteration 457400, Train Loss: 2.3204, Validation Loss: 2.2361\n",
      "Iteration 457500, Train Loss: 1.6917, Validation Loss: 2.5063\n",
      "Iteration 457600, Train Loss: 2.6848, Validation Loss: 2.6800\n",
      "Iteration 457700, Train Loss: 1.8554, Validation Loss: 1.6709\n",
      "Iteration 457800, Train Loss: 2.3509, Validation Loss: 2.2434\n",
      "Iteration 457900, Train Loss: 2.3462, Validation Loss: 1.9103\n",
      "Iteration 458000, Train Loss: 2.8201, Validation Loss: 2.4327\n",
      "Iteration 458100, Train Loss: 2.3551, Validation Loss: 2.6267\n",
      "Iteration 458200, Train Loss: 1.9114, Validation Loss: 2.3667\n",
      "Iteration 458300, Train Loss: 2.3228, Validation Loss: 2.2262\n",
      "Iteration 458400, Train Loss: 2.4194, Validation Loss: 1.9812\n",
      "Iteration 458500, Train Loss: 1.9049, Validation Loss: 2.0544\n",
      "Iteration 458600, Train Loss: 2.2971, Validation Loss: 2.5304\n",
      "Iteration 458700, Train Loss: 2.1903, Validation Loss: 1.8243\n",
      "Iteration 458800, Train Loss: 2.5269, Validation Loss: 2.3107\n",
      "Iteration 458900, Train Loss: 2.0267, Validation Loss: 2.2388\n",
      "Iteration 459000, Train Loss: 2.2186, Validation Loss: 2.3280\n",
      "Iteration 459100, Train Loss: 1.9319, Validation Loss: 2.2993\n",
      "Iteration 459200, Train Loss: 2.7543, Validation Loss: 2.4078\n",
      "Iteration 459300, Train Loss: 1.8593, Validation Loss: 2.1923\n",
      "Iteration 459400, Train Loss: 2.3874, Validation Loss: 2.0129\n",
      "Iteration 459500, Train Loss: 2.3153, Validation Loss: 1.9059\n",
      "Iteration 459600, Train Loss: 1.9234, Validation Loss: 1.9754\n",
      "Iteration 459700, Train Loss: 2.1443, Validation Loss: 2.4695\n",
      "Iteration 459800, Train Loss: 1.8119, Validation Loss: 2.5445\n",
      "Iteration 459900, Train Loss: 2.2172, Validation Loss: 2.1943\n",
      "Iteration 460000, Train Loss: 2.2651, Validation Loss: 2.5998\n",
      "Iteration 460100, Train Loss: 2.1136, Validation Loss: 2.2942\n",
      "Iteration 460200, Train Loss: 2.4191, Validation Loss: 2.3784\n",
      "Iteration 460300, Train Loss: 2.4288, Validation Loss: 2.1943\n",
      "Iteration 460400, Train Loss: 2.1673, Validation Loss: 2.2569\n",
      "Iteration 460500, Train Loss: 2.1198, Validation Loss: 2.4008\n",
      "Iteration 460600, Train Loss: 2.0084, Validation Loss: 2.5494\n",
      "Iteration 460700, Train Loss: 2.2436, Validation Loss: 1.8708\n",
      "Iteration 460800, Train Loss: 2.6102, Validation Loss: 2.5993\n",
      "Iteration 460900, Train Loss: 2.3632, Validation Loss: 2.4383\n",
      "Iteration 461000, Train Loss: 1.9291, Validation Loss: 2.4227\n",
      "Iteration 461100, Train Loss: 2.1855, Validation Loss: 2.1436\n",
      "Iteration 461200, Train Loss: 2.4151, Validation Loss: 2.1295\n",
      "Iteration 461300, Train Loss: 2.4762, Validation Loss: 2.1876\n",
      "Iteration 461400, Train Loss: 2.3182, Validation Loss: 2.1060\n",
      "Iteration 461500, Train Loss: 1.9346, Validation Loss: 2.4639\n",
      "Iteration 461600, Train Loss: 2.2265, Validation Loss: 2.1943\n",
      "Iteration 461700, Train Loss: 2.3075, Validation Loss: 2.1487\n",
      "Iteration 461800, Train Loss: 2.3490, Validation Loss: 2.6148\n",
      "Iteration 461900, Train Loss: 2.3884, Validation Loss: 2.2832\n",
      "Iteration 462000, Train Loss: 2.6638, Validation Loss: 1.7761\n",
      "Iteration 462100, Train Loss: 2.5913, Validation Loss: 2.4098\n",
      "Iteration 462200, Train Loss: 2.4044, Validation Loss: 2.5874\n",
      "Iteration 462300, Train Loss: 2.6588, Validation Loss: 2.1762\n",
      "Iteration 462400, Train Loss: 2.2960, Validation Loss: 2.2999\n",
      "Iteration 462500, Train Loss: 2.5378, Validation Loss: 2.7546\n",
      "Iteration 462600, Train Loss: 2.0491, Validation Loss: 2.2106\n",
      "Iteration 462700, Train Loss: 2.2596, Validation Loss: 2.1061\n",
      "Iteration 462800, Train Loss: 2.1474, Validation Loss: 2.4981\n",
      "Iteration 462900, Train Loss: 2.2507, Validation Loss: 2.5283\n",
      "Iteration 463000, Train Loss: 2.7594, Validation Loss: 2.3947\n",
      "Iteration 463100, Train Loss: 1.7904, Validation Loss: 1.9901\n",
      "Iteration 463200, Train Loss: 2.6063, Validation Loss: 1.9503\n",
      "Iteration 463300, Train Loss: 2.0204, Validation Loss: 2.3958\n",
      "Iteration 463400, Train Loss: 2.3175, Validation Loss: 2.4288\n",
      "Iteration 463500, Train Loss: 2.0531, Validation Loss: 2.4211\n",
      "Iteration 463600, Train Loss: 2.2306, Validation Loss: 2.1120\n",
      "Iteration 463700, Train Loss: 2.1422, Validation Loss: 2.5489\n",
      "Iteration 463800, Train Loss: 2.1510, Validation Loss: 2.6205\n",
      "Iteration 463900, Train Loss: 2.0338, Validation Loss: 2.3830\n",
      "Iteration 464000, Train Loss: 2.4539, Validation Loss: 1.7011\n",
      "Iteration 464100, Train Loss: 2.5145, Validation Loss: 2.3342\n",
      "Iteration 464200, Train Loss: 2.8014, Validation Loss: 1.9923\n",
      "Iteration 464300, Train Loss: 2.3981, Validation Loss: 2.3722\n",
      "Iteration 464400, Train Loss: 2.1670, Validation Loss: 1.9903\n",
      "Iteration 464500, Train Loss: 2.0767, Validation Loss: 2.3178\n",
      "Iteration 464600, Train Loss: 2.3447, Validation Loss: 2.0935\n",
      "Iteration 464700, Train Loss: 2.0808, Validation Loss: 1.9147\n",
      "Iteration 464800, Train Loss: 2.4169, Validation Loss: 2.1593\n",
      "Iteration 464900, Train Loss: 2.6437, Validation Loss: 1.9954\n",
      "Iteration 465000, Train Loss: 1.7931, Validation Loss: 2.3353\n",
      "Iteration 465100, Train Loss: 2.1158, Validation Loss: 2.2037\n",
      "Iteration 465200, Train Loss: 2.1383, Validation Loss: 2.4829\n",
      "Iteration 465300, Train Loss: 2.2311, Validation Loss: 2.3755\n",
      "Iteration 465400, Train Loss: 2.4327, Validation Loss: 2.0944\n",
      "Iteration 465500, Train Loss: 2.1291, Validation Loss: 2.2365\n",
      "Iteration 465600, Train Loss: 2.0675, Validation Loss: 2.2970\n",
      "Iteration 465700, Train Loss: 2.6405, Validation Loss: 2.3269\n",
      "Iteration 465800, Train Loss: 2.3984, Validation Loss: 2.0763\n",
      "Iteration 465900, Train Loss: 2.5125, Validation Loss: 2.2514\n",
      "Iteration 466000, Train Loss: 2.0873, Validation Loss: 2.1567\n",
      "Iteration 466100, Train Loss: 2.4863, Validation Loss: 2.1694\n",
      "Iteration 466200, Train Loss: 2.3299, Validation Loss: 1.9165\n",
      "Iteration 466300, Train Loss: 2.3545, Validation Loss: 2.3647\n",
      "Iteration 466400, Train Loss: 2.0445, Validation Loss: 2.2671\n",
      "Iteration 466500, Train Loss: 2.3645, Validation Loss: 1.8003\n",
      "Iteration 466600, Train Loss: 1.9862, Validation Loss: 2.7707\n",
      "Iteration 466700, Train Loss: 2.6793, Validation Loss: 2.1712\n",
      "Iteration 466800, Train Loss: 1.9574, Validation Loss: 2.4275\n",
      "Iteration 466900, Train Loss: 2.1857, Validation Loss: 2.3536\n",
      "Iteration 467000, Train Loss: 2.1998, Validation Loss: 2.7399\n",
      "Iteration 467100, Train Loss: 2.4936, Validation Loss: 2.4489\n",
      "Iteration 467200, Train Loss: 2.4938, Validation Loss: 2.3063\n",
      "Iteration 467300, Train Loss: 2.1525, Validation Loss: 2.7017\n",
      "Iteration 467400, Train Loss: 2.1201, Validation Loss: 2.3624\n",
      "Iteration 467500, Train Loss: 2.6887, Validation Loss: 2.0680\n",
      "Iteration 467600, Train Loss: 2.4624, Validation Loss: 1.8292\n",
      "Iteration 467700, Train Loss: 2.2864, Validation Loss: 2.3223\n",
      "Iteration 467800, Train Loss: 2.1520, Validation Loss: 2.3940\n",
      "Iteration 467900, Train Loss: 2.3877, Validation Loss: 2.5874\n",
      "Iteration 468000, Train Loss: 2.1212, Validation Loss: 2.1204\n",
      "Iteration 468100, Train Loss: 2.2972, Validation Loss: 2.2433\n",
      "Iteration 468200, Train Loss: 1.8980, Validation Loss: 2.6006\n",
      "Iteration 468300, Train Loss: 1.9120, Validation Loss: 2.4198\n",
      "Iteration 468400, Train Loss: 2.5354, Validation Loss: 2.4342\n",
      "Iteration 468500, Train Loss: 2.0732, Validation Loss: 2.3910\n",
      "Iteration 468600, Train Loss: 2.6106, Validation Loss: 2.0489\n",
      "Iteration 468700, Train Loss: 1.7868, Validation Loss: 2.3205\n",
      "Iteration 468800, Train Loss: 2.2038, Validation Loss: 2.2956\n",
      "Iteration 468900, Train Loss: 2.4689, Validation Loss: 2.2741\n",
      "Iteration 469000, Train Loss: 2.2620, Validation Loss: 1.9224\n",
      "Iteration 469100, Train Loss: 2.5014, Validation Loss: 2.2168\n",
      "Iteration 469200, Train Loss: 2.4554, Validation Loss: 2.3251\n",
      "Iteration 469300, Train Loss: 2.0388, Validation Loss: 2.1489\n",
      "Iteration 469400, Train Loss: 1.9413, Validation Loss: 2.2313\n",
      "Iteration 469500, Train Loss: 2.1097, Validation Loss: 1.9610\n",
      "Iteration 469600, Train Loss: 2.4435, Validation Loss: 2.1350\n",
      "Iteration 469700, Train Loss: 2.1600, Validation Loss: 2.2509\n",
      "Iteration 469800, Train Loss: 2.1744, Validation Loss: 2.3763\n",
      "Iteration 469900, Train Loss: 2.1808, Validation Loss: 2.4035\n",
      "Iteration 470000, Train Loss: 2.5553, Validation Loss: 1.9907\n",
      "Iteration 470100, Train Loss: 1.8146, Validation Loss: 2.0777\n",
      "Iteration 470200, Train Loss: 2.2930, Validation Loss: 2.1858\n",
      "Iteration 470300, Train Loss: 1.9477, Validation Loss: 2.2652\n",
      "Iteration 470400, Train Loss: 2.2657, Validation Loss: 2.7182\n",
      "Iteration 470500, Train Loss: 2.3821, Validation Loss: 2.1731\n",
      "Iteration 470600, Train Loss: 1.7646, Validation Loss: 2.0346\n",
      "Iteration 470700, Train Loss: 2.4071, Validation Loss: 2.0460\n",
      "Iteration 470800, Train Loss: 2.3172, Validation Loss: 2.3051\n",
      "Iteration 470900, Train Loss: 2.2304, Validation Loss: 1.7404\n",
      "Iteration 471000, Train Loss: 2.3429, Validation Loss: 2.0837\n",
      "Iteration 471100, Train Loss: 2.2977, Validation Loss: 2.3302\n",
      "Iteration 471200, Train Loss: 2.2494, Validation Loss: 1.7845\n",
      "Iteration 471300, Train Loss: 2.2527, Validation Loss: 2.5056\n",
      "Iteration 471400, Train Loss: 2.7328, Validation Loss: 2.4904\n",
      "Iteration 471500, Train Loss: 2.4366, Validation Loss: 2.1433\n",
      "Iteration 471600, Train Loss: 1.9096, Validation Loss: 2.7990\n",
      "Iteration 471700, Train Loss: 2.4818, Validation Loss: 2.7139\n",
      "Iteration 471800, Train Loss: 2.1781, Validation Loss: 2.2133\n",
      "Iteration 471900, Train Loss: 2.0704, Validation Loss: 2.0470\n",
      "Iteration 472000, Train Loss: 2.4910, Validation Loss: 2.2034\n",
      "Iteration 472100, Train Loss: 2.3278, Validation Loss: 2.6401\n",
      "Iteration 472200, Train Loss: 2.3184, Validation Loss: 2.4085\n",
      "Iteration 472300, Train Loss: 1.9219, Validation Loss: 2.2008\n",
      "Iteration 472400, Train Loss: 1.9002, Validation Loss: 2.1753\n",
      "Iteration 472500, Train Loss: 2.3007, Validation Loss: 2.6722\n",
      "Iteration 472600, Train Loss: 2.2877, Validation Loss: 2.0775\n",
      "Iteration 472700, Train Loss: 2.2722, Validation Loss: 2.4780\n",
      "Iteration 472800, Train Loss: 2.3755, Validation Loss: 2.3692\n",
      "Iteration 472900, Train Loss: 2.4860, Validation Loss: 1.9994\n",
      "Iteration 473000, Train Loss: 2.5433, Validation Loss: 2.3205\n",
      "Iteration 473100, Train Loss: 2.6833, Validation Loss: 2.4648\n",
      "Iteration 473200, Train Loss: 2.2650, Validation Loss: 2.0237\n",
      "Iteration 473300, Train Loss: 2.3263, Validation Loss: 2.4985\n",
      "Iteration 473400, Train Loss: 2.2019, Validation Loss: 2.5629\n",
      "Iteration 473500, Train Loss: 2.3442, Validation Loss: 2.3824\n",
      "Iteration 473600, Train Loss: 2.2716, Validation Loss: 2.3603\n",
      "Iteration 473700, Train Loss: 2.3116, Validation Loss: 2.3908\n",
      "Iteration 473800, Train Loss: 3.1135, Validation Loss: 2.1236\n",
      "Iteration 473900, Train Loss: 2.4683, Validation Loss: 2.1164\n",
      "Iteration 474000, Train Loss: 2.4007, Validation Loss: 2.3044\n",
      "Iteration 474100, Train Loss: 1.9361, Validation Loss: 2.1522\n",
      "Iteration 474200, Train Loss: 2.2540, Validation Loss: 2.7298\n",
      "Iteration 474300, Train Loss: 2.2779, Validation Loss: 2.1312\n",
      "Iteration 474400, Train Loss: 2.6202, Validation Loss: 2.4220\n",
      "Iteration 474500, Train Loss: 1.8883, Validation Loss: 2.5585\n",
      "Iteration 474600, Train Loss: 2.4222, Validation Loss: 2.6614\n",
      "Iteration 474700, Train Loss: 2.2713, Validation Loss: 2.2539\n",
      "Iteration 474800, Train Loss: 2.1110, Validation Loss: 2.2024\n",
      "Iteration 474900, Train Loss: 2.3610, Validation Loss: 2.5584\n",
      "Iteration 475000, Train Loss: 2.2582, Validation Loss: 2.2622\n",
      "Iteration 475100, Train Loss: 2.1376, Validation Loss: 2.3965\n",
      "Iteration 475200, Train Loss: 2.4785, Validation Loss: 1.9422\n",
      "Iteration 475300, Train Loss: 2.5806, Validation Loss: 2.1693\n",
      "Iteration 475400, Train Loss: 2.7024, Validation Loss: 2.7328\n",
      "Iteration 475500, Train Loss: 2.1864, Validation Loss: 2.3696\n",
      "Iteration 475600, Train Loss: 2.4117, Validation Loss: 2.0673\n",
      "Iteration 475700, Train Loss: 2.1011, Validation Loss: 2.2555\n",
      "Iteration 475800, Train Loss: 2.3446, Validation Loss: 2.6126\n",
      "Iteration 475900, Train Loss: 2.3578, Validation Loss: 2.3111\n",
      "Iteration 476000, Train Loss: 2.4932, Validation Loss: 2.2740\n",
      "Iteration 476100, Train Loss: 2.0129, Validation Loss: 2.2252\n",
      "Iteration 476200, Train Loss: 1.9195, Validation Loss: 2.3773\n",
      "Iteration 476300, Train Loss: 2.4772, Validation Loss: 2.3692\n",
      "Iteration 476400, Train Loss: 2.3503, Validation Loss: 2.8356\n",
      "Iteration 476500, Train Loss: 2.3741, Validation Loss: 2.4294\n",
      "Iteration 476600, Train Loss: 2.3103, Validation Loss: 2.3423\n",
      "Iteration 476700, Train Loss: 2.0870, Validation Loss: 2.5230\n",
      "Iteration 476800, Train Loss: 2.1945, Validation Loss: 2.5998\n",
      "Iteration 476900, Train Loss: 2.3713, Validation Loss: 1.8539\n",
      "Iteration 477000, Train Loss: 2.0570, Validation Loss: 2.0698\n",
      "Iteration 477100, Train Loss: 2.0380, Validation Loss: 2.3214\n",
      "Iteration 477200, Train Loss: 2.1963, Validation Loss: 2.5204\n",
      "Iteration 477300, Train Loss: 2.3401, Validation Loss: 2.3079\n",
      "Iteration 477400, Train Loss: 2.1806, Validation Loss: 2.5064\n",
      "Iteration 477500, Train Loss: 2.3524, Validation Loss: 2.2901\n",
      "Iteration 477600, Train Loss: 2.5645, Validation Loss: 2.1213\n",
      "Iteration 477700, Train Loss: 2.1435, Validation Loss: 1.9922\n",
      "Iteration 477800, Train Loss: 2.1138, Validation Loss: 2.1386\n",
      "Iteration 477900, Train Loss: 2.2504, Validation Loss: 2.3113\n",
      "Iteration 478000, Train Loss: 2.0815, Validation Loss: 2.8298\n",
      "Iteration 478100, Train Loss: 2.2119, Validation Loss: 1.9432\n",
      "Iteration 478200, Train Loss: 2.4417, Validation Loss: 2.4735\n",
      "Iteration 478300, Train Loss: 2.5869, Validation Loss: 2.1524\n",
      "Iteration 478400, Train Loss: 2.1897, Validation Loss: 2.3904\n",
      "Iteration 478500, Train Loss: 2.2869, Validation Loss: 2.1259\n",
      "Iteration 478600, Train Loss: 2.3294, Validation Loss: 2.3125\n",
      "Iteration 478700, Train Loss: 1.8906, Validation Loss: 2.5746\n",
      "Iteration 478800, Train Loss: 2.3393, Validation Loss: 2.5468\n",
      "Iteration 478900, Train Loss: 2.4798, Validation Loss: 2.2880\n",
      "Iteration 479000, Train Loss: 2.7410, Validation Loss: 2.3064\n",
      "Iteration 479100, Train Loss: 2.2929, Validation Loss: 2.1998\n",
      "Iteration 479200, Train Loss: 2.6202, Validation Loss: 2.1842\n",
      "Iteration 479300, Train Loss: 2.0036, Validation Loss: 2.2813\n",
      "Iteration 479400, Train Loss: 2.4309, Validation Loss: 2.3820\n",
      "Iteration 479500, Train Loss: 2.1149, Validation Loss: 2.3527\n",
      "Iteration 479600, Train Loss: 2.0755, Validation Loss: 2.2216\n",
      "Iteration 479700, Train Loss: 1.9698, Validation Loss: 2.3451\n",
      "Iteration 479800, Train Loss: 2.2127, Validation Loss: 2.2996\n",
      "Iteration 479900, Train Loss: 2.5692, Validation Loss: 2.2949\n",
      "Iteration 480000, Train Loss: 2.4329, Validation Loss: 2.2958\n",
      "Iteration 480100, Train Loss: 2.2228, Validation Loss: 2.3651\n",
      "Iteration 480200, Train Loss: 2.3877, Validation Loss: 2.1094\n",
      "Iteration 480300, Train Loss: 2.1439, Validation Loss: 2.1853\n",
      "Iteration 480400, Train Loss: 2.0364, Validation Loss: 2.1290\n",
      "Iteration 480500, Train Loss: 2.3083, Validation Loss: 2.4387\n",
      "Iteration 480600, Train Loss: 2.4713, Validation Loss: 2.2971\n",
      "Iteration 480700, Train Loss: 2.3215, Validation Loss: 2.3059\n",
      "Iteration 480800, Train Loss: 2.1326, Validation Loss: 2.6852\n",
      "Iteration 480900, Train Loss: 2.5293, Validation Loss: 2.1301\n",
      "Iteration 481000, Train Loss: 1.9875, Validation Loss: 2.2156\n",
      "Iteration 481100, Train Loss: 2.6324, Validation Loss: 2.4587\n",
      "Iteration 481200, Train Loss: 2.4070, Validation Loss: 2.2533\n",
      "Iteration 481300, Train Loss: 2.4205, Validation Loss: 1.8958\n",
      "Iteration 481400, Train Loss: 2.2664, Validation Loss: 1.8874\n",
      "Iteration 481500, Train Loss: 1.8030, Validation Loss: 1.8150\n",
      "Iteration 481600, Train Loss: 2.1407, Validation Loss: 2.3384\n",
      "Iteration 481700, Train Loss: 2.5713, Validation Loss: 2.1734\n",
      "Iteration 481800, Train Loss: 2.5433, Validation Loss: 2.4562\n",
      "Iteration 481900, Train Loss: 2.6991, Validation Loss: 2.0125\n",
      "Iteration 482000, Train Loss: 2.5864, Validation Loss: 1.9474\n",
      "Iteration 482100, Train Loss: 2.4735, Validation Loss: 2.2281\n",
      "Iteration 482200, Train Loss: 2.4765, Validation Loss: 2.1892\n",
      "Iteration 482300, Train Loss: 2.3324, Validation Loss: 2.5898\n",
      "Iteration 482400, Train Loss: 2.1633, Validation Loss: 2.4258\n",
      "Iteration 482500, Train Loss: 2.3155, Validation Loss: 2.1348\n",
      "Iteration 482600, Train Loss: 2.2559, Validation Loss: 1.8491\n",
      "Iteration 482700, Train Loss: 2.0900, Validation Loss: 2.5109\n",
      "Iteration 482800, Train Loss: 2.5171, Validation Loss: 2.4913\n",
      "Iteration 482900, Train Loss: 2.1419, Validation Loss: 1.9080\n",
      "Iteration 483000, Train Loss: 2.3466, Validation Loss: 2.1219\n",
      "Iteration 483100, Train Loss: 2.4052, Validation Loss: 2.4953\n",
      "Iteration 483200, Train Loss: 2.2505, Validation Loss: 2.4528\n",
      "Iteration 483300, Train Loss: 2.2586, Validation Loss: 1.9308\n",
      "Iteration 483400, Train Loss: 2.7279, Validation Loss: 2.0936\n",
      "Iteration 483500, Train Loss: 2.3536, Validation Loss: 2.2017\n",
      "Iteration 483600, Train Loss: 2.6309, Validation Loss: 2.6886\n",
      "Iteration 483700, Train Loss: 2.1232, Validation Loss: 2.5369\n",
      "Iteration 483800, Train Loss: 2.1052, Validation Loss: 2.5065\n",
      "Iteration 483900, Train Loss: 2.6975, Validation Loss: 1.9129\n",
      "Iteration 484000, Train Loss: 2.5501, Validation Loss: 2.2949\n",
      "Iteration 484100, Train Loss: 2.4985, Validation Loss: 2.4716\n",
      "Iteration 484200, Train Loss: 2.4233, Validation Loss: 2.3672\n",
      "Iteration 484300, Train Loss: 2.4364, Validation Loss: 2.0774\n",
      "Iteration 484400, Train Loss: 2.1882, Validation Loss: 1.9476\n",
      "Iteration 484500, Train Loss: 2.1960, Validation Loss: 2.0811\n",
      "Iteration 484600, Train Loss: 2.0852, Validation Loss: 1.8293\n",
      "Iteration 484700, Train Loss: 2.5818, Validation Loss: 2.8346\n",
      "Iteration 484800, Train Loss: 2.4451, Validation Loss: 2.5949\n",
      "Iteration 484900, Train Loss: 2.5523, Validation Loss: 1.9844\n",
      "Iteration 485000, Train Loss: 2.7554, Validation Loss: 2.3660\n",
      "Iteration 485100, Train Loss: 2.5081, Validation Loss: 2.3957\n",
      "Iteration 485200, Train Loss: 2.5722, Validation Loss: 2.4244\n",
      "Iteration 485300, Train Loss: 2.0729, Validation Loss: 2.2662\n",
      "Iteration 485400, Train Loss: 2.5848, Validation Loss: 2.0647\n",
      "Iteration 485500, Train Loss: 1.8553, Validation Loss: 2.1938\n",
      "Iteration 485600, Train Loss: 1.9658, Validation Loss: 2.3058\n",
      "Iteration 485700, Train Loss: 2.4004, Validation Loss: 2.6855\n",
      "Iteration 485800, Train Loss: 2.0625, Validation Loss: 2.3966\n",
      "Iteration 485900, Train Loss: 2.0979, Validation Loss: 2.6689\n",
      "Iteration 486000, Train Loss: 1.9966, Validation Loss: 2.6475\n",
      "Iteration 486100, Train Loss: 2.1803, Validation Loss: 2.3196\n",
      "Iteration 486200, Train Loss: 2.6878, Validation Loss: 2.9524\n",
      "Iteration 486300, Train Loss: 2.0532, Validation Loss: 2.3781\n",
      "Iteration 486400, Train Loss: 2.2501, Validation Loss: 2.5893\n",
      "Iteration 486500, Train Loss: 2.2149, Validation Loss: 2.7800\n",
      "Iteration 486600, Train Loss: 2.0426, Validation Loss: 2.3749\n",
      "Iteration 486700, Train Loss: 2.1177, Validation Loss: 2.0388\n",
      "Iteration 486800, Train Loss: 2.3606, Validation Loss: 2.1759\n",
      "Iteration 486900, Train Loss: 2.0872, Validation Loss: 2.2205\n",
      "Iteration 487000, Train Loss: 2.1548, Validation Loss: 2.0982\n",
      "Iteration 487100, Train Loss: 2.0571, Validation Loss: 2.5615\n",
      "Iteration 487200, Train Loss: 2.2086, Validation Loss: 2.2551\n",
      "Iteration 487300, Train Loss: 2.1707, Validation Loss: 1.9693\n",
      "Iteration 487400, Train Loss: 2.0467, Validation Loss: 2.4774\n",
      "Iteration 487500, Train Loss: 2.7714, Validation Loss: 2.3674\n",
      "Iteration 487600, Train Loss: 2.1648, Validation Loss: 2.1246\n",
      "Iteration 487700, Train Loss: 2.1515, Validation Loss: 1.7224\n",
      "Iteration 487800, Train Loss: 2.1958, Validation Loss: 2.1666\n",
      "Iteration 487900, Train Loss: 2.1981, Validation Loss: 2.2476\n",
      "Iteration 488000, Train Loss: 2.5177, Validation Loss: 2.4584\n",
      "Iteration 488100, Train Loss: 2.0433, Validation Loss: 2.0189\n",
      "Iteration 488200, Train Loss: 2.4761, Validation Loss: 2.3627\n",
      "Iteration 488300, Train Loss: 2.3453, Validation Loss: 2.8483\n",
      "Iteration 488400, Train Loss: 2.5089, Validation Loss: 2.3987\n",
      "Iteration 488500, Train Loss: 2.3173, Validation Loss: 2.7009\n",
      "Iteration 488600, Train Loss: 2.4805, Validation Loss: 2.1347\n",
      "Iteration 488700, Train Loss: 1.8973, Validation Loss: 2.3570\n",
      "Iteration 488800, Train Loss: 2.5324, Validation Loss: 2.0028\n",
      "Iteration 488900, Train Loss: 2.2264, Validation Loss: 2.1250\n",
      "Iteration 489000, Train Loss: 2.2901, Validation Loss: 2.8959\n",
      "Iteration 489100, Train Loss: 1.8976, Validation Loss: 2.3804\n",
      "Iteration 489200, Train Loss: 2.3456, Validation Loss: 2.1814\n",
      "Iteration 489300, Train Loss: 2.3156, Validation Loss: 2.7772\n",
      "Iteration 489400, Train Loss: 2.0350, Validation Loss: 2.5149\n",
      "Iteration 489500, Train Loss: 2.3282, Validation Loss: 2.3706\n",
      "Iteration 489600, Train Loss: 1.9776, Validation Loss: 2.8938\n",
      "Iteration 489700, Train Loss: 2.4167, Validation Loss: 2.0719\n",
      "Iteration 489800, Train Loss: 2.4075, Validation Loss: 2.3503\n",
      "Iteration 489900, Train Loss: 2.4006, Validation Loss: 2.2338\n",
      "Iteration 490000, Train Loss: 2.4139, Validation Loss: 2.0839\n",
      "Iteration 490100, Train Loss: 2.0096, Validation Loss: 2.4376\n",
      "Iteration 490200, Train Loss: 2.2273, Validation Loss: 2.3067\n",
      "Iteration 490300, Train Loss: 2.4452, Validation Loss: 2.5567\n",
      "Iteration 490400, Train Loss: 1.9651, Validation Loss: 2.5922\n",
      "Iteration 490500, Train Loss: 2.2002, Validation Loss: 2.4427\n",
      "Iteration 490600, Train Loss: 2.1236, Validation Loss: 2.5716\n",
      "Iteration 490700, Train Loss: 2.0817, Validation Loss: 2.1917\n",
      "Iteration 490800, Train Loss: 2.2392, Validation Loss: 2.2415\n",
      "Iteration 490900, Train Loss: 2.3433, Validation Loss: 2.3884\n",
      "Iteration 491000, Train Loss: 2.2813, Validation Loss: 2.4518\n",
      "Iteration 491100, Train Loss: 2.2092, Validation Loss: 2.5555\n",
      "Iteration 491200, Train Loss: 2.1762, Validation Loss: 1.9872\n",
      "Iteration 491300, Train Loss: 2.4067, Validation Loss: 2.7463\n",
      "Iteration 491400, Train Loss: 1.9287, Validation Loss: 2.4606\n",
      "Iteration 491500, Train Loss: 1.7854, Validation Loss: 2.0717\n",
      "Iteration 491600, Train Loss: 2.3929, Validation Loss: 2.4711\n",
      "Iteration 491700, Train Loss: 2.5915, Validation Loss: 1.8077\n",
      "Iteration 491800, Train Loss: 2.2836, Validation Loss: 2.4081\n",
      "Iteration 491900, Train Loss: 2.3783, Validation Loss: 2.7544\n",
      "Iteration 492000, Train Loss: 2.3136, Validation Loss: 2.3040\n",
      "Iteration 492100, Train Loss: 2.1360, Validation Loss: 2.3323\n",
      "Iteration 492200, Train Loss: 2.3578, Validation Loss: 2.3193\n",
      "Iteration 492300, Train Loss: 1.8676, Validation Loss: 2.4024\n",
      "Iteration 492400, Train Loss: 2.0273, Validation Loss: 1.5817\n",
      "Iteration 492500, Train Loss: 2.3154, Validation Loss: 2.2559\n",
      "Iteration 492600, Train Loss: 1.9067, Validation Loss: 2.4864\n",
      "Iteration 492700, Train Loss: 2.3599, Validation Loss: 2.4582\n",
      "Iteration 492800, Train Loss: 2.2665, Validation Loss: 2.5807\n",
      "Iteration 492900, Train Loss: 2.4155, Validation Loss: 2.3659\n",
      "Iteration 493000, Train Loss: 2.2317, Validation Loss: 2.3811\n",
      "Iteration 493100, Train Loss: 2.3827, Validation Loss: 2.4575\n",
      "Iteration 493200, Train Loss: 2.0645, Validation Loss: 2.4419\n",
      "Iteration 493300, Train Loss: 2.6413, Validation Loss: 2.3023\n",
      "Iteration 493400, Train Loss: 2.4735, Validation Loss: 1.9909\n",
      "Iteration 493500, Train Loss: 2.4532, Validation Loss: 2.2501\n",
      "Iteration 493600, Train Loss: 2.2751, Validation Loss: 2.2263\n",
      "Iteration 493700, Train Loss: 2.5856, Validation Loss: 2.0910\n",
      "Iteration 493800, Train Loss: 2.4177, Validation Loss: 2.0505\n",
      "Iteration 493900, Train Loss: 2.0273, Validation Loss: 2.3260\n",
      "Iteration 494000, Train Loss: 2.1268, Validation Loss: 2.5244\n",
      "Iteration 494100, Train Loss: 1.9907, Validation Loss: 2.5398\n",
      "Iteration 494200, Train Loss: 2.6416, Validation Loss: 1.8120\n",
      "Iteration 494300, Train Loss: 2.2070, Validation Loss: 1.9865\n",
      "Iteration 494400, Train Loss: 1.9691, Validation Loss: 2.7086\n",
      "Iteration 494500, Train Loss: 1.9404, Validation Loss: 2.2553\n",
      "Iteration 494600, Train Loss: 2.0360, Validation Loss: 2.1616\n",
      "Iteration 494700, Train Loss: 2.2894, Validation Loss: 2.0654\n",
      "Iteration 494800, Train Loss: 2.0870, Validation Loss: 2.1559\n",
      "Iteration 494900, Train Loss: 2.3143, Validation Loss: 2.1363\n",
      "Iteration 495000, Train Loss: 2.3394, Validation Loss: 2.3846\n",
      "Iteration 495100, Train Loss: 2.2703, Validation Loss: 2.6601\n",
      "Iteration 495200, Train Loss: 1.7731, Validation Loss: 2.3366\n",
      "Iteration 495300, Train Loss: 2.3940, Validation Loss: 2.2601\n",
      "Iteration 495400, Train Loss: 2.5607, Validation Loss: 2.4158\n",
      "Iteration 495500, Train Loss: 2.6361, Validation Loss: 2.7740\n",
      "Iteration 495600, Train Loss: 2.5378, Validation Loss: 2.0848\n",
      "Iteration 495700, Train Loss: 2.1040, Validation Loss: 2.3366\n",
      "Iteration 495800, Train Loss: 2.3981, Validation Loss: 2.1632\n",
      "Iteration 495900, Train Loss: 2.5810, Validation Loss: 2.2375\n",
      "Iteration 496000, Train Loss: 2.6845, Validation Loss: 2.7792\n",
      "Iteration 496100, Train Loss: 2.4943, Validation Loss: 2.2447\n",
      "Iteration 496200, Train Loss: 2.5866, Validation Loss: 2.7064\n",
      "Iteration 496300, Train Loss: 1.8308, Validation Loss: 2.2070\n",
      "Iteration 496400, Train Loss: 2.3417, Validation Loss: 2.5543\n",
      "Iteration 496500, Train Loss: 2.8693, Validation Loss: 2.2479\n",
      "Iteration 496600, Train Loss: 1.9492, Validation Loss: 2.1773\n",
      "Iteration 496700, Train Loss: 2.5781, Validation Loss: 2.5486\n",
      "Iteration 496800, Train Loss: 2.2759, Validation Loss: 2.5274\n",
      "Iteration 496900, Train Loss: 2.6482, Validation Loss: 2.7488\n",
      "Iteration 497000, Train Loss: 2.4014, Validation Loss: 2.4755\n",
      "Iteration 497100, Train Loss: 2.1057, Validation Loss: 2.3601\n",
      "Iteration 497200, Train Loss: 2.0567, Validation Loss: 2.6160\n",
      "Iteration 497300, Train Loss: 2.3388, Validation Loss: 1.9904\n",
      "Iteration 497400, Train Loss: 2.4344, Validation Loss: 2.1056\n",
      "Iteration 497500, Train Loss: 2.1495, Validation Loss: 2.4303\n",
      "Iteration 497600, Train Loss: 2.7055, Validation Loss: 2.3181\n",
      "Iteration 497700, Train Loss: 2.5283, Validation Loss: 2.2820\n",
      "Iteration 497800, Train Loss: 2.3831, Validation Loss: 2.4078\n",
      "Iteration 497900, Train Loss: 2.4077, Validation Loss: 2.0911\n",
      "Iteration 498000, Train Loss: 2.3278, Validation Loss: 2.7582\n",
      "Iteration 498100, Train Loss: 2.6594, Validation Loss: 1.9462\n",
      "Iteration 498200, Train Loss: 2.3962, Validation Loss: 2.1942\n",
      "Iteration 498300, Train Loss: 2.5332, Validation Loss: 3.2082\n",
      "Iteration 498400, Train Loss: 2.3925, Validation Loss: 2.2038\n",
      "Iteration 498500, Train Loss: 2.0769, Validation Loss: 2.0830\n",
      "Iteration 498600, Train Loss: 2.4477, Validation Loss: 2.7331\n",
      "Iteration 498700, Train Loss: 2.3733, Validation Loss: 2.0433\n",
      "Iteration 498800, Train Loss: 2.0868, Validation Loss: 2.3040\n",
      "Iteration 498900, Train Loss: 2.7318, Validation Loss: 1.8813\n",
      "Iteration 499000, Train Loss: 2.2640, Validation Loss: 2.6293\n",
      "Iteration 499100, Train Loss: 2.1170, Validation Loss: 2.3139\n",
      "Iteration 499200, Train Loss: 2.3373, Validation Loss: 2.5393\n",
      "Iteration 499300, Train Loss: 2.2090, Validation Loss: 2.2725\n",
      "Iteration 499400, Train Loss: 2.1939, Validation Loss: 1.8581\n",
      "Iteration 499500, Train Loss: 2.5113, Validation Loss: 1.7632\n",
      "Iteration 499600, Train Loss: 1.9753, Validation Loss: 2.0431\n",
      "Iteration 499700, Train Loss: 2.2233, Validation Loss: 2.4242\n",
      "Iteration 499800, Train Loss: 2.0287, Validation Loss: 2.4186\n",
      "Iteration 499900, Train Loss: 1.9511, Validation Loss: 2.0225\n",
      "Iteration 500000, Train Loss: 2.1232, Validation Loss: 2.4352\n",
      "Iteration 500100, Train Loss: 2.5937, Validation Loss: 1.6875\n",
      "Iteration 500200, Train Loss: 2.4035, Validation Loss: 2.0830\n",
      "Iteration 500300, Train Loss: 2.2401, Validation Loss: 2.0797\n",
      "Iteration 500400, Train Loss: 2.5630, Validation Loss: 2.0594\n",
      "Iteration 500500, Train Loss: 1.8944, Validation Loss: 2.3653\n",
      "Iteration 500600, Train Loss: 2.4618, Validation Loss: 2.6276\n",
      "Iteration 500700, Train Loss: 2.3184, Validation Loss: 2.9419\n",
      "Iteration 500800, Train Loss: 2.2239, Validation Loss: 2.2856\n",
      "Iteration 500900, Train Loss: 2.3294, Validation Loss: 2.3071\n",
      "Iteration 501000, Train Loss: 2.2787, Validation Loss: 2.9260\n",
      "Iteration 501100, Train Loss: 2.7428, Validation Loss: 2.4221\n",
      "Iteration 501200, Train Loss: 1.9101, Validation Loss: 2.3090\n",
      "Iteration 501300, Train Loss: 2.0868, Validation Loss: 2.4378\n",
      "Iteration 501400, Train Loss: 2.2005, Validation Loss: 2.4794\n",
      "Iteration 501500, Train Loss: 2.5871, Validation Loss: 2.4138\n",
      "Iteration 501600, Train Loss: 1.9914, Validation Loss: 2.3358\n",
      "Iteration 501700, Train Loss: 2.1524, Validation Loss: 2.2286\n",
      "Iteration 501800, Train Loss: 2.3894, Validation Loss: 2.4480\n",
      "Iteration 501900, Train Loss: 2.0835, Validation Loss: 2.5917\n",
      "Iteration 502000, Train Loss: 2.5913, Validation Loss: 2.4312\n",
      "Iteration 502100, Train Loss: 2.4150, Validation Loss: 2.4994\n",
      "Iteration 502200, Train Loss: 2.1712, Validation Loss: 2.1443\n",
      "Iteration 502300, Train Loss: 2.6167, Validation Loss: 1.9983\n",
      "Iteration 502400, Train Loss: 2.2049, Validation Loss: 2.5695\n",
      "Iteration 502500, Train Loss: 2.3413, Validation Loss: 2.5385\n",
      "Iteration 502600, Train Loss: 2.4469, Validation Loss: 2.0664\n",
      "Iteration 502700, Train Loss: 2.1553, Validation Loss: 1.9351\n",
      "Iteration 502800, Train Loss: 2.2160, Validation Loss: 2.7713\n",
      "Iteration 502900, Train Loss: 2.3538, Validation Loss: 2.4596\n",
      "Iteration 503000, Train Loss: 2.3777, Validation Loss: 2.5286\n",
      "Iteration 503100, Train Loss: 2.1418, Validation Loss: 2.5747\n",
      "Iteration 503200, Train Loss: 2.3681, Validation Loss: 2.4740\n",
      "Iteration 503300, Train Loss: 2.0618, Validation Loss: 2.4310\n",
      "Iteration 503400, Train Loss: 2.3905, Validation Loss: 2.3231\n",
      "Iteration 503500, Train Loss: 1.6814, Validation Loss: 2.4601\n",
      "Iteration 503600, Train Loss: 2.2984, Validation Loss: 2.6066\n",
      "Iteration 503700, Train Loss: 2.1352, Validation Loss: 1.9709\n",
      "Iteration 503800, Train Loss: 2.3217, Validation Loss: 2.2117\n",
      "Iteration 503900, Train Loss: 2.2223, Validation Loss: 2.0355\n",
      "Iteration 504000, Train Loss: 2.0703, Validation Loss: 2.2735\n",
      "Iteration 504100, Train Loss: 2.3523, Validation Loss: 2.2249\n",
      "Iteration 504200, Train Loss: 2.0175, Validation Loss: 2.6023\n",
      "Iteration 504300, Train Loss: 2.4009, Validation Loss: 2.1017\n",
      "Iteration 504400, Train Loss: 2.7351, Validation Loss: 2.0537\n",
      "Iteration 504500, Train Loss: 2.0318, Validation Loss: 2.2033\n",
      "Iteration 504600, Train Loss: 2.3005, Validation Loss: 2.4353\n",
      "Iteration 504700, Train Loss: 2.0876, Validation Loss: 2.4273\n",
      "Iteration 504800, Train Loss: 2.0591, Validation Loss: 2.5755\n",
      "Iteration 504900, Train Loss: 2.2895, Validation Loss: 1.6640\n",
      "Iteration 505000, Train Loss: 2.0702, Validation Loss: 2.2595\n",
      "Iteration 505100, Train Loss: 2.5690, Validation Loss: 2.3424\n",
      "Iteration 505200, Train Loss: 2.1827, Validation Loss: 2.3318\n",
      "Iteration 505300, Train Loss: 2.2973, Validation Loss: 2.3882\n",
      "Iteration 505400, Train Loss: 2.0964, Validation Loss: 1.9271\n",
      "Iteration 505500, Train Loss: 2.4632, Validation Loss: 2.1837\n",
      "Iteration 505600, Train Loss: 2.2103, Validation Loss: 2.3050\n",
      "Iteration 505700, Train Loss: 2.1009, Validation Loss: 2.9055\n",
      "Iteration 505800, Train Loss: 2.6137, Validation Loss: 2.4467\n",
      "Iteration 505900, Train Loss: 2.2924, Validation Loss: 2.4299\n",
      "Iteration 506000, Train Loss: 2.0165, Validation Loss: 2.2613\n",
      "Iteration 506100, Train Loss: 2.3529, Validation Loss: 2.1125\n",
      "Iteration 506200, Train Loss: 2.2327, Validation Loss: 2.5787\n",
      "Iteration 506300, Train Loss: 2.1983, Validation Loss: 2.0017\n",
      "Iteration 506400, Train Loss: 2.2125, Validation Loss: 2.0676\n",
      "Iteration 506500, Train Loss: 2.1068, Validation Loss: 2.2322\n",
      "Iteration 506600, Train Loss: 2.3848, Validation Loss: 2.5076\n",
      "Iteration 506700, Train Loss: 2.4003, Validation Loss: 2.0721\n",
      "Iteration 506800, Train Loss: 2.5593, Validation Loss: 2.4226\n",
      "Iteration 506900, Train Loss: 2.1059, Validation Loss: 1.8924\n",
      "Iteration 507000, Train Loss: 2.0262, Validation Loss: 2.1394\n",
      "Iteration 507100, Train Loss: 2.3230, Validation Loss: 2.3175\n",
      "Iteration 507200, Train Loss: 2.3970, Validation Loss: 2.6958\n",
      "Iteration 507300, Train Loss: 2.1906, Validation Loss: 2.0883\n",
      "Iteration 507400, Train Loss: 2.8357, Validation Loss: 1.8809\n",
      "Iteration 507500, Train Loss: 2.2240, Validation Loss: 2.6872\n",
      "Iteration 507600, Train Loss: 1.9200, Validation Loss: 2.3486\n",
      "Iteration 507700, Train Loss: 2.6348, Validation Loss: 2.3631\n",
      "Iteration 507800, Train Loss: 2.0686, Validation Loss: 2.4354\n",
      "Iteration 507900, Train Loss: 2.5182, Validation Loss: 2.0431\n",
      "Iteration 508000, Train Loss: 2.1434, Validation Loss: 1.8297\n",
      "Iteration 508100, Train Loss: 2.7673, Validation Loss: 2.1581\n",
      "Iteration 508200, Train Loss: 2.4712, Validation Loss: 2.7471\n",
      "Iteration 508300, Train Loss: 2.7658, Validation Loss: 2.2791\n",
      "Iteration 508400, Train Loss: 2.3929, Validation Loss: 2.3273\n",
      "Iteration 508500, Train Loss: 2.0927, Validation Loss: 2.0545\n",
      "Iteration 508600, Train Loss: 2.4972, Validation Loss: 2.4035\n",
      "Iteration 508700, Train Loss: 2.4905, Validation Loss: 2.2037\n",
      "Iteration 508800, Train Loss: 2.3658, Validation Loss: 2.2056\n",
      "Iteration 508900, Train Loss: 2.3205, Validation Loss: 2.8142\n",
      "Iteration 509000, Train Loss: 2.3279, Validation Loss: 2.1101\n",
      "Iteration 509100, Train Loss: 2.6750, Validation Loss: 1.9625\n",
      "Iteration 509200, Train Loss: 2.6666, Validation Loss: 2.2542\n",
      "Iteration 509300, Train Loss: 2.3180, Validation Loss: 2.3802\n",
      "Iteration 509400, Train Loss: 2.1725, Validation Loss: 2.3658\n",
      "Iteration 509500, Train Loss: 2.2070, Validation Loss: 2.4337\n",
      "Iteration 509600, Train Loss: 2.0667, Validation Loss: 2.2126\n",
      "Iteration 509700, Train Loss: 2.3942, Validation Loss: 2.3051\n",
      "Iteration 509800, Train Loss: 2.6982, Validation Loss: 2.2445\n",
      "Iteration 509900, Train Loss: 1.9407, Validation Loss: 2.0083\n",
      "Iteration 510000, Train Loss: 2.5286, Validation Loss: 2.6030\n",
      "Iteration 510100, Train Loss: 2.0774, Validation Loss: 1.9663\n",
      "Iteration 510200, Train Loss: 2.4376, Validation Loss: 2.2492\n",
      "Iteration 510300, Train Loss: 1.8607, Validation Loss: 2.1960\n",
      "Iteration 510400, Train Loss: 2.3062, Validation Loss: 2.3414\n",
      "Iteration 510500, Train Loss: 2.4678, Validation Loss: 2.6176\n",
      "Iteration 510600, Train Loss: 2.1235, Validation Loss: 2.1466\n",
      "Iteration 510700, Train Loss: 2.0560, Validation Loss: 2.1103\n",
      "Iteration 510800, Train Loss: 2.6405, Validation Loss: 2.3095\n",
      "Iteration 510900, Train Loss: 2.5793, Validation Loss: 2.1071\n",
      "Iteration 511000, Train Loss: 2.5721, Validation Loss: 1.9789\n",
      "Iteration 511100, Train Loss: 2.1240, Validation Loss: 2.1620\n",
      "Iteration 511200, Train Loss: 2.1605, Validation Loss: 2.4239\n",
      "Iteration 511300, Train Loss: 2.4309, Validation Loss: 2.5603\n",
      "Iteration 511400, Train Loss: 2.2973, Validation Loss: 2.5504\n",
      "Iteration 511500, Train Loss: 2.0596, Validation Loss: 2.4405\n",
      "Iteration 511600, Train Loss: 2.2479, Validation Loss: 2.1554\n",
      "Iteration 511700, Train Loss: 2.3687, Validation Loss: 2.0436\n",
      "Iteration 511800, Train Loss: 2.1228, Validation Loss: 2.2775\n",
      "Iteration 511900, Train Loss: 2.2011, Validation Loss: 2.6143\n",
      "Iteration 512000, Train Loss: 3.0633, Validation Loss: 2.5157\n",
      "Iteration 512100, Train Loss: 2.4253, Validation Loss: 1.8231\n",
      "Iteration 512200, Train Loss: 2.2055, Validation Loss: 2.4798\n",
      "Iteration 512300, Train Loss: 1.9604, Validation Loss: 2.4307\n",
      "Iteration 512400, Train Loss: 2.2419, Validation Loss: 2.3932\n",
      "Iteration 512500, Train Loss: 2.3601, Validation Loss: 2.3242\n",
      "Iteration 512600, Train Loss: 2.1693, Validation Loss: 2.3630\n",
      "Iteration 512700, Train Loss: 2.1590, Validation Loss: 2.5276\n",
      "Iteration 512800, Train Loss: 2.4254, Validation Loss: 2.1447\n",
      "Iteration 512900, Train Loss: 2.8550, Validation Loss: 2.3726\n",
      "Iteration 513000, Train Loss: 2.3801, Validation Loss: 2.1217\n",
      "Iteration 513100, Train Loss: 2.2532, Validation Loss: 2.0017\n",
      "Iteration 513200, Train Loss: 1.9552, Validation Loss: 2.5087\n",
      "Iteration 513300, Train Loss: 2.1197, Validation Loss: 2.9049\n",
      "Iteration 513400, Train Loss: 2.4066, Validation Loss: 1.7270\n",
      "Iteration 513500, Train Loss: 1.8960, Validation Loss: 1.5492\n",
      "Iteration 513600, Train Loss: 2.3240, Validation Loss: 2.7022\n",
      "Iteration 513700, Train Loss: 2.2735, Validation Loss: 2.3360\n",
      "Iteration 513800, Train Loss: 2.1022, Validation Loss: 2.3104\n",
      "Iteration 513900, Train Loss: 2.2506, Validation Loss: 2.6952\n",
      "Iteration 514000, Train Loss: 2.4038, Validation Loss: 1.8093\n",
      "Iteration 514100, Train Loss: 2.8568, Validation Loss: 2.1135\n",
      "Iteration 514200, Train Loss: 2.0007, Validation Loss: 2.2882\n",
      "Iteration 514300, Train Loss: 2.6582, Validation Loss: 2.2014\n",
      "Iteration 514400, Train Loss: 1.8256, Validation Loss: 2.2017\n",
      "Iteration 514500, Train Loss: 2.0687, Validation Loss: 2.1022\n",
      "Iteration 514600, Train Loss: 2.2714, Validation Loss: 2.3569\n",
      "Iteration 514700, Train Loss: 2.2523, Validation Loss: 2.1942\n",
      "Iteration 514800, Train Loss: 2.2498, Validation Loss: 2.5198\n",
      "Iteration 514900, Train Loss: 2.2127, Validation Loss: 2.1195\n",
      "Iteration 515000, Train Loss: 2.2689, Validation Loss: 1.9474\n",
      "Iteration 515100, Train Loss: 2.5048, Validation Loss: 2.5054\n",
      "Iteration 515200, Train Loss: 1.7971, Validation Loss: 2.3669\n",
      "Iteration 515300, Train Loss: 2.2004, Validation Loss: 2.9335\n",
      "Iteration 515400, Train Loss: 2.5569, Validation Loss: 2.8008\n",
      "Iteration 515500, Train Loss: 1.9720, Validation Loss: 2.2794\n",
      "Iteration 515600, Train Loss: 2.1465, Validation Loss: 2.3280\n",
      "Iteration 515700, Train Loss: 1.8438, Validation Loss: 2.1477\n",
      "Iteration 515800, Train Loss: 3.0784, Validation Loss: 2.1658\n",
      "Iteration 515900, Train Loss: 1.9826, Validation Loss: 2.4383\n",
      "Iteration 516000, Train Loss: 2.3232, Validation Loss: 2.0233\n",
      "Iteration 516100, Train Loss: 2.1200, Validation Loss: 2.0619\n",
      "Iteration 516200, Train Loss: 1.9960, Validation Loss: 2.4572\n",
      "Iteration 516300, Train Loss: 2.3625, Validation Loss: 1.9952\n",
      "Iteration 516400, Train Loss: 2.0638, Validation Loss: 1.9227\n",
      "Iteration 516500, Train Loss: 2.5261, Validation Loss: 2.4615\n",
      "Iteration 516600, Train Loss: 2.2094, Validation Loss: 2.1963\n",
      "Iteration 516700, Train Loss: 1.9279, Validation Loss: 2.7795\n",
      "Iteration 516800, Train Loss: 2.3600, Validation Loss: 2.8712\n",
      "Iteration 516900, Train Loss: 2.4937, Validation Loss: 2.5067\n",
      "Iteration 517000, Train Loss: 2.3600, Validation Loss: 2.9474\n",
      "Iteration 517100, Train Loss: 2.3283, Validation Loss: 2.2363\n",
      "Iteration 517200, Train Loss: 2.1069, Validation Loss: 2.6362\n",
      "Iteration 517300, Train Loss: 2.3522, Validation Loss: 2.4823\n",
      "Iteration 517400, Train Loss: 2.1850, Validation Loss: 2.2497\n",
      "Iteration 517500, Train Loss: 1.9716, Validation Loss: 2.4720\n",
      "Iteration 517600, Train Loss: 2.3234, Validation Loss: 1.9680\n",
      "Iteration 517700, Train Loss: 2.7027, Validation Loss: 2.1810\n",
      "Iteration 517800, Train Loss: 2.5326, Validation Loss: 2.1384\n",
      "Iteration 517900, Train Loss: 1.8242, Validation Loss: 2.3698\n",
      "Iteration 518000, Train Loss: 1.9696, Validation Loss: 2.5479\n",
      "Iteration 518100, Train Loss: 2.5507, Validation Loss: 2.1471\n",
      "Iteration 518200, Train Loss: 2.4590, Validation Loss: 2.2473\n",
      "Iteration 518300, Train Loss: 1.8445, Validation Loss: 2.5351\n",
      "Iteration 518400, Train Loss: 2.2590, Validation Loss: 2.3373\n",
      "Iteration 518500, Train Loss: 2.2254, Validation Loss: 2.0873\n",
      "Iteration 518600, Train Loss: 2.2930, Validation Loss: 2.3223\n",
      "Iteration 518700, Train Loss: 2.7655, Validation Loss: 2.2329\n",
      "Iteration 518800, Train Loss: 2.6412, Validation Loss: 2.4476\n",
      "Iteration 518900, Train Loss: 2.2631, Validation Loss: 2.5424\n",
      "Iteration 519000, Train Loss: 2.1603, Validation Loss: 2.4046\n",
      "Iteration 519100, Train Loss: 2.0727, Validation Loss: 2.3032\n",
      "Iteration 519200, Train Loss: 2.3082, Validation Loss: 2.7070\n",
      "Iteration 519300, Train Loss: 2.4588, Validation Loss: 1.8995\n",
      "Iteration 519400, Train Loss: 2.1229, Validation Loss: 2.2165\n",
      "Iteration 519500, Train Loss: 2.5547, Validation Loss: 2.5123\n",
      "Iteration 519600, Train Loss: 2.1610, Validation Loss: 2.2097\n",
      "Iteration 519700, Train Loss: 2.3783, Validation Loss: 2.1730\n",
      "Iteration 519800, Train Loss: 2.1701, Validation Loss: 2.1754\n",
      "Iteration 519900, Train Loss: 2.0148, Validation Loss: 2.3210\n",
      "Iteration 520000, Train Loss: 2.1002, Validation Loss: 2.5214\n",
      "Iteration 520100, Train Loss: 1.9813, Validation Loss: 2.0773\n",
      "Iteration 520200, Train Loss: 2.0127, Validation Loss: 2.3938\n",
      "Iteration 520300, Train Loss: 2.2528, Validation Loss: 2.0088\n",
      "Iteration 520400, Train Loss: 2.1553, Validation Loss: 2.2216\n",
      "Iteration 520500, Train Loss: 2.3847, Validation Loss: 2.1859\n",
      "Iteration 520600, Train Loss: 2.0571, Validation Loss: 2.4504\n",
      "Iteration 520700, Train Loss: 2.2981, Validation Loss: 2.6456\n",
      "Iteration 520800, Train Loss: 2.3312, Validation Loss: 2.4171\n",
      "Iteration 520900, Train Loss: 1.9204, Validation Loss: 2.6261\n",
      "Iteration 521000, Train Loss: 2.3278, Validation Loss: 2.2119\n",
      "Iteration 521100, Train Loss: 2.3272, Validation Loss: 2.6360\n",
      "Iteration 521200, Train Loss: 2.3024, Validation Loss: 2.6057\n",
      "Iteration 521300, Train Loss: 2.4264, Validation Loss: 2.0454\n",
      "Iteration 521400, Train Loss: 1.9286, Validation Loss: 2.2726\n",
      "Iteration 521500, Train Loss: 2.2455, Validation Loss: 2.3790\n",
      "Iteration 521600, Train Loss: 2.4745, Validation Loss: 2.3081\n",
      "Iteration 521700, Train Loss: 2.4540, Validation Loss: 2.2198\n",
      "Iteration 521800, Train Loss: 2.2572, Validation Loss: 2.7397\n",
      "Iteration 521900, Train Loss: 2.0344, Validation Loss: 2.1987\n",
      "Iteration 522000, Train Loss: 2.4649, Validation Loss: 2.5633\n",
      "Iteration 522100, Train Loss: 2.3101, Validation Loss: 2.8583\n",
      "Iteration 522200, Train Loss: 2.5067, Validation Loss: 2.1169\n",
      "Iteration 522300, Train Loss: 2.6120, Validation Loss: 2.8589\n",
      "Iteration 522400, Train Loss: 2.2872, Validation Loss: 2.4627\n",
      "Iteration 522500, Train Loss: 2.6436, Validation Loss: 2.4043\n",
      "Iteration 522600, Train Loss: 2.5707, Validation Loss: 2.4583\n",
      "Iteration 522700, Train Loss: 1.8755, Validation Loss: 2.5792\n",
      "Iteration 522800, Train Loss: 2.3336, Validation Loss: 1.9847\n",
      "Iteration 522900, Train Loss: 2.1753, Validation Loss: 2.2720\n",
      "Iteration 523000, Train Loss: 2.5596, Validation Loss: 2.5169\n",
      "Iteration 523100, Train Loss: 1.8572, Validation Loss: 1.8470\n",
      "Iteration 523200, Train Loss: 2.0394, Validation Loss: 2.9667\n",
      "Iteration 523300, Train Loss: 1.6825, Validation Loss: 2.4231\n",
      "Iteration 523400, Train Loss: 2.1215, Validation Loss: 2.4593\n",
      "Iteration 523500, Train Loss: 2.4790, Validation Loss: 2.5236\n",
      "Iteration 523600, Train Loss: 2.3785, Validation Loss: 2.4476\n",
      "Iteration 523700, Train Loss: 2.6964, Validation Loss: 2.3389\n",
      "Iteration 523800, Train Loss: 2.4325, Validation Loss: 2.3464\n",
      "Iteration 523900, Train Loss: 2.1722, Validation Loss: 2.5128\n",
      "Iteration 524000, Train Loss: 2.4830, Validation Loss: 2.1851\n",
      "Iteration 524100, Train Loss: 1.8043, Validation Loss: 1.8821\n",
      "Iteration 524200, Train Loss: 2.3078, Validation Loss: 2.3006\n",
      "Iteration 524300, Train Loss: 2.1228, Validation Loss: 2.2530\n",
      "Iteration 524400, Train Loss: 2.4583, Validation Loss: 2.3427\n",
      "Iteration 524500, Train Loss: 2.2832, Validation Loss: 2.2060\n",
      "Iteration 524600, Train Loss: 1.9681, Validation Loss: 2.3052\n",
      "Iteration 524700, Train Loss: 1.9266, Validation Loss: 2.4833\n",
      "Iteration 524800, Train Loss: 2.1550, Validation Loss: 1.7592\n",
      "Iteration 524900, Train Loss: 2.0673, Validation Loss: 1.8798\n",
      "Iteration 525000, Train Loss: 1.7716, Validation Loss: 2.6037\n",
      "Iteration 525100, Train Loss: 1.6504, Validation Loss: 2.3412\n",
      "Iteration 525200, Train Loss: 2.3314, Validation Loss: 3.0492\n",
      "Iteration 525300, Train Loss: 2.3115, Validation Loss: 2.2832\n",
      "Iteration 525400, Train Loss: 2.2307, Validation Loss: 1.8081\n",
      "Iteration 525500, Train Loss: 2.0440, Validation Loss: 2.8704\n",
      "Iteration 525600, Train Loss: 2.0487, Validation Loss: 2.1547\n",
      "Iteration 525700, Train Loss: 2.2459, Validation Loss: 2.6438\n",
      "Iteration 525800, Train Loss: 2.0777, Validation Loss: 2.3235\n",
      "Iteration 525900, Train Loss: 2.3132, Validation Loss: 2.0992\n",
      "Iteration 526000, Train Loss: 1.9559, Validation Loss: 3.0108\n",
      "Iteration 526100, Train Loss: 2.1801, Validation Loss: 2.5162\n",
      "Iteration 526200, Train Loss: 1.9957, Validation Loss: 2.2930\n",
      "Iteration 526300, Train Loss: 2.3522, Validation Loss: 2.1499\n",
      "Iteration 526400, Train Loss: 2.0748, Validation Loss: 2.2692\n",
      "Iteration 526500, Train Loss: 2.1771, Validation Loss: 2.1413\n",
      "Iteration 526600, Train Loss: 2.5261, Validation Loss: 1.9641\n",
      "Iteration 526700, Train Loss: 2.0087, Validation Loss: 2.1705\n",
      "Iteration 526800, Train Loss: 2.4828, Validation Loss: 1.9077\n",
      "Iteration 526900, Train Loss: 2.0153, Validation Loss: 2.5913\n",
      "Iteration 527000, Train Loss: 2.2247, Validation Loss: 1.8794\n",
      "Iteration 527100, Train Loss: 2.1034, Validation Loss: 2.3415\n",
      "Iteration 527200, Train Loss: 1.9061, Validation Loss: 2.0978\n",
      "Iteration 527300, Train Loss: 2.4859, Validation Loss: 1.9623\n",
      "Iteration 527400, Train Loss: 1.8708, Validation Loss: 2.5641\n",
      "Iteration 527500, Train Loss: 2.0618, Validation Loss: 2.2359\n",
      "Iteration 527600, Train Loss: 2.5877, Validation Loss: 2.0000\n",
      "Iteration 527700, Train Loss: 2.0157, Validation Loss: 2.1484\n",
      "Iteration 527800, Train Loss: 2.3714, Validation Loss: 1.8884\n",
      "Iteration 527900, Train Loss: 2.2732, Validation Loss: 2.1057\n",
      "Iteration 528000, Train Loss: 2.0220, Validation Loss: 2.8399\n",
      "Iteration 528100, Train Loss: 2.1334, Validation Loss: 2.0439\n",
      "Iteration 528200, Train Loss: 2.2052, Validation Loss: 2.3886\n",
      "Iteration 528300, Train Loss: 2.5111, Validation Loss: 2.6339\n",
      "Iteration 528400, Train Loss: 2.4071, Validation Loss: 2.5916\n",
      "Iteration 528500, Train Loss: 2.4482, Validation Loss: 2.4035\n",
      "Iteration 528600, Train Loss: 2.3135, Validation Loss: 2.0325\n",
      "Iteration 528700, Train Loss: 2.5638, Validation Loss: 1.8960\n",
      "Iteration 528800, Train Loss: 2.4667, Validation Loss: 2.1775\n",
      "Iteration 528900, Train Loss: 2.2779, Validation Loss: 2.4330\n",
      "Iteration 529000, Train Loss: 2.1901, Validation Loss: 1.8690\n",
      "Iteration 529100, Train Loss: 2.2835, Validation Loss: 2.2422\n",
      "Iteration 529200, Train Loss: 2.1026, Validation Loss: 2.3694\n",
      "Iteration 529300, Train Loss: 2.1412, Validation Loss: 1.9309\n",
      "Iteration 529400, Train Loss: 2.3546, Validation Loss: 2.1061\n",
      "Iteration 529500, Train Loss: 2.1960, Validation Loss: 2.4594\n",
      "Iteration 529600, Train Loss: 2.2518, Validation Loss: 2.5497\n",
      "Iteration 529700, Train Loss: 2.5808, Validation Loss: 2.0265\n",
      "Iteration 529800, Train Loss: 2.0813, Validation Loss: 2.7766\n",
      "Iteration 529900, Train Loss: 2.3061, Validation Loss: 2.4154\n",
      "Iteration 530000, Train Loss: 2.1966, Validation Loss: 2.5110\n",
      "Iteration 530100, Train Loss: 2.2575, Validation Loss: 1.9831\n",
      "Iteration 530200, Train Loss: 2.0253, Validation Loss: 2.2618\n",
      "Iteration 530300, Train Loss: 2.3765, Validation Loss: 2.2881\n",
      "Iteration 530400, Train Loss: 2.2599, Validation Loss: 2.4382\n",
      "Iteration 530500, Train Loss: 2.4030, Validation Loss: 2.3280\n",
      "Iteration 530600, Train Loss: 2.2060, Validation Loss: 2.3688\n",
      "Iteration 530700, Train Loss: 2.0214, Validation Loss: 2.0136\n",
      "Iteration 530800, Train Loss: 2.7125, Validation Loss: 2.3288\n",
      "Iteration 530900, Train Loss: 2.1873, Validation Loss: 2.3588\n",
      "Iteration 531000, Train Loss: 2.5372, Validation Loss: 2.0754\n",
      "Iteration 531100, Train Loss: 2.0149, Validation Loss: 2.2916\n",
      "Iteration 531200, Train Loss: 2.1898, Validation Loss: 2.0131\n",
      "Iteration 531300, Train Loss: 2.2475, Validation Loss: 2.2317\n",
      "Iteration 531400, Train Loss: 2.3499, Validation Loss: 2.0463\n",
      "Iteration 531500, Train Loss: 2.5585, Validation Loss: 1.9484\n",
      "Iteration 531600, Train Loss: 2.2393, Validation Loss: 2.1845\n",
      "Iteration 531700, Train Loss: 2.0341, Validation Loss: 2.1277\n",
      "Iteration 531800, Train Loss: 2.5356, Validation Loss: 2.2370\n",
      "Iteration 531900, Train Loss: 1.9420, Validation Loss: 2.5235\n",
      "Iteration 532000, Train Loss: 2.7347, Validation Loss: 1.9959\n",
      "Iteration 532100, Train Loss: 2.3021, Validation Loss: 1.9369\n",
      "Iteration 532200, Train Loss: 2.5203, Validation Loss: 1.8003\n",
      "Iteration 532300, Train Loss: 2.0411, Validation Loss: 2.1224\n",
      "Iteration 532400, Train Loss: 2.3020, Validation Loss: 2.0512\n",
      "Iteration 532500, Train Loss: 2.6177, Validation Loss: 2.4414\n",
      "Iteration 532600, Train Loss: 2.5467, Validation Loss: 2.4927\n",
      "Iteration 532700, Train Loss: 2.2816, Validation Loss: 2.2435\n",
      "Iteration 532800, Train Loss: 2.2776, Validation Loss: 2.5518\n",
      "Iteration 532900, Train Loss: 1.8314, Validation Loss: 2.1826\n",
      "Iteration 533000, Train Loss: 2.3356, Validation Loss: 2.7014\n",
      "Iteration 533100, Train Loss: 2.6719, Validation Loss: 2.6570\n",
      "Iteration 533200, Train Loss: 2.7160, Validation Loss: 2.1356\n",
      "Iteration 533300, Train Loss: 2.2164, Validation Loss: 2.7806\n",
      "Iteration 533400, Train Loss: 2.0271, Validation Loss: 1.7010\n",
      "Iteration 533500, Train Loss: 2.1665, Validation Loss: 2.1300\n",
      "Iteration 533600, Train Loss: 2.1139, Validation Loss: 2.8044\n",
      "Iteration 533700, Train Loss: 2.2273, Validation Loss: 2.2534\n",
      "Iteration 533800, Train Loss: 2.6129, Validation Loss: 2.9524\n",
      "Iteration 533900, Train Loss: 2.2060, Validation Loss: 2.4521\n",
      "Iteration 534000, Train Loss: 2.6282, Validation Loss: 2.1808\n",
      "Iteration 534100, Train Loss: 2.2669, Validation Loss: 2.3324\n",
      "Iteration 534200, Train Loss: 1.9619, Validation Loss: 1.7973\n",
      "Iteration 534300, Train Loss: 2.4116, Validation Loss: 1.7830\n",
      "Iteration 534400, Train Loss: 2.2687, Validation Loss: 2.4888\n",
      "Iteration 534500, Train Loss: 2.5516, Validation Loss: 2.5693\n",
      "Iteration 534600, Train Loss: 2.5178, Validation Loss: 2.5117\n",
      "Iteration 534700, Train Loss: 2.2493, Validation Loss: 2.4057\n",
      "Iteration 534800, Train Loss: 2.3436, Validation Loss: 2.6337\n",
      "Iteration 534900, Train Loss: 2.1893, Validation Loss: 2.3090\n",
      "Iteration 535000, Train Loss: 1.9353, Validation Loss: 2.3239\n",
      "Iteration 535100, Train Loss: 2.3091, Validation Loss: 2.2635\n",
      "Iteration 535200, Train Loss: 2.6539, Validation Loss: 2.4641\n",
      "Iteration 535300, Train Loss: 2.0836, Validation Loss: 2.2950\n",
      "Iteration 535400, Train Loss: 2.0424, Validation Loss: 1.9405\n",
      "Iteration 535500, Train Loss: 2.2871, Validation Loss: 2.5294\n",
      "Iteration 535600, Train Loss: 2.1717, Validation Loss: 1.8279\n",
      "Iteration 535700, Train Loss: 2.0749, Validation Loss: 2.1354\n",
      "Iteration 535800, Train Loss: 2.5443, Validation Loss: 2.3920\n",
      "Iteration 535900, Train Loss: 2.0835, Validation Loss: 2.1026\n",
      "Iteration 536000, Train Loss: 2.0294, Validation Loss: 2.4998\n",
      "Iteration 536100, Train Loss: 1.8981, Validation Loss: 2.1154\n",
      "Iteration 536200, Train Loss: 2.7549, Validation Loss: 2.6800\n",
      "Iteration 536300, Train Loss: 2.4955, Validation Loss: 2.5958\n",
      "Iteration 536400, Train Loss: 2.2375, Validation Loss: 2.0197\n",
      "Iteration 536500, Train Loss: 2.2996, Validation Loss: 2.3922\n",
      "Iteration 536600, Train Loss: 2.1892, Validation Loss: 2.1990\n",
      "Iteration 536700, Train Loss: 2.0198, Validation Loss: 2.2246\n",
      "Iteration 536800, Train Loss: 2.3368, Validation Loss: 2.1469\n",
      "Iteration 536900, Train Loss: 2.4035, Validation Loss: 2.4691\n",
      "Iteration 537000, Train Loss: 2.2075, Validation Loss: 2.2177\n",
      "Iteration 537100, Train Loss: 2.3960, Validation Loss: 2.4419\n",
      "Iteration 537200, Train Loss: 2.4554, Validation Loss: 2.7409\n",
      "Iteration 537300, Train Loss: 2.0258, Validation Loss: 2.1363\n",
      "Iteration 537400, Train Loss: 2.2159, Validation Loss: 2.1958\n",
      "Iteration 537500, Train Loss: 2.2601, Validation Loss: 2.3792\n",
      "Iteration 537600, Train Loss: 2.4204, Validation Loss: 2.4594\n",
      "Iteration 537700, Train Loss: 2.1583, Validation Loss: 2.7139\n",
      "Iteration 537800, Train Loss: 2.3380, Validation Loss: 2.2922\n",
      "Iteration 537900, Train Loss: 2.2579, Validation Loss: 2.0899\n",
      "Iteration 538000, Train Loss: 1.8372, Validation Loss: 2.5127\n",
      "Iteration 538100, Train Loss: 2.4842, Validation Loss: 2.3508\n",
      "Iteration 538200, Train Loss: 2.5998, Validation Loss: 2.7214\n",
      "Iteration 538300, Train Loss: 2.1272, Validation Loss: 2.3261\n",
      "Iteration 538400, Train Loss: 2.0121, Validation Loss: 2.1267\n",
      "Iteration 538500, Train Loss: 2.1815, Validation Loss: 2.5402\n",
      "Iteration 538600, Train Loss: 2.1507, Validation Loss: 2.0453\n",
      "Iteration 538700, Train Loss: 1.7547, Validation Loss: 2.3563\n",
      "Iteration 538800, Train Loss: 1.9331, Validation Loss: 2.7386\n",
      "Iteration 538900, Train Loss: 2.3588, Validation Loss: 1.9320\n",
      "Iteration 539000, Train Loss: 2.1215, Validation Loss: 2.1806\n",
      "Iteration 539100, Train Loss: 2.1729, Validation Loss: 1.9726\n",
      "Iteration 539200, Train Loss: 2.2909, Validation Loss: 1.7708\n",
      "Iteration 539300, Train Loss: 2.5367, Validation Loss: 2.3989\n",
      "Iteration 539400, Train Loss: 2.1907, Validation Loss: 2.2917\n",
      "Iteration 539500, Train Loss: 2.4649, Validation Loss: 2.3421\n",
      "Iteration 539600, Train Loss: 2.3249, Validation Loss: 2.5510\n",
      "Iteration 539700, Train Loss: 2.3592, Validation Loss: 2.5431\n",
      "Iteration 539800, Train Loss: 2.0303, Validation Loss: 2.8056\n",
      "Iteration 539900, Train Loss: 2.1643, Validation Loss: 1.7871\n",
      "Iteration 540000, Train Loss: 2.0403, Validation Loss: 2.1079\n",
      "Iteration 540100, Train Loss: 2.5979, Validation Loss: 2.7765\n",
      "Iteration 540200, Train Loss: 2.3771, Validation Loss: 2.1445\n",
      "Iteration 540300, Train Loss: 2.5319, Validation Loss: 2.0981\n",
      "Iteration 540400, Train Loss: 1.9633, Validation Loss: 2.7260\n",
      "Iteration 540500, Train Loss: 2.3042, Validation Loss: 2.5160\n",
      "Iteration 540600, Train Loss: 2.1334, Validation Loss: 2.6023\n",
      "Iteration 540700, Train Loss: 2.2060, Validation Loss: 2.1254\n",
      "Iteration 540800, Train Loss: 1.8157, Validation Loss: 2.2098\n",
      "Iteration 540900, Train Loss: 1.9433, Validation Loss: 2.4926\n",
      "Iteration 541000, Train Loss: 2.1944, Validation Loss: 2.7507\n",
      "Iteration 541100, Train Loss: 2.0966, Validation Loss: 2.6276\n",
      "Iteration 541200, Train Loss: 2.2994, Validation Loss: 2.0508\n",
      "Iteration 541300, Train Loss: 2.0363, Validation Loss: 2.5968\n",
      "Iteration 541400, Train Loss: 2.3682, Validation Loss: 2.0588\n",
      "Iteration 541500, Train Loss: 2.2776, Validation Loss: 2.2472\n",
      "Iteration 541600, Train Loss: 2.7227, Validation Loss: 2.3381\n",
      "Iteration 541700, Train Loss: 2.2280, Validation Loss: 2.2252\n",
      "Iteration 541800, Train Loss: 2.1564, Validation Loss: 2.2576\n",
      "Iteration 541900, Train Loss: 2.0352, Validation Loss: 2.0923\n",
      "Iteration 542000, Train Loss: 2.3154, Validation Loss: 2.6603\n",
      "Iteration 542100, Train Loss: 2.4161, Validation Loss: 2.4769\n",
      "Iteration 542200, Train Loss: 2.3008, Validation Loss: 2.2896\n",
      "Iteration 542300, Train Loss: 2.4805, Validation Loss: 2.3829\n",
      "Iteration 542400, Train Loss: 2.0759, Validation Loss: 2.2070\n",
      "Iteration 542500, Train Loss: 2.2165, Validation Loss: 2.5757\n",
      "Iteration 542600, Train Loss: 2.3096, Validation Loss: 2.2051\n",
      "Iteration 542700, Train Loss: 1.8769, Validation Loss: 2.4144\n",
      "Iteration 542800, Train Loss: 2.0985, Validation Loss: 2.2054\n",
      "Iteration 542900, Train Loss: 2.3671, Validation Loss: 2.3214\n",
      "Iteration 543000, Train Loss: 2.2014, Validation Loss: 2.0848\n",
      "Iteration 543100, Train Loss: 2.8434, Validation Loss: 2.2017\n",
      "Iteration 543200, Train Loss: 2.0714, Validation Loss: 2.1759\n",
      "Iteration 543300, Train Loss: 2.3746, Validation Loss: 2.4652\n",
      "Iteration 543400, Train Loss: 2.2076, Validation Loss: 2.4895\n",
      "Iteration 543500, Train Loss: 2.3688, Validation Loss: 2.3330\n",
      "Iteration 543600, Train Loss: 2.5707, Validation Loss: 2.2798\n",
      "Iteration 543700, Train Loss: 2.0472, Validation Loss: 2.2046\n",
      "Iteration 543800, Train Loss: 2.2091, Validation Loss: 2.2198\n",
      "Iteration 543900, Train Loss: 2.1276, Validation Loss: 2.5011\n",
      "Iteration 544000, Train Loss: 2.0979, Validation Loss: 2.4057\n",
      "Iteration 544100, Train Loss: 2.1879, Validation Loss: 2.1640\n",
      "Iteration 544200, Train Loss: 2.3983, Validation Loss: 2.1553\n",
      "Iteration 544300, Train Loss: 2.0194, Validation Loss: 1.9099\n",
      "Iteration 544400, Train Loss: 2.2011, Validation Loss: 2.4245\n",
      "Iteration 544500, Train Loss: 2.0742, Validation Loss: 2.1100\n",
      "Iteration 544600, Train Loss: 2.3111, Validation Loss: 3.1972\n",
      "Iteration 544700, Train Loss: 1.9910, Validation Loss: 2.2747\n",
      "Iteration 544800, Train Loss: 2.3164, Validation Loss: 2.4041\n",
      "Iteration 544900, Train Loss: 2.5416, Validation Loss: 1.9933\n",
      "Iteration 545000, Train Loss: 1.9898, Validation Loss: 2.1577\n",
      "Iteration 545100, Train Loss: 2.2511, Validation Loss: 1.9995\n",
      "Iteration 545200, Train Loss: 2.3105, Validation Loss: 2.3004\n",
      "Iteration 545300, Train Loss: 2.2163, Validation Loss: 2.2458\n",
      "Iteration 545400, Train Loss: 2.3155, Validation Loss: 2.3695\n",
      "Iteration 545500, Train Loss: 2.3988, Validation Loss: 2.5673\n",
      "Iteration 545600, Train Loss: 2.6140, Validation Loss: 2.6980\n",
      "Iteration 545700, Train Loss: 2.2217, Validation Loss: 2.0912\n",
      "Iteration 545800, Train Loss: 2.2981, Validation Loss: 1.8939\n",
      "Iteration 545900, Train Loss: 2.0489, Validation Loss: 2.4098\n",
      "Iteration 546000, Train Loss: 2.0164, Validation Loss: 2.0219\n",
      "Iteration 546100, Train Loss: 1.9172, Validation Loss: 2.2033\n",
      "Iteration 546200, Train Loss: 2.1479, Validation Loss: 2.4042\n",
      "Iteration 546300, Train Loss: 1.9870, Validation Loss: 2.1815\n",
      "Iteration 546400, Train Loss: 2.5173, Validation Loss: 1.8642\n",
      "Iteration 546500, Train Loss: 2.2727, Validation Loss: 2.5170\n",
      "Iteration 546600, Train Loss: 2.6267, Validation Loss: 2.4774\n",
      "Iteration 546700, Train Loss: 2.5027, Validation Loss: 2.1442\n",
      "Iteration 546800, Train Loss: 1.9991, Validation Loss: 2.4364\n",
      "Iteration 546900, Train Loss: 2.6323, Validation Loss: 2.6009\n",
      "Iteration 547000, Train Loss: 2.0567, Validation Loss: 2.0591\n",
      "Iteration 547100, Train Loss: 2.5811, Validation Loss: 2.2714\n",
      "Iteration 547200, Train Loss: 2.3977, Validation Loss: 2.2757\n",
      "Iteration 547300, Train Loss: 2.2903, Validation Loss: 2.6623\n",
      "Iteration 547400, Train Loss: 2.4960, Validation Loss: 1.9850\n",
      "Iteration 547500, Train Loss: 1.9475, Validation Loss: 2.1891\n",
      "Iteration 547600, Train Loss: 1.5631, Validation Loss: 2.3396\n",
      "Iteration 547700, Train Loss: 2.0465, Validation Loss: 2.1774\n",
      "Iteration 547800, Train Loss: 2.1069, Validation Loss: 2.4902\n",
      "Iteration 547900, Train Loss: 2.3147, Validation Loss: 2.4332\n",
      "Iteration 548000, Train Loss: 1.7810, Validation Loss: 2.4841\n",
      "Iteration 548100, Train Loss: 2.4890, Validation Loss: 2.2208\n",
      "Iteration 548200, Train Loss: 2.2622, Validation Loss: 2.2702\n",
      "Iteration 548300, Train Loss: 2.5287, Validation Loss: 2.3555\n",
      "Iteration 548400, Train Loss: 2.1283, Validation Loss: 2.3207\n",
      "Iteration 548500, Train Loss: 2.2386, Validation Loss: 2.5951\n",
      "Iteration 548600, Train Loss: 2.5576, Validation Loss: 2.2094\n",
      "Iteration 548700, Train Loss: 2.5043, Validation Loss: 2.5214\n",
      "Iteration 548800, Train Loss: 2.5034, Validation Loss: 2.2193\n",
      "Iteration 548900, Train Loss: 2.3759, Validation Loss: 2.1534\n",
      "Iteration 549000, Train Loss: 2.2809, Validation Loss: 2.3526\n",
      "Iteration 549100, Train Loss: 1.9700, Validation Loss: 2.4817\n",
      "Iteration 549200, Train Loss: 2.2368, Validation Loss: 2.4054\n",
      "Iteration 549300, Train Loss: 2.7364, Validation Loss: 1.9800\n",
      "Iteration 549400, Train Loss: 1.9984, Validation Loss: 1.9045\n",
      "Iteration 549500, Train Loss: 2.3375, Validation Loss: 2.2486\n",
      "Iteration 549600, Train Loss: 2.4793, Validation Loss: 2.1637\n",
      "Iteration 549700, Train Loss: 2.7617, Validation Loss: 2.4807\n",
      "Iteration 549800, Train Loss: 1.8471, Validation Loss: 2.4663\n",
      "Iteration 549900, Train Loss: 2.2845, Validation Loss: 2.1767\n",
      "Iteration 550000, Train Loss: 2.3854, Validation Loss: 2.2500\n",
      "Iteration 550100, Train Loss: 1.9626, Validation Loss: 1.9353\n",
      "Iteration 550200, Train Loss: 2.0363, Validation Loss: 1.9190\n",
      "Iteration 550300, Train Loss: 2.3567, Validation Loss: 2.0588\n",
      "Iteration 550400, Train Loss: 2.1139, Validation Loss: 2.4346\n",
      "Iteration 550500, Train Loss: 1.8062, Validation Loss: 2.0808\n",
      "Iteration 550600, Train Loss: 2.1566, Validation Loss: 2.5409\n",
      "Iteration 550700, Train Loss: 2.1651, Validation Loss: 1.8143\n",
      "Iteration 550800, Train Loss: 2.1738, Validation Loss: 2.5537\n",
      "Iteration 550900, Train Loss: 1.7649, Validation Loss: 2.4292\n",
      "Iteration 551000, Train Loss: 2.4736, Validation Loss: 2.0564\n",
      "Iteration 551100, Train Loss: 2.2116, Validation Loss: 2.0767\n",
      "Iteration 551200, Train Loss: 2.3997, Validation Loss: 2.1866\n",
      "Iteration 551300, Train Loss: 2.3956, Validation Loss: 2.1834\n",
      "Iteration 551400, Train Loss: 2.0482, Validation Loss: 2.2582\n",
      "Iteration 551500, Train Loss: 2.1601, Validation Loss: 2.0792\n",
      "Iteration 551600, Train Loss: 2.0554, Validation Loss: 2.0091\n",
      "Iteration 551700, Train Loss: 2.1330, Validation Loss: 2.1163\n",
      "Iteration 551800, Train Loss: 2.4654, Validation Loss: 2.1188\n",
      "Iteration 551900, Train Loss: 2.1670, Validation Loss: 2.6853\n",
      "Iteration 552000, Train Loss: 1.9835, Validation Loss: 2.2458\n",
      "Iteration 552100, Train Loss: 2.1304, Validation Loss: 2.4535\n",
      "Iteration 552200, Train Loss: 2.4403, Validation Loss: 2.2976\n",
      "Iteration 552300, Train Loss: 1.9853, Validation Loss: 2.6537\n",
      "Iteration 552400, Train Loss: 2.2312, Validation Loss: 1.8895\n",
      "Iteration 552500, Train Loss: 1.9390, Validation Loss: 2.2644\n",
      "Iteration 552600, Train Loss: 2.0084, Validation Loss: 1.9904\n",
      "Iteration 552700, Train Loss: 2.2229, Validation Loss: 2.3124\n",
      "Iteration 552800, Train Loss: 2.1412, Validation Loss: 2.3835\n",
      "Iteration 552900, Train Loss: 2.7649, Validation Loss: 2.3816\n",
      "Iteration 553000, Train Loss: 2.3389, Validation Loss: 2.2080\n",
      "Iteration 553100, Train Loss: 2.2986, Validation Loss: 2.5130\n",
      "Iteration 553200, Train Loss: 2.1358, Validation Loss: 2.5876\n",
      "Iteration 553300, Train Loss: 2.1969, Validation Loss: 2.3425\n",
      "Iteration 553400, Train Loss: 2.1657, Validation Loss: 2.6655\n",
      "Iteration 553500, Train Loss: 2.4338, Validation Loss: 2.6117\n",
      "Iteration 553600, Train Loss: 2.2577, Validation Loss: 1.9548\n",
      "Iteration 553700, Train Loss: 2.4563, Validation Loss: 2.3066\n",
      "Iteration 553800, Train Loss: 2.3649, Validation Loss: 2.2208\n",
      "Iteration 553900, Train Loss: 2.6448, Validation Loss: 2.1254\n",
      "Iteration 554000, Train Loss: 1.7842, Validation Loss: 2.4231\n",
      "Iteration 554100, Train Loss: 2.1769, Validation Loss: 3.0812\n",
      "Iteration 554200, Train Loss: 2.1380, Validation Loss: 2.5510\n",
      "Iteration 554300, Train Loss: 2.7703, Validation Loss: 2.1833\n",
      "Iteration 554400, Train Loss: 2.0077, Validation Loss: 1.8304\n",
      "Iteration 554500, Train Loss: 1.9707, Validation Loss: 2.0535\n",
      "Iteration 554600, Train Loss: 2.8789, Validation Loss: 2.2458\n",
      "Iteration 554700, Train Loss: 1.9363, Validation Loss: 1.9280\n",
      "Iteration 554800, Train Loss: 2.4248, Validation Loss: 2.2565\n",
      "Iteration 554900, Train Loss: 2.6709, Validation Loss: 2.3031\n",
      "Iteration 555000, Train Loss: 2.4090, Validation Loss: 2.8023\n",
      "Iteration 555100, Train Loss: 2.5473, Validation Loss: 2.4308\n",
      "Iteration 555200, Train Loss: 2.2110, Validation Loss: 2.3816\n",
      "Iteration 555300, Train Loss: 2.4704, Validation Loss: 2.0750\n",
      "Iteration 555400, Train Loss: 2.3712, Validation Loss: 1.9919\n",
      "Iteration 555500, Train Loss: 2.0957, Validation Loss: 2.6096\n",
      "Iteration 555600, Train Loss: 1.9106, Validation Loss: 2.3244\n",
      "Iteration 555700, Train Loss: 2.2067, Validation Loss: 1.9894\n",
      "Iteration 555800, Train Loss: 2.2468, Validation Loss: 2.3303\n",
      "Iteration 555900, Train Loss: 2.3259, Validation Loss: 2.1364\n",
      "Iteration 556000, Train Loss: 2.5054, Validation Loss: 2.6316\n",
      "Iteration 556100, Train Loss: 2.2643, Validation Loss: 2.1680\n",
      "Iteration 556200, Train Loss: 2.5305, Validation Loss: 2.7477\n",
      "Iteration 556300, Train Loss: 2.2284, Validation Loss: 2.7618\n",
      "Iteration 556400, Train Loss: 2.7454, Validation Loss: 1.8047\n",
      "Iteration 556500, Train Loss: 2.0702, Validation Loss: 2.7258\n",
      "Iteration 556600, Train Loss: 2.8160, Validation Loss: 2.6912\n",
      "Iteration 556700, Train Loss: 2.2950, Validation Loss: 2.3014\n",
      "Iteration 556800, Train Loss: 2.0393, Validation Loss: 2.0829\n",
      "Iteration 556900, Train Loss: 2.1732, Validation Loss: 2.4577\n",
      "Iteration 557000, Train Loss: 2.2245, Validation Loss: 2.3266\n",
      "Iteration 557100, Train Loss: 1.9436, Validation Loss: 2.3168\n",
      "Iteration 557200, Train Loss: 2.2151, Validation Loss: 2.6158\n",
      "Iteration 557300, Train Loss: 2.4257, Validation Loss: 2.0005\n",
      "Iteration 557400, Train Loss: 2.4533, Validation Loss: 2.1398\n",
      "Iteration 557500, Train Loss: 2.3903, Validation Loss: 2.5808\n",
      "Iteration 557600, Train Loss: 2.3057, Validation Loss: 2.3611\n",
      "Iteration 557700, Train Loss: 2.3854, Validation Loss: 2.6198\n",
      "Iteration 557800, Train Loss: 2.2160, Validation Loss: 2.5977\n",
      "Iteration 557900, Train Loss: 2.3595, Validation Loss: 2.4388\n",
      "Iteration 558000, Train Loss: 2.3435, Validation Loss: 2.3645\n",
      "Iteration 558100, Train Loss: 2.1404, Validation Loss: 2.5647\n",
      "Iteration 558200, Train Loss: 1.8511, Validation Loss: 2.0521\n",
      "Iteration 558300, Train Loss: 2.2471, Validation Loss: 2.5299\n",
      "Iteration 558400, Train Loss: 2.2203, Validation Loss: 1.9746\n",
      "Iteration 558500, Train Loss: 2.2547, Validation Loss: 2.6363\n",
      "Iteration 558600, Train Loss: 2.0663, Validation Loss: 2.3058\n",
      "Iteration 558700, Train Loss: 2.0973, Validation Loss: 2.4699\n",
      "Iteration 558800, Train Loss: 2.4882, Validation Loss: 2.3014\n",
      "Iteration 558900, Train Loss: 2.4108, Validation Loss: 2.2092\n",
      "Iteration 559000, Train Loss: 2.6070, Validation Loss: 2.6440\n",
      "Iteration 559100, Train Loss: 2.0827, Validation Loss: 2.1317\n",
      "Iteration 559200, Train Loss: 1.9903, Validation Loss: 1.6826\n",
      "Iteration 559300, Train Loss: 2.3970, Validation Loss: 2.5739\n",
      "Iteration 559400, Train Loss: 2.4873, Validation Loss: 2.6470\n",
      "Iteration 559500, Train Loss: 2.2173, Validation Loss: 2.3685\n",
      "Iteration 559600, Train Loss: 2.9525, Validation Loss: 2.1197\n",
      "Iteration 559700, Train Loss: 2.6516, Validation Loss: 2.3875\n",
      "Iteration 559800, Train Loss: 1.8374, Validation Loss: 2.0834\n",
      "Iteration 559900, Train Loss: 2.0922, Validation Loss: 2.4309\n",
      "Iteration 560000, Train Loss: 2.2327, Validation Loss: 1.8867\n",
      "Iteration 560100, Train Loss: 2.4119, Validation Loss: 2.3338\n",
      "Iteration 560200, Train Loss: 2.2396, Validation Loss: 2.2212\n",
      "Iteration 560300, Train Loss: 2.5149, Validation Loss: 2.6762\n",
      "Iteration 560400, Train Loss: 2.2438, Validation Loss: 1.8798\n",
      "Iteration 560500, Train Loss: 2.0569, Validation Loss: 2.0002\n",
      "Iteration 560600, Train Loss: 2.6206, Validation Loss: 1.8801\n",
      "Iteration 560700, Train Loss: 2.0510, Validation Loss: 2.2079\n",
      "Iteration 560800, Train Loss: 2.6730, Validation Loss: 2.8491\n",
      "Iteration 560900, Train Loss: 2.2645, Validation Loss: 2.5061\n",
      "Iteration 561000, Train Loss: 2.1362, Validation Loss: 2.1791\n",
      "Iteration 561100, Train Loss: 2.3056, Validation Loss: 1.8700\n",
      "Iteration 561200, Train Loss: 2.2729, Validation Loss: 2.4241\n",
      "Iteration 561300, Train Loss: 2.1883, Validation Loss: 2.5868\n",
      "Iteration 561400, Train Loss: 2.1823, Validation Loss: 2.3417\n",
      "Iteration 561500, Train Loss: 2.1821, Validation Loss: 2.1822\n",
      "Iteration 561600, Train Loss: 2.6933, Validation Loss: 2.1867\n",
      "Iteration 561700, Train Loss: 2.1139, Validation Loss: 2.3412\n",
      "Iteration 561800, Train Loss: 2.2920, Validation Loss: 1.5529\n",
      "Iteration 561900, Train Loss: 2.0472, Validation Loss: 2.3820\n",
      "Iteration 562000, Train Loss: 2.0048, Validation Loss: 2.5804\n",
      "Iteration 562100, Train Loss: 2.2887, Validation Loss: 2.5341\n",
      "Iteration 562200, Train Loss: 2.0769, Validation Loss: 1.9824\n",
      "Iteration 562300, Train Loss: 2.4318, Validation Loss: 2.5944\n",
      "Iteration 562400, Train Loss: 1.8842, Validation Loss: 2.2625\n",
      "Iteration 562500, Train Loss: 2.5128, Validation Loss: 2.0497\n",
      "Iteration 562600, Train Loss: 2.4316, Validation Loss: 2.5514\n",
      "Iteration 562700, Train Loss: 2.1513, Validation Loss: 2.5569\n",
      "Iteration 562800, Train Loss: 2.2142, Validation Loss: 2.3031\n",
      "Iteration 562900, Train Loss: 2.3378, Validation Loss: 2.6761\n",
      "Iteration 563000, Train Loss: 1.8376, Validation Loss: 2.5711\n",
      "Iteration 563100, Train Loss: 2.3729, Validation Loss: 2.7225\n",
      "Iteration 563200, Train Loss: 2.2353, Validation Loss: 2.6112\n",
      "Iteration 563300, Train Loss: 2.1040, Validation Loss: 2.3037\n",
      "Iteration 563400, Train Loss: 2.2208, Validation Loss: 2.7516\n",
      "Iteration 563500, Train Loss: 1.8675, Validation Loss: 2.4737\n",
      "Iteration 563600, Train Loss: 2.4171, Validation Loss: 1.8909\n",
      "Iteration 563700, Train Loss: 2.0327, Validation Loss: 2.6090\n",
      "Iteration 563800, Train Loss: 2.2141, Validation Loss: 1.8971\n",
      "Iteration 563900, Train Loss: 2.3942, Validation Loss: 2.1332\n",
      "Iteration 564000, Train Loss: 2.1527, Validation Loss: 2.4218\n",
      "Iteration 564100, Train Loss: 2.2794, Validation Loss: 2.4027\n",
      "Iteration 564200, Train Loss: 2.6052, Validation Loss: 2.4231\n",
      "Iteration 564300, Train Loss: 2.6002, Validation Loss: 2.1029\n",
      "Iteration 564400, Train Loss: 2.6865, Validation Loss: 2.1997\n",
      "Iteration 564500, Train Loss: 2.1835, Validation Loss: 1.9491\n",
      "Iteration 564600, Train Loss: 2.0355, Validation Loss: 2.2843\n",
      "Iteration 564700, Train Loss: 2.4822, Validation Loss: 2.2887\n",
      "Iteration 564800, Train Loss: 2.3131, Validation Loss: 2.1843\n",
      "Iteration 564900, Train Loss: 2.8010, Validation Loss: 2.1133\n",
      "Iteration 565000, Train Loss: 2.3348, Validation Loss: 2.5188\n",
      "Iteration 565100, Train Loss: 2.0369, Validation Loss: 2.6862\n",
      "Iteration 565200, Train Loss: 2.0137, Validation Loss: 2.2610\n",
      "Iteration 565300, Train Loss: 2.1127, Validation Loss: 2.6347\n",
      "Iteration 565400, Train Loss: 1.9418, Validation Loss: 2.3879\n",
      "Iteration 565500, Train Loss: 2.4925, Validation Loss: 2.4714\n",
      "Iteration 565600, Train Loss: 2.1771, Validation Loss: 2.1936\n",
      "Iteration 565700, Train Loss: 2.4439, Validation Loss: 2.1398\n",
      "Iteration 565800, Train Loss: 2.3517, Validation Loss: 1.9505\n",
      "Iteration 565900, Train Loss: 1.8040, Validation Loss: 2.3698\n",
      "Iteration 566000, Train Loss: 2.5158, Validation Loss: 2.5690\n",
      "Iteration 566100, Train Loss: 2.0256, Validation Loss: 2.4941\n",
      "Iteration 566200, Train Loss: 2.3579, Validation Loss: 1.7798\n",
      "Iteration 566300, Train Loss: 2.1046, Validation Loss: 2.1411\n",
      "Iteration 566400, Train Loss: 2.1622, Validation Loss: 2.0852\n",
      "Iteration 566500, Train Loss: 2.2171, Validation Loss: 1.9894\n",
      "Iteration 566600, Train Loss: 2.5540, Validation Loss: 2.1725\n",
      "Iteration 566700, Train Loss: 1.8255, Validation Loss: 2.3070\n",
      "Iteration 566800, Train Loss: 2.4410, Validation Loss: 2.2753\n",
      "Iteration 566900, Train Loss: 2.6799, Validation Loss: 2.3783\n",
      "Iteration 567000, Train Loss: 2.1975, Validation Loss: 2.6902\n",
      "Iteration 567100, Train Loss: 2.1737, Validation Loss: 2.3700\n",
      "Iteration 567200, Train Loss: 2.2688, Validation Loss: 2.2484\n",
      "Iteration 567300, Train Loss: 2.3166, Validation Loss: 2.4594\n",
      "Iteration 567400, Train Loss: 2.4762, Validation Loss: 1.8770\n",
      "Iteration 567500, Train Loss: 2.2089, Validation Loss: 2.7833\n",
      "Iteration 567600, Train Loss: 1.8978, Validation Loss: 2.5458\n",
      "Iteration 567700, Train Loss: 2.1391, Validation Loss: 2.2219\n",
      "Iteration 567800, Train Loss: 2.5605, Validation Loss: 2.1541\n",
      "Iteration 567900, Train Loss: 2.3151, Validation Loss: 2.5399\n",
      "Iteration 568000, Train Loss: 2.2037, Validation Loss: 2.0512\n",
      "Iteration 568100, Train Loss: 2.3070, Validation Loss: 2.2427\n",
      "Iteration 568200, Train Loss: 2.5005, Validation Loss: 2.2360\n",
      "Iteration 568300, Train Loss: 2.4600, Validation Loss: 2.3420\n",
      "Iteration 568400, Train Loss: 2.3247, Validation Loss: 2.1275\n",
      "Iteration 568500, Train Loss: 2.1119, Validation Loss: 1.7212\n",
      "Iteration 568600, Train Loss: 2.0148, Validation Loss: 2.5134\n",
      "Iteration 568700, Train Loss: 2.2666, Validation Loss: 1.8478\n",
      "Iteration 568800, Train Loss: 2.6558, Validation Loss: 1.9589\n",
      "Iteration 568900, Train Loss: 2.2043, Validation Loss: 1.9859\n",
      "Iteration 569000, Train Loss: 2.6067, Validation Loss: 2.0855\n",
      "Iteration 569100, Train Loss: 2.4662, Validation Loss: 2.4866\n",
      "Iteration 569200, Train Loss: 2.3400, Validation Loss: 2.4262\n",
      "Iteration 569300, Train Loss: 2.3330, Validation Loss: 2.2369\n",
      "Iteration 569400, Train Loss: 2.1770, Validation Loss: 2.5272\n",
      "Iteration 569500, Train Loss: 2.2543, Validation Loss: 2.1195\n",
      "Iteration 569600, Train Loss: 2.4081, Validation Loss: 2.0688\n",
      "Iteration 569700, Train Loss: 2.2878, Validation Loss: 2.0640\n",
      "Iteration 569800, Train Loss: 2.2411, Validation Loss: 2.7269\n",
      "Iteration 569900, Train Loss: 2.2595, Validation Loss: 2.1512\n",
      "Iteration 570000, Train Loss: 2.0357, Validation Loss: 2.4458\n",
      "Iteration 570100, Train Loss: 2.2191, Validation Loss: 2.3096\n",
      "Iteration 570200, Train Loss: 2.5412, Validation Loss: 2.3343\n",
      "Iteration 570300, Train Loss: 2.3402, Validation Loss: 2.4088\n",
      "Iteration 570400, Train Loss: 2.0307, Validation Loss: 2.5141\n",
      "Iteration 570500, Train Loss: 2.4737, Validation Loss: 2.3649\n",
      "Iteration 570600, Train Loss: 1.8932, Validation Loss: 2.4660\n",
      "Iteration 570700, Train Loss: 2.4214, Validation Loss: 2.1576\n",
      "Iteration 570800, Train Loss: 2.0973, Validation Loss: 2.4451\n",
      "Iteration 570900, Train Loss: 2.3372, Validation Loss: 1.9795\n",
      "Iteration 571000, Train Loss: 2.1385, Validation Loss: 2.4873\n",
      "Iteration 571100, Train Loss: 1.7225, Validation Loss: 1.9740\n",
      "Iteration 571200, Train Loss: 2.6957, Validation Loss: 1.9029\n",
      "Iteration 571300, Train Loss: 2.7476, Validation Loss: 2.0786\n",
      "Iteration 571400, Train Loss: 1.9316, Validation Loss: 1.9666\n",
      "Iteration 571500, Train Loss: 1.8656, Validation Loss: 2.1937\n",
      "Iteration 571600, Train Loss: 2.0348, Validation Loss: 2.2196\n",
      "Iteration 571700, Train Loss: 2.3801, Validation Loss: 2.3534\n",
      "Iteration 571800, Train Loss: 2.5883, Validation Loss: 2.0162\n",
      "Iteration 571900, Train Loss: 2.0196, Validation Loss: 2.2846\n",
      "Iteration 572000, Train Loss: 2.0731, Validation Loss: 2.3192\n",
      "Iteration 572100, Train Loss: 2.0160, Validation Loss: 2.3118\n",
      "Iteration 572200, Train Loss: 2.0369, Validation Loss: 2.5167\n",
      "Iteration 572300, Train Loss: 2.2082, Validation Loss: 2.2289\n",
      "Iteration 572400, Train Loss: 2.7514, Validation Loss: 2.3604\n",
      "Iteration 572500, Train Loss: 1.9810, Validation Loss: 2.3161\n",
      "Iteration 572600, Train Loss: 2.4802, Validation Loss: 2.0975\n",
      "Iteration 572700, Train Loss: 2.1744, Validation Loss: 2.7991\n",
      "Iteration 572800, Train Loss: 1.9618, Validation Loss: 2.5896\n",
      "Iteration 572900, Train Loss: 2.2152, Validation Loss: 2.8509\n",
      "Iteration 573000, Train Loss: 2.5029, Validation Loss: 2.3364\n",
      "Iteration 573100, Train Loss: 2.2925, Validation Loss: 2.2803\n",
      "Iteration 573200, Train Loss: 2.2038, Validation Loss: 2.3731\n",
      "Iteration 573300, Train Loss: 2.0329, Validation Loss: 2.4426\n",
      "Iteration 573400, Train Loss: 2.0827, Validation Loss: 2.6952\n",
      "Iteration 573500, Train Loss: 2.2517, Validation Loss: 2.2561\n",
      "Iteration 573600, Train Loss: 2.2749, Validation Loss: 2.2635\n",
      "Iteration 573700, Train Loss: 2.5596, Validation Loss: 2.2773\n",
      "Iteration 573800, Train Loss: 2.2756, Validation Loss: 2.2431\n",
      "Iteration 573900, Train Loss: 2.4527, Validation Loss: 2.3268\n",
      "Iteration 574000, Train Loss: 2.4362, Validation Loss: 1.9122\n",
      "Iteration 574100, Train Loss: 2.3883, Validation Loss: 2.1997\n",
      "Iteration 574200, Train Loss: 2.0646, Validation Loss: 2.4298\n",
      "Iteration 574300, Train Loss: 2.4260, Validation Loss: 2.0946\n",
      "Iteration 574400, Train Loss: 2.2134, Validation Loss: 2.1114\n",
      "Iteration 574500, Train Loss: 1.8046, Validation Loss: 2.2133\n",
      "Iteration 574600, Train Loss: 2.3386, Validation Loss: 2.3725\n",
      "Iteration 574700, Train Loss: 2.1966, Validation Loss: 2.2377\n",
      "Iteration 574800, Train Loss: 2.3262, Validation Loss: 2.4378\n",
      "Iteration 574900, Train Loss: 2.1233, Validation Loss: 2.1983\n",
      "Iteration 575000, Train Loss: 2.5874, Validation Loss: 2.0650\n",
      "Iteration 575100, Train Loss: 2.1945, Validation Loss: 2.5909\n",
      "Iteration 575200, Train Loss: 2.7999, Validation Loss: 2.4961\n",
      "Iteration 575300, Train Loss: 2.0730, Validation Loss: 2.2502\n",
      "Iteration 575400, Train Loss: 2.1918, Validation Loss: 2.3168\n",
      "Iteration 575500, Train Loss: 2.3487, Validation Loss: 2.4080\n",
      "Iteration 575600, Train Loss: 2.3876, Validation Loss: 2.0655\n",
      "Iteration 575700, Train Loss: 2.2151, Validation Loss: 2.4288\n",
      "Iteration 575800, Train Loss: 2.0803, Validation Loss: 2.1956\n",
      "Iteration 575900, Train Loss: 2.5432, Validation Loss: 2.0383\n",
      "Iteration 576000, Train Loss: 2.4159, Validation Loss: 2.1752\n",
      "Iteration 576100, Train Loss: 2.7667, Validation Loss: 2.3106\n",
      "Iteration 576200, Train Loss: 2.5418, Validation Loss: 2.7286\n",
      "Iteration 576300, Train Loss: 2.0464, Validation Loss: 2.1225\n",
      "Iteration 576400, Train Loss: 2.0677, Validation Loss: 2.4308\n",
      "Iteration 576500, Train Loss: 2.4030, Validation Loss: 2.1902\n",
      "Iteration 576600, Train Loss: 2.1359, Validation Loss: 2.1547\n",
      "Iteration 576700, Train Loss: 2.5671, Validation Loss: 2.2204\n",
      "Iteration 576800, Train Loss: 2.1811, Validation Loss: 2.4769\n",
      "Iteration 576900, Train Loss: 2.2501, Validation Loss: 2.2582\n",
      "Iteration 577000, Train Loss: 1.7881, Validation Loss: 1.9711\n",
      "Iteration 577100, Train Loss: 2.6919, Validation Loss: 2.0757\n",
      "Iteration 577200, Train Loss: 2.0665, Validation Loss: 2.5368\n",
      "Iteration 577300, Train Loss: 2.2236, Validation Loss: 2.1795\n",
      "Iteration 577400, Train Loss: 2.1025, Validation Loss: 2.3632\n",
      "Iteration 577500, Train Loss: 2.0845, Validation Loss: 2.4691\n",
      "Iteration 577600, Train Loss: 2.1558, Validation Loss: 2.1832\n",
      "Iteration 577700, Train Loss: 2.1491, Validation Loss: 2.3228\n",
      "Iteration 577800, Train Loss: 2.3648, Validation Loss: 2.4110\n",
      "Iteration 577900, Train Loss: 2.3624, Validation Loss: 2.1695\n",
      "Iteration 578000, Train Loss: 2.3291, Validation Loss: 2.7705\n",
      "Iteration 578100, Train Loss: 2.1850, Validation Loss: 2.2820\n",
      "Iteration 578200, Train Loss: 2.2518, Validation Loss: 2.3544\n",
      "Iteration 578300, Train Loss: 1.8293, Validation Loss: 2.2871\n",
      "Iteration 578400, Train Loss: 2.3128, Validation Loss: 2.1263\n",
      "Iteration 578500, Train Loss: 2.4048, Validation Loss: 2.3521\n",
      "Iteration 578600, Train Loss: 2.2602, Validation Loss: 2.2699\n",
      "Iteration 578700, Train Loss: 2.1949, Validation Loss: 1.9901\n",
      "Iteration 578800, Train Loss: 2.1288, Validation Loss: 2.5559\n",
      "Iteration 578900, Train Loss: 2.3353, Validation Loss: 2.6080\n",
      "Iteration 579000, Train Loss: 2.1051, Validation Loss: 2.1885\n",
      "Iteration 579100, Train Loss: 2.3650, Validation Loss: 2.3566\n",
      "Iteration 579200, Train Loss: 1.9316, Validation Loss: 2.1018\n",
      "Iteration 579300, Train Loss: 2.3074, Validation Loss: 2.1027\n",
      "Iteration 579400, Train Loss: 2.6131, Validation Loss: 2.7024\n",
      "Iteration 579500, Train Loss: 2.3416, Validation Loss: 2.1486\n",
      "Iteration 579600, Train Loss: 2.1012, Validation Loss: 2.4693\n",
      "Iteration 579700, Train Loss: 1.9992, Validation Loss: 2.5297\n",
      "Iteration 579800, Train Loss: 2.1506, Validation Loss: 2.0455\n",
      "Iteration 579900, Train Loss: 2.2613, Validation Loss: 3.2340\n",
      "Iteration 580000, Train Loss: 2.2156, Validation Loss: 2.3660\n",
      "Iteration 580100, Train Loss: 2.3307, Validation Loss: 2.7194\n",
      "Iteration 580200, Train Loss: 2.2696, Validation Loss: 2.2166\n",
      "Iteration 580300, Train Loss: 2.5515, Validation Loss: 2.3404\n",
      "Iteration 580400, Train Loss: 1.8330, Validation Loss: 2.4334\n",
      "Iteration 580500, Train Loss: 2.2350, Validation Loss: 2.7652\n",
      "Iteration 580600, Train Loss: 2.0902, Validation Loss: 2.8997\n",
      "Iteration 580700, Train Loss: 2.6568, Validation Loss: 2.1056\n",
      "Iteration 580800, Train Loss: 2.3480, Validation Loss: 2.2385\n",
      "Iteration 580900, Train Loss: 2.6236, Validation Loss: 2.2020\n",
      "Iteration 581000, Train Loss: 2.1294, Validation Loss: 1.9577\n",
      "Iteration 581100, Train Loss: 2.2299, Validation Loss: 2.3938\n",
      "Iteration 581200, Train Loss: 2.4968, Validation Loss: 2.4157\n",
      "Iteration 581300, Train Loss: 2.1026, Validation Loss: 2.4404\n",
      "Iteration 581400, Train Loss: 2.2537, Validation Loss: 2.3074\n",
      "Iteration 581500, Train Loss: 2.3273, Validation Loss: 2.9711\n",
      "Iteration 581600, Train Loss: 2.7742, Validation Loss: 2.0605\n",
      "Iteration 581700, Train Loss: 2.2051, Validation Loss: 2.2056\n",
      "Iteration 581800, Train Loss: 2.0482, Validation Loss: 2.4405\n",
      "Iteration 581900, Train Loss: 2.0160, Validation Loss: 2.1902\n",
      "Iteration 582000, Train Loss: 2.2594, Validation Loss: 2.9340\n",
      "Iteration 582100, Train Loss: 2.0704, Validation Loss: 2.2853\n",
      "Iteration 582200, Train Loss: 1.9936, Validation Loss: 2.3198\n",
      "Iteration 582300, Train Loss: 2.5512, Validation Loss: 2.0505\n",
      "Iteration 582400, Train Loss: 2.2935, Validation Loss: 2.3438\n",
      "Iteration 582500, Train Loss: 2.0059, Validation Loss: 2.4832\n",
      "Iteration 582600, Train Loss: 2.4839, Validation Loss: 2.1204\n",
      "Iteration 582700, Train Loss: 2.1246, Validation Loss: 2.3646\n",
      "Iteration 582800, Train Loss: 2.2638, Validation Loss: 2.1906\n",
      "Iteration 582900, Train Loss: 2.5657, Validation Loss: 2.4787\n",
      "Iteration 583000, Train Loss: 2.2490, Validation Loss: 2.6065\n",
      "Iteration 583100, Train Loss: 1.8228, Validation Loss: 2.2208\n",
      "Iteration 583200, Train Loss: 2.1378, Validation Loss: 2.1792\n",
      "Iteration 583300, Train Loss: 2.3130, Validation Loss: 2.6042\n",
      "Iteration 583400, Train Loss: 2.5248, Validation Loss: 2.1928\n",
      "Iteration 583500, Train Loss: 2.2439, Validation Loss: 2.4417\n",
      "Iteration 583600, Train Loss: 2.5047, Validation Loss: 2.1917\n",
      "Iteration 583700, Train Loss: 2.4205, Validation Loss: 2.0957\n",
      "Iteration 583800, Train Loss: 2.0899, Validation Loss: 2.5571\n",
      "Iteration 583900, Train Loss: 2.3015, Validation Loss: 2.2874\n",
      "Iteration 584000, Train Loss: 2.1029, Validation Loss: 2.3742\n",
      "Iteration 584100, Train Loss: 1.9128, Validation Loss: 2.4782\n",
      "Iteration 584200, Train Loss: 2.1550, Validation Loss: 2.1604\n",
      "Iteration 584300, Train Loss: 2.4666, Validation Loss: 2.8350\n",
      "Iteration 584400, Train Loss: 2.3536, Validation Loss: 1.8387\n",
      "Iteration 584500, Train Loss: 2.4744, Validation Loss: 1.9755\n",
      "Iteration 584600, Train Loss: 2.4087, Validation Loss: 2.7967\n",
      "Iteration 584700, Train Loss: 1.9530, Validation Loss: 2.3025\n",
      "Iteration 584800, Train Loss: 2.2158, Validation Loss: 2.2289\n",
      "Iteration 584900, Train Loss: 2.3901, Validation Loss: 2.1296\n",
      "Iteration 585000, Train Loss: 2.0471, Validation Loss: 2.0260\n",
      "Iteration 585100, Train Loss: 2.2915, Validation Loss: 1.7562\n",
      "Iteration 585200, Train Loss: 1.9569, Validation Loss: 2.6326\n",
      "Iteration 585300, Train Loss: 2.0604, Validation Loss: 2.3049\n",
      "Iteration 585400, Train Loss: 2.5727, Validation Loss: 1.6165\n",
      "Iteration 585500, Train Loss: 2.3474, Validation Loss: 2.2144\n",
      "Iteration 585600, Train Loss: 2.1563, Validation Loss: 2.2808\n",
      "Iteration 585700, Train Loss: 2.2975, Validation Loss: 2.0022\n",
      "Iteration 585800, Train Loss: 2.0011, Validation Loss: 2.3301\n",
      "Iteration 585900, Train Loss: 2.5437, Validation Loss: 2.0096\n",
      "Iteration 586000, Train Loss: 1.8600, Validation Loss: 2.3328\n",
      "Iteration 586100, Train Loss: 2.2979, Validation Loss: 2.3147\n",
      "Iteration 586200, Train Loss: 2.1742, Validation Loss: 2.3218\n",
      "Iteration 586300, Train Loss: 1.9985, Validation Loss: 2.4708\n",
      "Iteration 586400, Train Loss: 2.2644, Validation Loss: 2.4235\n",
      "Iteration 586500, Train Loss: 2.2897, Validation Loss: 2.4372\n",
      "Iteration 586600, Train Loss: 2.0833, Validation Loss: 1.8907\n",
      "Iteration 586700, Train Loss: 2.7124, Validation Loss: 2.2370\n",
      "Iteration 586800, Train Loss: 2.3109, Validation Loss: 2.2823\n",
      "Iteration 586900, Train Loss: 2.4676, Validation Loss: 2.8552\n",
      "Iteration 587000, Train Loss: 2.2427, Validation Loss: 2.3313\n",
      "Iteration 587100, Train Loss: 2.5869, Validation Loss: 2.1449\n",
      "Iteration 587200, Train Loss: 2.3573, Validation Loss: 2.3780\n",
      "Iteration 587300, Train Loss: 2.0017, Validation Loss: 2.1127\n",
      "Iteration 587400, Train Loss: 2.4919, Validation Loss: 2.1888\n",
      "Iteration 587500, Train Loss: 1.8840, Validation Loss: 2.0263\n",
      "Iteration 587600, Train Loss: 1.9398, Validation Loss: 2.2491\n",
      "Iteration 587700, Train Loss: 1.9834, Validation Loss: 2.3706\n",
      "Iteration 587800, Train Loss: 2.5534, Validation Loss: 2.5176\n",
      "Iteration 587900, Train Loss: 2.3729, Validation Loss: 1.9301\n",
      "Iteration 588000, Train Loss: 2.1622, Validation Loss: 2.2403\n",
      "Iteration 588100, Train Loss: 2.5118, Validation Loss: 2.0668\n",
      "Iteration 588200, Train Loss: 2.4452, Validation Loss: 2.3110\n",
      "Iteration 588300, Train Loss: 2.2670, Validation Loss: 3.1309\n",
      "Iteration 588400, Train Loss: 2.1583, Validation Loss: 1.9878\n",
      "Iteration 588500, Train Loss: 2.7334, Validation Loss: 2.3226\n",
      "Iteration 588600, Train Loss: 2.3363, Validation Loss: 2.7465\n",
      "Iteration 588700, Train Loss: 2.1994, Validation Loss: 2.1795\n",
      "Iteration 588800, Train Loss: 1.9841, Validation Loss: 1.9040\n",
      "Iteration 588900, Train Loss: 2.6576, Validation Loss: 2.6078\n",
      "Iteration 589000, Train Loss: 2.2934, Validation Loss: 2.4208\n",
      "Iteration 589100, Train Loss: 2.2779, Validation Loss: 2.2171\n",
      "Iteration 589200, Train Loss: 2.3326, Validation Loss: 2.0928\n",
      "Iteration 589300, Train Loss: 2.0936, Validation Loss: 2.5095\n",
      "Iteration 589400, Train Loss: 2.7026, Validation Loss: 1.9331\n",
      "Iteration 589500, Train Loss: 2.3557, Validation Loss: 2.4330\n",
      "Iteration 589600, Train Loss: 2.0660, Validation Loss: 1.7155\n",
      "Iteration 589700, Train Loss: 2.7181, Validation Loss: 2.2891\n",
      "Iteration 589800, Train Loss: 2.5747, Validation Loss: 2.2935\n",
      "Iteration 589900, Train Loss: 2.2414, Validation Loss: 2.3418\n",
      "Iteration 590000, Train Loss: 2.2675, Validation Loss: 2.2461\n",
      "Iteration 590100, Train Loss: 2.2766, Validation Loss: 2.3999\n",
      "Iteration 590200, Train Loss: 2.0810, Validation Loss: 2.6561\n",
      "Iteration 590300, Train Loss: 2.3941, Validation Loss: 2.0091\n",
      "Iteration 590400, Train Loss: 2.2236, Validation Loss: 2.1511\n",
      "Iteration 590500, Train Loss: 2.1130, Validation Loss: 2.4893\n",
      "Iteration 590600, Train Loss: 2.0341, Validation Loss: 2.3550\n",
      "Iteration 590700, Train Loss: 2.2821, Validation Loss: 2.0527\n",
      "Iteration 590800, Train Loss: 2.2917, Validation Loss: 2.2504\n",
      "Iteration 590900, Train Loss: 2.4735, Validation Loss: 2.2265\n",
      "Iteration 591000, Train Loss: 2.5054, Validation Loss: 2.2944\n",
      "Iteration 591100, Train Loss: 1.9883, Validation Loss: 2.3991\n",
      "Iteration 591200, Train Loss: 2.3232, Validation Loss: 2.9612\n",
      "Iteration 591300, Train Loss: 2.6095, Validation Loss: 2.5626\n",
      "Iteration 591400, Train Loss: 1.8803, Validation Loss: 2.1661\n",
      "Iteration 591500, Train Loss: 2.6080, Validation Loss: 2.6101\n",
      "Iteration 591600, Train Loss: 2.0619, Validation Loss: 2.3176\n",
      "Iteration 591700, Train Loss: 2.1929, Validation Loss: 2.4190\n",
      "Iteration 591800, Train Loss: 2.2638, Validation Loss: 2.3094\n",
      "Iteration 591900, Train Loss: 2.0351, Validation Loss: 1.9796\n",
      "Iteration 592000, Train Loss: 2.2345, Validation Loss: 2.2294\n",
      "Iteration 592100, Train Loss: 1.9905, Validation Loss: 2.4919\n",
      "Iteration 592200, Train Loss: 2.1150, Validation Loss: 1.9732\n",
      "Iteration 592300, Train Loss: 2.2809, Validation Loss: 2.3780\n",
      "Iteration 592400, Train Loss: 1.8695, Validation Loss: 2.0609\n",
      "Iteration 592500, Train Loss: 2.5328, Validation Loss: 2.1084\n",
      "Iteration 592600, Train Loss: 2.5075, Validation Loss: 1.8622\n",
      "Iteration 592700, Train Loss: 2.2607, Validation Loss: 2.4683\n",
      "Iteration 592800, Train Loss: 2.3757, Validation Loss: 2.6353\n",
      "Iteration 592900, Train Loss: 1.9673, Validation Loss: 2.3436\n",
      "Iteration 593000, Train Loss: 2.0162, Validation Loss: 2.1508\n",
      "Iteration 593100, Train Loss: 2.0380, Validation Loss: 2.1938\n",
      "Iteration 593200, Train Loss: 1.5644, Validation Loss: 2.3930\n",
      "Iteration 593300, Train Loss: 2.3372, Validation Loss: 2.2351\n",
      "Iteration 593400, Train Loss: 1.9315, Validation Loss: 2.7550\n",
      "Iteration 593500, Train Loss: 2.2913, Validation Loss: 2.1831\n",
      "Iteration 593600, Train Loss: 1.9806, Validation Loss: 2.2021\n",
      "Iteration 593700, Train Loss: 2.0384, Validation Loss: 2.0218\n",
      "Iteration 593800, Train Loss: 2.1699, Validation Loss: 1.8506\n",
      "Iteration 593900, Train Loss: 2.1474, Validation Loss: 2.3663\n",
      "Iteration 594000, Train Loss: 2.4912, Validation Loss: 2.9609\n",
      "Iteration 594100, Train Loss: 2.1880, Validation Loss: 2.0454\n",
      "Iteration 594200, Train Loss: 2.3084, Validation Loss: 2.3677\n",
      "Iteration 594300, Train Loss: 2.5580, Validation Loss: 2.0154\n",
      "Iteration 594400, Train Loss: 2.5415, Validation Loss: 2.5176\n",
      "Iteration 594500, Train Loss: 2.2160, Validation Loss: 2.1856\n",
      "Iteration 594600, Train Loss: 1.9857, Validation Loss: 2.0691\n",
      "Iteration 594700, Train Loss: 2.6398, Validation Loss: 1.9498\n",
      "Iteration 594800, Train Loss: 2.4174, Validation Loss: 2.1250\n",
      "Iteration 594900, Train Loss: 2.3764, Validation Loss: 2.2078\n",
      "Iteration 595000, Train Loss: 1.8646, Validation Loss: 2.4891\n",
      "Iteration 595100, Train Loss: 2.2507, Validation Loss: 2.6091\n",
      "Iteration 595200, Train Loss: 2.2857, Validation Loss: 2.6742\n",
      "Iteration 595300, Train Loss: 2.0154, Validation Loss: 2.5154\n",
      "Iteration 595400, Train Loss: 2.0623, Validation Loss: 2.2798\n",
      "Iteration 595500, Train Loss: 2.5471, Validation Loss: 2.1961\n",
      "Iteration 595600, Train Loss: 2.2977, Validation Loss: 2.2152\n",
      "Iteration 595700, Train Loss: 2.2822, Validation Loss: 2.6907\n",
      "Iteration 595800, Train Loss: 2.0630, Validation Loss: 2.3781\n",
      "Iteration 595900, Train Loss: 2.8260, Validation Loss: 2.0728\n",
      "Iteration 596000, Train Loss: 2.1148, Validation Loss: 2.3772\n",
      "Iteration 596100, Train Loss: 1.6184, Validation Loss: 2.6202\n",
      "Iteration 596200, Train Loss: 1.9013, Validation Loss: 2.0567\n",
      "Iteration 596300, Train Loss: 2.0559, Validation Loss: 2.0758\n",
      "Iteration 596400, Train Loss: 2.6975, Validation Loss: 2.4559\n",
      "Iteration 596500, Train Loss: 2.4474, Validation Loss: 2.4452\n",
      "Iteration 596600, Train Loss: 1.9074, Validation Loss: 1.7630\n",
      "Iteration 596700, Train Loss: 2.0192, Validation Loss: 2.2847\n",
      "Iteration 596800, Train Loss: 2.3251, Validation Loss: 1.9036\n",
      "Iteration 596900, Train Loss: 2.1327, Validation Loss: 2.6449\n",
      "Iteration 597000, Train Loss: 2.2829, Validation Loss: 2.2155\n",
      "Iteration 597100, Train Loss: 2.4172, Validation Loss: 2.6675\n",
      "Iteration 597200, Train Loss: 2.4321, Validation Loss: 2.2512\n",
      "Iteration 597300, Train Loss: 2.2133, Validation Loss: 2.3432\n",
      "Iteration 597400, Train Loss: 2.3482, Validation Loss: 2.6791\n",
      "Iteration 597500, Train Loss: 2.1642, Validation Loss: 2.0666\n",
      "Iteration 597600, Train Loss: 2.4370, Validation Loss: 2.4009\n",
      "Iteration 597700, Train Loss: 2.3173, Validation Loss: 2.5136\n",
      "Iteration 597800, Train Loss: 2.1381, Validation Loss: 2.1229\n",
      "Iteration 597900, Train Loss: 2.6708, Validation Loss: 2.4245\n",
      "Iteration 598000, Train Loss: 1.8413, Validation Loss: 2.2305\n",
      "Iteration 598100, Train Loss: 2.5154, Validation Loss: 2.3687\n",
      "Iteration 598200, Train Loss: 2.2457, Validation Loss: 1.9934\n",
      "Iteration 598300, Train Loss: 2.0060, Validation Loss: 2.0509\n",
      "Iteration 598400, Train Loss: 2.2094, Validation Loss: 1.9083\n",
      "Iteration 598500, Train Loss: 2.6129, Validation Loss: 2.1917\n",
      "Iteration 598600, Train Loss: 2.2731, Validation Loss: 2.0073\n",
      "Iteration 598700, Train Loss: 2.4740, Validation Loss: 2.4455\n",
      "Iteration 598800, Train Loss: 2.4939, Validation Loss: 1.9358\n",
      "Iteration 598900, Train Loss: 1.9548, Validation Loss: 2.1047\n",
      "Iteration 599000, Train Loss: 2.4918, Validation Loss: 2.5320\n",
      "Iteration 599100, Train Loss: 1.7357, Validation Loss: 2.2732\n",
      "Iteration 599200, Train Loss: 2.6651, Validation Loss: 2.4267\n",
      "Iteration 599300, Train Loss: 2.3425, Validation Loss: 2.4691\n",
      "Iteration 599400, Train Loss: 1.8033, Validation Loss: 2.2769\n",
      "Iteration 599500, Train Loss: 2.1819, Validation Loss: 2.3347\n",
      "Iteration 599600, Train Loss: 2.4467, Validation Loss: 2.3889\n",
      "Iteration 599700, Train Loss: 2.1361, Validation Loss: 2.3298\n",
      "Iteration 599800, Train Loss: 1.9930, Validation Loss: 2.2690\n",
      "Iteration 599900, Train Loss: 2.6603, Validation Loss: 2.4491\n",
      "Iteration 600000, Train Loss: 2.0305, Validation Loss: 2.1873\n",
      "Iteration 600100, Train Loss: 2.1935, Validation Loss: 1.9645\n",
      "Iteration 600200, Train Loss: 2.8489, Validation Loss: 1.9254\n",
      "Iteration 600300, Train Loss: 2.8155, Validation Loss: 2.1616\n",
      "Iteration 600400, Train Loss: 2.7149, Validation Loss: 1.9842\n",
      "Iteration 600500, Train Loss: 2.2693, Validation Loss: 1.8381\n",
      "Iteration 600600, Train Loss: 2.1601, Validation Loss: 2.8014\n",
      "Iteration 600700, Train Loss: 2.3177, Validation Loss: 2.5406\n",
      "Iteration 600800, Train Loss: 2.0338, Validation Loss: 2.0188\n",
      "Iteration 600900, Train Loss: 1.8387, Validation Loss: 2.0245\n",
      "Iteration 601000, Train Loss: 2.7637, Validation Loss: 2.3636\n",
      "Iteration 601100, Train Loss: 2.4138, Validation Loss: 2.6967\n",
      "Iteration 601200, Train Loss: 2.2519, Validation Loss: 2.6134\n",
      "Iteration 601300, Train Loss: 2.1727, Validation Loss: 1.8686\n",
      "Iteration 601400, Train Loss: 2.1801, Validation Loss: 2.2241\n",
      "Iteration 601500, Train Loss: 2.2416, Validation Loss: 2.2588\n",
      "Iteration 601600, Train Loss: 1.9867, Validation Loss: 2.4086\n",
      "Iteration 601700, Train Loss: 2.5925, Validation Loss: 2.3249\n",
      "Iteration 601800, Train Loss: 1.8619, Validation Loss: 2.8763\n",
      "Iteration 601900, Train Loss: 2.1481, Validation Loss: 2.2558\n",
      "Iteration 602000, Train Loss: 2.3609, Validation Loss: 2.3702\n",
      "Iteration 602100, Train Loss: 2.2990, Validation Loss: 2.4850\n",
      "Iteration 602200, Train Loss: 2.1650, Validation Loss: 1.9545\n",
      "Iteration 602300, Train Loss: 1.7942, Validation Loss: 2.0938\n",
      "Iteration 602400, Train Loss: 2.1703, Validation Loss: 2.4579\n",
      "Iteration 602500, Train Loss: 2.5066, Validation Loss: 2.8333\n",
      "Iteration 602600, Train Loss: 1.7956, Validation Loss: 2.6334\n",
      "Iteration 602700, Train Loss: 2.3669, Validation Loss: 1.7795\n",
      "Iteration 602800, Train Loss: 2.1216, Validation Loss: 2.3255\n",
      "Iteration 602900, Train Loss: 2.2383, Validation Loss: 1.9260\n",
      "Iteration 603000, Train Loss: 2.0783, Validation Loss: 2.5791\n",
      "Iteration 603100, Train Loss: 2.3579, Validation Loss: 1.7230\n",
      "Iteration 603200, Train Loss: 2.2027, Validation Loss: 2.5528\n",
      "Iteration 603300, Train Loss: 1.9934, Validation Loss: 2.1223\n",
      "Iteration 603400, Train Loss: 2.3504, Validation Loss: 2.1328\n",
      "Iteration 603500, Train Loss: 2.2011, Validation Loss: 2.2688\n",
      "Iteration 603600, Train Loss: 2.6253, Validation Loss: 2.2038\n",
      "Iteration 603700, Train Loss: 2.4919, Validation Loss: 2.5901\n",
      "Iteration 603800, Train Loss: 2.3339, Validation Loss: 1.9382\n",
      "Iteration 603900, Train Loss: 2.0961, Validation Loss: 2.3187\n",
      "Iteration 604000, Train Loss: 2.2388, Validation Loss: 2.2035\n",
      "Iteration 604100, Train Loss: 2.0687, Validation Loss: 1.9868\n",
      "Iteration 604200, Train Loss: 2.2155, Validation Loss: 1.8705\n",
      "Iteration 604300, Train Loss: 2.2958, Validation Loss: 1.8911\n",
      "Iteration 604400, Train Loss: 2.1780, Validation Loss: 2.0786\n",
      "Iteration 604500, Train Loss: 2.1001, Validation Loss: 2.4392\n",
      "Iteration 604600, Train Loss: 1.9741, Validation Loss: 2.5697\n",
      "Iteration 604700, Train Loss: 2.5636, Validation Loss: 2.3283\n",
      "Iteration 604800, Train Loss: 1.8572, Validation Loss: 2.3020\n",
      "Iteration 604900, Train Loss: 2.2188, Validation Loss: 1.7711\n",
      "Iteration 605000, Train Loss: 2.6490, Validation Loss: 2.8547\n",
      "Iteration 605100, Train Loss: 2.3765, Validation Loss: 2.0314\n",
      "Iteration 605200, Train Loss: 2.2170, Validation Loss: 1.9598\n",
      "Iteration 605300, Train Loss: 2.5141, Validation Loss: 2.2536\n",
      "Iteration 605400, Train Loss: 2.0572, Validation Loss: 2.4777\n",
      "Iteration 605500, Train Loss: 1.7339, Validation Loss: 2.1759\n",
      "Iteration 605600, Train Loss: 2.0123, Validation Loss: 2.1912\n",
      "Iteration 605700, Train Loss: 2.6352, Validation Loss: 2.3082\n",
      "Iteration 605800, Train Loss: 2.4935, Validation Loss: 2.4017\n",
      "Iteration 605900, Train Loss: 2.2546, Validation Loss: 2.4547\n",
      "Iteration 606000, Train Loss: 2.0231, Validation Loss: 1.8869\n",
      "Iteration 606100, Train Loss: 2.2125, Validation Loss: 1.9666\n",
      "Iteration 606200, Train Loss: 1.8818, Validation Loss: 2.0862\n",
      "Iteration 606300, Train Loss: 2.0291, Validation Loss: 2.5759\n",
      "Iteration 606400, Train Loss: 2.1302, Validation Loss: 2.5255\n",
      "Iteration 606500, Train Loss: 1.9335, Validation Loss: 2.2548\n",
      "Iteration 606600, Train Loss: 2.0024, Validation Loss: 2.0903\n",
      "Iteration 606700, Train Loss: 2.5400, Validation Loss: 2.0372\n",
      "Iteration 606800, Train Loss: 2.2956, Validation Loss: 2.7492\n",
      "Iteration 606900, Train Loss: 2.3501, Validation Loss: 2.3009\n",
      "Iteration 607000, Train Loss: 2.3484, Validation Loss: 2.2616\n",
      "Iteration 607100, Train Loss: 2.3371, Validation Loss: 2.1791\n",
      "Iteration 607200, Train Loss: 2.2812, Validation Loss: 2.4841\n",
      "Iteration 607300, Train Loss: 2.2045, Validation Loss: 2.2950\n",
      "Iteration 607400, Train Loss: 2.3615, Validation Loss: 2.2607\n",
      "Iteration 607500, Train Loss: 2.5354, Validation Loss: 2.2608\n",
      "Iteration 607600, Train Loss: 2.1104, Validation Loss: 2.1567\n",
      "Iteration 607700, Train Loss: 2.0455, Validation Loss: 2.3054\n",
      "Iteration 607800, Train Loss: 2.1043, Validation Loss: 2.2390\n",
      "Iteration 607900, Train Loss: 2.2032, Validation Loss: 2.2388\n",
      "Iteration 608000, Train Loss: 2.1020, Validation Loss: 2.3563\n",
      "Iteration 608100, Train Loss: 2.0132, Validation Loss: 2.3921\n",
      "Iteration 608200, Train Loss: 2.2609, Validation Loss: 2.1915\n",
      "Iteration 608300, Train Loss: 1.9538, Validation Loss: 2.2909\n",
      "Iteration 608400, Train Loss: 2.7356, Validation Loss: 2.5180\n",
      "Iteration 608500, Train Loss: 2.6185, Validation Loss: 2.1864\n",
      "Iteration 608600, Train Loss: 2.1190, Validation Loss: 2.5173\n",
      "Iteration 608700, Train Loss: 2.2196, Validation Loss: 2.5853\n",
      "Iteration 608800, Train Loss: 2.3357, Validation Loss: 2.5363\n",
      "Iteration 608900, Train Loss: 2.5796, Validation Loss: 2.2881\n",
      "Iteration 609000, Train Loss: 2.1295, Validation Loss: 2.1835\n",
      "Iteration 609100, Train Loss: 2.3016, Validation Loss: 2.1359\n",
      "Iteration 609200, Train Loss: 1.8660, Validation Loss: 2.3036\n",
      "Iteration 609300, Train Loss: 2.6830, Validation Loss: 2.3159\n",
      "Iteration 609400, Train Loss: 2.2953, Validation Loss: 2.2160\n",
      "Iteration 609500, Train Loss: 1.8763, Validation Loss: 2.5447\n",
      "Iteration 609600, Train Loss: 2.3068, Validation Loss: 2.2835\n",
      "Iteration 609700, Train Loss: 2.4433, Validation Loss: 2.3397\n",
      "Iteration 609800, Train Loss: 2.4059, Validation Loss: 2.3280\n",
      "Iteration 609900, Train Loss: 2.4039, Validation Loss: 2.2708\n",
      "Iteration 610000, Train Loss: 1.9664, Validation Loss: 1.8778\n",
      "Iteration 610100, Train Loss: 2.1437, Validation Loss: 2.1325\n",
      "Iteration 610200, Train Loss: 2.3916, Validation Loss: 1.9753\n",
      "Iteration 610300, Train Loss: 2.3939, Validation Loss: 2.1446\n",
      "Iteration 610400, Train Loss: 2.0175, Validation Loss: 2.4578\n",
      "Iteration 610500, Train Loss: 2.0423, Validation Loss: 2.3132\n",
      "Iteration 610600, Train Loss: 1.9687, Validation Loss: 2.1697\n",
      "Iteration 610700, Train Loss: 2.2726, Validation Loss: 1.8859\n",
      "Iteration 610800, Train Loss: 2.2217, Validation Loss: 2.1829\n",
      "Iteration 610900, Train Loss: 2.4923, Validation Loss: 2.2020\n",
      "Iteration 611000, Train Loss: 2.4862, Validation Loss: 2.1557\n",
      "Iteration 611100, Train Loss: 2.2398, Validation Loss: 2.4098\n",
      "Iteration 611200, Train Loss: 2.2124, Validation Loss: 2.3030\n",
      "Iteration 611300, Train Loss: 2.3929, Validation Loss: 2.1034\n",
      "Iteration 611400, Train Loss: 1.5344, Validation Loss: 2.4347\n",
      "Iteration 611500, Train Loss: 2.3418, Validation Loss: 2.0287\n",
      "Iteration 611600, Train Loss: 2.3496, Validation Loss: 2.3672\n",
      "Iteration 611700, Train Loss: 2.0223, Validation Loss: 2.3321\n",
      "Iteration 611800, Train Loss: 2.7434, Validation Loss: 2.3734\n",
      "Iteration 611900, Train Loss: 2.2129, Validation Loss: 2.0405\n",
      "Iteration 612000, Train Loss: 2.3446, Validation Loss: 2.0351\n",
      "Iteration 612100, Train Loss: 2.2704, Validation Loss: 1.8569\n",
      "Iteration 612200, Train Loss: 2.0847, Validation Loss: 2.6611\n",
      "Iteration 612300, Train Loss: 2.0545, Validation Loss: 2.4090\n",
      "Iteration 612400, Train Loss: 2.1466, Validation Loss: 2.4133\n",
      "Iteration 612500, Train Loss: 2.2519, Validation Loss: 1.9309\n",
      "Iteration 612600, Train Loss: 2.4384, Validation Loss: 2.4758\n",
      "Iteration 612700, Train Loss: 2.0723, Validation Loss: 2.5009\n",
      "Iteration 612800, Train Loss: 1.9469, Validation Loss: 2.4245\n",
      "Iteration 612900, Train Loss: 2.3248, Validation Loss: 2.6169\n",
      "Iteration 613000, Train Loss: 2.5381, Validation Loss: 2.3369\n",
      "Iteration 613100, Train Loss: 2.6006, Validation Loss: 2.3354\n",
      "Iteration 613200, Train Loss: 2.4708, Validation Loss: 2.0190\n",
      "Iteration 613300, Train Loss: 2.5492, Validation Loss: 2.2929\n",
      "Iteration 613400, Train Loss: 2.2995, Validation Loss: 2.3827\n",
      "Iteration 613500, Train Loss: 2.4528, Validation Loss: 2.2463\n",
      "Iteration 613600, Train Loss: 2.0918, Validation Loss: 2.2040\n",
      "Iteration 613700, Train Loss: 1.7559, Validation Loss: 2.1143\n",
      "Iteration 613800, Train Loss: 2.0601, Validation Loss: 2.5339\n",
      "Iteration 613900, Train Loss: 2.0828, Validation Loss: 1.8968\n",
      "Iteration 614000, Train Loss: 2.0783, Validation Loss: 1.6950\n",
      "Iteration 614100, Train Loss: 2.2895, Validation Loss: 2.7142\n",
      "Iteration 614200, Train Loss: 2.7832, Validation Loss: 2.9691\n",
      "Iteration 614300, Train Loss: 2.2819, Validation Loss: 2.1502\n",
      "Iteration 614400, Train Loss: 2.3304, Validation Loss: 1.9770\n",
      "Iteration 614500, Train Loss: 2.2734, Validation Loss: 1.7954\n",
      "Iteration 614600, Train Loss: 2.1781, Validation Loss: 2.6847\n",
      "Iteration 614700, Train Loss: 1.7284, Validation Loss: 2.5109\n",
      "Iteration 614800, Train Loss: 2.3745, Validation Loss: 2.4581\n",
      "Iteration 614900, Train Loss: 2.5530, Validation Loss: 2.0201\n",
      "Iteration 615000, Train Loss: 2.4484, Validation Loss: 1.9536\n",
      "Iteration 615100, Train Loss: 2.0594, Validation Loss: 2.2843\n",
      "Iteration 615200, Train Loss: 2.0902, Validation Loss: 2.2875\n",
      "Iteration 615300, Train Loss: 2.3016, Validation Loss: 1.8499\n",
      "Iteration 615400, Train Loss: 2.3149, Validation Loss: 2.8466\n",
      "Iteration 615500, Train Loss: 2.6122, Validation Loss: 2.2622\n",
      "Iteration 615600, Train Loss: 2.1533, Validation Loss: 2.1782\n",
      "Iteration 615700, Train Loss: 2.3116, Validation Loss: 2.2322\n",
      "Iteration 615800, Train Loss: 2.3762, Validation Loss: 2.0938\n",
      "Iteration 615900, Train Loss: 2.7146, Validation Loss: 2.0843\n",
      "Iteration 616000, Train Loss: 2.0492, Validation Loss: 2.1438\n",
      "Iteration 616100, Train Loss: 1.9933, Validation Loss: 2.8975\n",
      "Iteration 616200, Train Loss: 2.5104, Validation Loss: 2.0604\n",
      "Iteration 616300, Train Loss: 2.0452, Validation Loss: 1.9667\n",
      "Iteration 616400, Train Loss: 2.6001, Validation Loss: 2.1605\n",
      "Iteration 616500, Train Loss: 2.0330, Validation Loss: 2.2993\n",
      "Iteration 616600, Train Loss: 1.9058, Validation Loss: 2.0439\n",
      "Iteration 616700, Train Loss: 2.2789, Validation Loss: 1.8821\n",
      "Iteration 616800, Train Loss: 2.0723, Validation Loss: 2.5933\n",
      "Iteration 616900, Train Loss: 2.2188, Validation Loss: 1.9027\n",
      "Iteration 617000, Train Loss: 2.1667, Validation Loss: 2.8334\n",
      "Iteration 617100, Train Loss: 2.0441, Validation Loss: 2.1924\n",
      "Iteration 617200, Train Loss: 2.1511, Validation Loss: 2.3503\n",
      "Iteration 617300, Train Loss: 2.1351, Validation Loss: 1.9877\n",
      "Iteration 617400, Train Loss: 2.0598, Validation Loss: 2.5946\n",
      "Iteration 617500, Train Loss: 2.1415, Validation Loss: 2.1299\n",
      "Iteration 617600, Train Loss: 2.0887, Validation Loss: 2.2609\n",
      "Iteration 617700, Train Loss: 1.9953, Validation Loss: 2.0988\n",
      "Iteration 617800, Train Loss: 2.5284, Validation Loss: 2.4072\n",
      "Iteration 617900, Train Loss: 2.3955, Validation Loss: 2.5578\n",
      "Iteration 618000, Train Loss: 1.4495, Validation Loss: 2.3666\n",
      "Iteration 618100, Train Loss: 2.0680, Validation Loss: 2.1417\n",
      "Iteration 618200, Train Loss: 2.0298, Validation Loss: 2.4272\n",
      "Iteration 618300, Train Loss: 2.2217, Validation Loss: 2.3825\n",
      "Iteration 618400, Train Loss: 2.0342, Validation Loss: 2.2372\n",
      "Iteration 618500, Train Loss: 2.2988, Validation Loss: 2.5886\n",
      "Iteration 618600, Train Loss: 2.1209, Validation Loss: 2.2440\n",
      "Iteration 618700, Train Loss: 2.5734, Validation Loss: 2.3510\n",
      "Iteration 618800, Train Loss: 2.3961, Validation Loss: 2.5336\n",
      "Iteration 618900, Train Loss: 2.5985, Validation Loss: 2.1089\n",
      "Iteration 619000, Train Loss: 2.1467, Validation Loss: 2.5991\n",
      "Iteration 619100, Train Loss: 2.0397, Validation Loss: 2.1581\n",
      "Iteration 619200, Train Loss: 2.4290, Validation Loss: 2.3925\n",
      "Iteration 619300, Train Loss: 2.5379, Validation Loss: 2.4589\n",
      "Iteration 619400, Train Loss: 2.1270, Validation Loss: 2.4600\n",
      "Iteration 619500, Train Loss: 2.4422, Validation Loss: 2.4967\n",
      "Iteration 619600, Train Loss: 1.7371, Validation Loss: 1.8875\n",
      "Iteration 619700, Train Loss: 2.3960, Validation Loss: 2.5897\n",
      "Iteration 619800, Train Loss: 1.9975, Validation Loss: 2.3601\n",
      "Iteration 619900, Train Loss: 2.0776, Validation Loss: 2.4938\n",
      "Iteration 620000, Train Loss: 2.6114, Validation Loss: 2.2583\n",
      "Iteration 620100, Train Loss: 2.2806, Validation Loss: 2.1475\n",
      "Iteration 620200, Train Loss: 2.2136, Validation Loss: 2.9490\n",
      "Iteration 620300, Train Loss: 2.1759, Validation Loss: 2.4468\n",
      "Iteration 620400, Train Loss: 2.1773, Validation Loss: 2.1446\n",
      "Iteration 620500, Train Loss: 2.3512, Validation Loss: 2.5836\n",
      "Iteration 620600, Train Loss: 2.1191, Validation Loss: 2.3961\n",
      "Iteration 620700, Train Loss: 1.9839, Validation Loss: 2.7271\n",
      "Iteration 620800, Train Loss: 2.3766, Validation Loss: 2.5416\n",
      "Iteration 620900, Train Loss: 2.4986, Validation Loss: 2.2265\n",
      "Iteration 621000, Train Loss: 2.4096, Validation Loss: 2.2624\n",
      "Iteration 621100, Train Loss: 2.4587, Validation Loss: 2.3402\n",
      "Iteration 621200, Train Loss: 2.6278, Validation Loss: 2.8011\n",
      "Iteration 621300, Train Loss: 1.9454, Validation Loss: 2.2793\n",
      "Iteration 621400, Train Loss: 2.5854, Validation Loss: 2.2138\n",
      "Iteration 621500, Train Loss: 2.4656, Validation Loss: 2.1603\n",
      "Iteration 621600, Train Loss: 2.2636, Validation Loss: 2.2035\n",
      "Iteration 621700, Train Loss: 2.0882, Validation Loss: 2.4287\n",
      "Iteration 621800, Train Loss: 2.0399, Validation Loss: 2.6725\n",
      "Iteration 621900, Train Loss: 1.9312, Validation Loss: 2.2664\n",
      "Iteration 622000, Train Loss: 2.1368, Validation Loss: 2.3766\n",
      "Iteration 622100, Train Loss: 2.0379, Validation Loss: 2.5526\n",
      "Iteration 622200, Train Loss: 2.1408, Validation Loss: 1.8241\n",
      "Iteration 622300, Train Loss: 2.3302, Validation Loss: 2.9231\n",
      "Iteration 622400, Train Loss: 2.5211, Validation Loss: 2.5946\n",
      "Iteration 622500, Train Loss: 2.1934, Validation Loss: 2.3653\n",
      "Iteration 622600, Train Loss: 2.0000, Validation Loss: 2.4124\n",
      "Iteration 622700, Train Loss: 2.1859, Validation Loss: 2.3560\n",
      "Iteration 622800, Train Loss: 2.3050, Validation Loss: 2.0886\n",
      "Iteration 622900, Train Loss: 2.0957, Validation Loss: 1.9389\n",
      "Iteration 623000, Train Loss: 2.0879, Validation Loss: 1.9532\n",
      "Iteration 623100, Train Loss: 1.9683, Validation Loss: 2.2891\n",
      "Iteration 623200, Train Loss: 2.6068, Validation Loss: 2.4792\n",
      "Iteration 623300, Train Loss: 2.4468, Validation Loss: 2.3587\n",
      "Iteration 623400, Train Loss: 2.5915, Validation Loss: 2.3714\n",
      "Iteration 623500, Train Loss: 2.5943, Validation Loss: 2.2233\n",
      "Iteration 623600, Train Loss: 1.9779, Validation Loss: 2.1688\n",
      "Iteration 623700, Train Loss: 2.1866, Validation Loss: 2.5581\n",
      "Iteration 623800, Train Loss: 2.8356, Validation Loss: 1.9839\n",
      "Iteration 623900, Train Loss: 2.0815, Validation Loss: 2.1795\n",
      "Iteration 624000, Train Loss: 2.0895, Validation Loss: 2.4543\n",
      "Iteration 624100, Train Loss: 2.5884, Validation Loss: 2.4953\n",
      "Iteration 624200, Train Loss: 2.1394, Validation Loss: 2.1060\n",
      "Iteration 624300, Train Loss: 2.2372, Validation Loss: 2.2394\n",
      "Iteration 624400, Train Loss: 2.1763, Validation Loss: 2.6266\n",
      "Iteration 624500, Train Loss: 2.0179, Validation Loss: 1.7106\n",
      "Iteration 624600, Train Loss: 2.0530, Validation Loss: 2.1537\n",
      "Iteration 624700, Train Loss: 1.9015, Validation Loss: 2.6374\n",
      "Iteration 624800, Train Loss: 2.2178, Validation Loss: 2.0478\n",
      "Iteration 624900, Train Loss: 2.2207, Validation Loss: 2.2389\n",
      "Iteration 625000, Train Loss: 1.7903, Validation Loss: 2.5992\n",
      "Iteration 625100, Train Loss: 2.2991, Validation Loss: 2.0365\n",
      "Iteration 625200, Train Loss: 2.2447, Validation Loss: 2.4820\n",
      "Iteration 625300, Train Loss: 2.1087, Validation Loss: 2.1109\n",
      "Iteration 625400, Train Loss: 2.3252, Validation Loss: 1.9329\n",
      "Iteration 625500, Train Loss: 2.3897, Validation Loss: 2.3660\n",
      "Iteration 625600, Train Loss: 2.3880, Validation Loss: 2.0913\n",
      "Iteration 625700, Train Loss: 2.4021, Validation Loss: 2.3412\n",
      "Iteration 625800, Train Loss: 2.2456, Validation Loss: 2.1290\n",
      "Iteration 625900, Train Loss: 1.8836, Validation Loss: 2.3731\n",
      "Iteration 626000, Train Loss: 2.1146, Validation Loss: 2.0483\n",
      "Iteration 626100, Train Loss: 2.4408, Validation Loss: 2.3294\n",
      "Iteration 626200, Train Loss: 2.6212, Validation Loss: 2.1102\n",
      "Iteration 626300, Train Loss: 2.6042, Validation Loss: 2.2907\n",
      "Iteration 626400, Train Loss: 2.1394, Validation Loss: 1.9913\n",
      "Iteration 626500, Train Loss: 1.8750, Validation Loss: 2.0489\n",
      "Iteration 626600, Train Loss: 2.1058, Validation Loss: 2.4839\n",
      "Iteration 626700, Train Loss: 2.4784, Validation Loss: 2.3361\n",
      "Iteration 626800, Train Loss: 2.0227, Validation Loss: 2.1089\n",
      "Iteration 626900, Train Loss: 2.7883, Validation Loss: 2.2191\n",
      "Iteration 627000, Train Loss: 2.4788, Validation Loss: 2.5308\n",
      "Iteration 627100, Train Loss: 2.3629, Validation Loss: 2.1434\n",
      "Iteration 627200, Train Loss: 1.9721, Validation Loss: 2.2799\n",
      "Iteration 627300, Train Loss: 2.1763, Validation Loss: 2.1247\n",
      "Iteration 627400, Train Loss: 1.9616, Validation Loss: 2.5583\n",
      "Iteration 627500, Train Loss: 2.3696, Validation Loss: 2.2736\n",
      "Iteration 627600, Train Loss: 2.5633, Validation Loss: 1.8097\n",
      "Iteration 627700, Train Loss: 1.7766, Validation Loss: 2.3247\n",
      "Iteration 627800, Train Loss: 2.1519, Validation Loss: 2.3667\n",
      "Iteration 627900, Train Loss: 2.0319, Validation Loss: 2.6423\n",
      "Iteration 628000, Train Loss: 2.1887, Validation Loss: 2.3177\n",
      "Iteration 628100, Train Loss: 2.2352, Validation Loss: 2.5997\n",
      "Iteration 628200, Train Loss: 1.8416, Validation Loss: 2.2452\n",
      "Iteration 628300, Train Loss: 2.2162, Validation Loss: 2.3576\n",
      "Iteration 628400, Train Loss: 2.0452, Validation Loss: 2.4475\n",
      "Iteration 628500, Train Loss: 2.1388, Validation Loss: 2.4817\n",
      "Iteration 628600, Train Loss: 2.7438, Validation Loss: 2.2864\n",
      "Iteration 628700, Train Loss: 2.2637, Validation Loss: 2.5045\n",
      "Iteration 628800, Train Loss: 2.3745, Validation Loss: 2.3281\n",
      "Iteration 628900, Train Loss: 2.6224, Validation Loss: 2.1008\n",
      "Iteration 629000, Train Loss: 2.2451, Validation Loss: 2.0321\n",
      "Iteration 629100, Train Loss: 1.8659, Validation Loss: 2.7677\n",
      "Iteration 629200, Train Loss: 2.5561, Validation Loss: 2.1234\n",
      "Iteration 629300, Train Loss: 2.4559, Validation Loss: 2.4055\n",
      "Iteration 629400, Train Loss: 1.9842, Validation Loss: 2.4052\n",
      "Iteration 629500, Train Loss: 2.1142, Validation Loss: 2.4471\n",
      "Iteration 629600, Train Loss: 1.8915, Validation Loss: 2.3518\n",
      "Iteration 629700, Train Loss: 1.7317, Validation Loss: 2.0570\n",
      "Iteration 629800, Train Loss: 2.2220, Validation Loss: 2.0631\n",
      "Iteration 629900, Train Loss: 2.1185, Validation Loss: 2.1369\n",
      "Iteration 630000, Train Loss: 2.1062, Validation Loss: 2.1939\n",
      "Iteration 630100, Train Loss: 2.5615, Validation Loss: 2.4374\n",
      "Iteration 630200, Train Loss: 2.4412, Validation Loss: 2.3285\n",
      "Iteration 630300, Train Loss: 2.5783, Validation Loss: 2.1615\n",
      "Iteration 630400, Train Loss: 2.2034, Validation Loss: 2.0899\n",
      "Iteration 630500, Train Loss: 2.1469, Validation Loss: 2.3894\n",
      "Iteration 630600, Train Loss: 2.1902, Validation Loss: 1.8663\n",
      "Iteration 630700, Train Loss: 2.3189, Validation Loss: 2.7583\n",
      "Iteration 630800, Train Loss: 1.8976, Validation Loss: 2.9205\n",
      "Iteration 630900, Train Loss: 2.1335, Validation Loss: 2.5287\n",
      "Iteration 631000, Train Loss: 2.5637, Validation Loss: 2.3187\n",
      "Iteration 631100, Train Loss: 2.2511, Validation Loss: 2.2591\n",
      "Iteration 631200, Train Loss: 2.3253, Validation Loss: 2.4023\n",
      "Iteration 631300, Train Loss: 2.4479, Validation Loss: 2.1620\n",
      "Iteration 631400, Train Loss: 2.5319, Validation Loss: 2.2058\n",
      "Iteration 631500, Train Loss: 2.3149, Validation Loss: 2.2906\n",
      "Iteration 631600, Train Loss: 2.2750, Validation Loss: 2.1724\n",
      "Iteration 631700, Train Loss: 2.1831, Validation Loss: 2.2886\n",
      "Iteration 631800, Train Loss: 2.1061, Validation Loss: 2.2948\n",
      "Iteration 631900, Train Loss: 2.1937, Validation Loss: 2.7563\n",
      "Iteration 632000, Train Loss: 2.1596, Validation Loss: 2.5477\n",
      "Iteration 632100, Train Loss: 1.9201, Validation Loss: 2.3220\n",
      "Iteration 632200, Train Loss: 2.1337, Validation Loss: 2.2624\n",
      "Iteration 632300, Train Loss: 2.7242, Validation Loss: 2.1990\n",
      "Iteration 632400, Train Loss: 2.3463, Validation Loss: 2.2978\n",
      "Iteration 632500, Train Loss: 2.1924, Validation Loss: 2.0079\n",
      "Iteration 632600, Train Loss: 2.0933, Validation Loss: 2.1140\n",
      "Iteration 632700, Train Loss: 2.3096, Validation Loss: 2.5115\n",
      "Iteration 632800, Train Loss: 2.1195, Validation Loss: 2.2731\n",
      "Iteration 632900, Train Loss: 2.3025, Validation Loss: 2.0531\n",
      "Iteration 633000, Train Loss: 1.9099, Validation Loss: 2.0934\n",
      "Iteration 633100, Train Loss: 2.2692, Validation Loss: 2.4082\n",
      "Iteration 633200, Train Loss: 2.4450, Validation Loss: 2.3118\n",
      "Iteration 633300, Train Loss: 2.3219, Validation Loss: 2.3223\n",
      "Iteration 633400, Train Loss: 2.0719, Validation Loss: 2.0962\n",
      "Iteration 633500, Train Loss: 1.9805, Validation Loss: 2.1588\n",
      "Iteration 633600, Train Loss: 2.6655, Validation Loss: 2.2703\n",
      "Iteration 633700, Train Loss: 2.4734, Validation Loss: 2.1651\n",
      "Iteration 633800, Train Loss: 2.2359, Validation Loss: 2.2375\n",
      "Iteration 633900, Train Loss: 2.0735, Validation Loss: 2.2521\n",
      "Iteration 634000, Train Loss: 2.0375, Validation Loss: 2.4410\n",
      "Iteration 634100, Train Loss: 2.0387, Validation Loss: 2.2628\n",
      "Iteration 634200, Train Loss: 2.4016, Validation Loss: 2.0895\n",
      "Iteration 634300, Train Loss: 2.1159, Validation Loss: 2.5114\n",
      "Iteration 634400, Train Loss: 2.5641, Validation Loss: 2.4436\n",
      "Iteration 634500, Train Loss: 2.3836, Validation Loss: 2.3960\n",
      "Iteration 634600, Train Loss: 2.3658, Validation Loss: 1.9222\n",
      "Iteration 634700, Train Loss: 2.1087, Validation Loss: 2.4982\n",
      "Iteration 634800, Train Loss: 2.3160, Validation Loss: 1.9781\n",
      "Iteration 634900, Train Loss: 2.4232, Validation Loss: 2.2976\n",
      "Iteration 635000, Train Loss: 2.6012, Validation Loss: 2.3742\n",
      "Iteration 635100, Train Loss: 2.1401, Validation Loss: 1.9600\n",
      "Iteration 635200, Train Loss: 2.1721, Validation Loss: 2.2977\n",
      "Iteration 635300, Train Loss: 2.4682, Validation Loss: 2.2521\n",
      "Iteration 635400, Train Loss: 2.6283, Validation Loss: 2.4514\n",
      "Iteration 635500, Train Loss: 1.8011, Validation Loss: 2.0765\n",
      "Iteration 635600, Train Loss: 2.4199, Validation Loss: 2.7527\n",
      "Iteration 635700, Train Loss: 2.3830, Validation Loss: 2.3730\n",
      "Iteration 635800, Train Loss: 2.3834, Validation Loss: 2.2702\n",
      "Iteration 635900, Train Loss: 2.5873, Validation Loss: 2.4050\n",
      "Iteration 636000, Train Loss: 2.3906, Validation Loss: 1.8569\n",
      "Iteration 636100, Train Loss: 2.1364, Validation Loss: 2.5030\n",
      "Iteration 636200, Train Loss: 2.3733, Validation Loss: 2.6840\n",
      "Iteration 636300, Train Loss: 2.1663, Validation Loss: 2.3637\n",
      "Iteration 636400, Train Loss: 2.2159, Validation Loss: 2.4504\n",
      "Iteration 636500, Train Loss: 2.1545, Validation Loss: 2.2505\n",
      "Iteration 636600, Train Loss: 2.2030, Validation Loss: 2.5071\n",
      "Iteration 636700, Train Loss: 1.8630, Validation Loss: 1.7450\n",
      "Iteration 636800, Train Loss: 2.0588, Validation Loss: 2.7028\n",
      "Iteration 636900, Train Loss: 2.1704, Validation Loss: 2.9620\n",
      "Iteration 637000, Train Loss: 2.2846, Validation Loss: 2.1797\n",
      "Iteration 637100, Train Loss: 1.7968, Validation Loss: 2.5793\n",
      "Iteration 637200, Train Loss: 2.4764, Validation Loss: 2.5545\n",
      "Iteration 637300, Train Loss: 1.8985, Validation Loss: 2.0399\n",
      "Iteration 637400, Train Loss: 2.2952, Validation Loss: 2.0013\n",
      "Iteration 637500, Train Loss: 2.0159, Validation Loss: 2.1024\n",
      "Iteration 637600, Train Loss: 2.3987, Validation Loss: 2.2087\n",
      "Iteration 637700, Train Loss: 1.9691, Validation Loss: 2.2128\n",
      "Iteration 637800, Train Loss: 2.5804, Validation Loss: 2.1633\n",
      "Iteration 637900, Train Loss: 2.3209, Validation Loss: 2.4985\n",
      "Iteration 638000, Train Loss: 2.4106, Validation Loss: 1.9423\n",
      "Iteration 638100, Train Loss: 2.3715, Validation Loss: 2.4021\n",
      "Iteration 638200, Train Loss: 2.9059, Validation Loss: 2.4008\n",
      "Iteration 638300, Train Loss: 2.3445, Validation Loss: 1.8389\n",
      "Iteration 638400, Train Loss: 2.6788, Validation Loss: 2.2977\n",
      "Iteration 638500, Train Loss: 2.1378, Validation Loss: 3.0198\n",
      "Iteration 638600, Train Loss: 1.9482, Validation Loss: 2.1012\n",
      "Iteration 638700, Train Loss: 2.2733, Validation Loss: 2.3742\n",
      "Iteration 638800, Train Loss: 1.8509, Validation Loss: 2.2856\n",
      "Iteration 638900, Train Loss: 1.9381, Validation Loss: 2.2886\n",
      "Iteration 639000, Train Loss: 2.6644, Validation Loss: 2.0643\n",
      "Iteration 639100, Train Loss: 2.2229, Validation Loss: 2.1830\n",
      "Iteration 639200, Train Loss: 1.7961, Validation Loss: 1.7473\n",
      "Iteration 639300, Train Loss: 1.9400, Validation Loss: 2.3984\n",
      "Iteration 639400, Train Loss: 2.5660, Validation Loss: 2.8850\n",
      "Iteration 639500, Train Loss: 2.1094, Validation Loss: 2.3230\n",
      "Iteration 639600, Train Loss: 3.2777, Validation Loss: 2.2804\n",
      "Iteration 639700, Train Loss: 2.4242, Validation Loss: 2.1753\n",
      "Iteration 639800, Train Loss: 1.8616, Validation Loss: 2.4410\n",
      "Iteration 639900, Train Loss: 2.2767, Validation Loss: 1.9354\n",
      "Iteration 640000, Train Loss: 2.0604, Validation Loss: 1.9909\n",
      "Iteration 640100, Train Loss: 2.5663, Validation Loss: 2.2247\n",
      "Iteration 640200, Train Loss: 2.5833, Validation Loss: 2.1532\n",
      "Iteration 640300, Train Loss: 2.5539, Validation Loss: 2.4364\n",
      "Iteration 640400, Train Loss: 2.2736, Validation Loss: 2.5149\n",
      "Iteration 640500, Train Loss: 1.8236, Validation Loss: 2.3005\n",
      "Iteration 640600, Train Loss: 2.2514, Validation Loss: 2.2166\n",
      "Iteration 640700, Train Loss: 2.3973, Validation Loss: 2.3197\n",
      "Iteration 640800, Train Loss: 2.3150, Validation Loss: 2.2078\n",
      "Iteration 640900, Train Loss: 2.1350, Validation Loss: 2.0604\n",
      "Iteration 641000, Train Loss: 2.1976, Validation Loss: 2.1349\n",
      "Iteration 641100, Train Loss: 2.5095, Validation Loss: 2.0055\n",
      "Iteration 641200, Train Loss: 2.7549, Validation Loss: 2.1738\n",
      "Iteration 641300, Train Loss: 2.3695, Validation Loss: 2.2005\n",
      "Iteration 641400, Train Loss: 1.9914, Validation Loss: 2.3203\n",
      "Iteration 641500, Train Loss: 2.3765, Validation Loss: 2.1123\n",
      "Iteration 641600, Train Loss: 2.0396, Validation Loss: 2.5369\n",
      "Iteration 641700, Train Loss: 2.3281, Validation Loss: 2.4253\n",
      "Iteration 641800, Train Loss: 2.5689, Validation Loss: 2.7928\n",
      "Iteration 641900, Train Loss: 2.1452, Validation Loss: 2.6389\n",
      "Iteration 642000, Train Loss: 2.1254, Validation Loss: 2.0264\n",
      "Iteration 642100, Train Loss: 2.5698, Validation Loss: 2.2977\n",
      "Iteration 642200, Train Loss: 2.3066, Validation Loss: 2.2394\n",
      "Iteration 642300, Train Loss: 1.6928, Validation Loss: 1.8503\n",
      "Iteration 642400, Train Loss: 2.3523, Validation Loss: 2.4533\n",
      "Iteration 642500, Train Loss: 2.4912, Validation Loss: 2.1509\n",
      "Iteration 642600, Train Loss: 2.2000, Validation Loss: 1.9247\n",
      "Iteration 642700, Train Loss: 2.6363, Validation Loss: 2.4951\n",
      "Iteration 642800, Train Loss: 1.9388, Validation Loss: 2.2839\n",
      "Iteration 642900, Train Loss: 2.1243, Validation Loss: 2.4156\n",
      "Iteration 643000, Train Loss: 2.3746, Validation Loss: 2.2528\n",
      "Iteration 643100, Train Loss: 2.1525, Validation Loss: 2.4774\n",
      "Iteration 643200, Train Loss: 2.6190, Validation Loss: 2.1258\n",
      "Iteration 643300, Train Loss: 1.9832, Validation Loss: 2.3030\n",
      "Iteration 643400, Train Loss: 2.2547, Validation Loss: 2.0157\n",
      "Iteration 643500, Train Loss: 2.2499, Validation Loss: 2.0669\n",
      "Iteration 643600, Train Loss: 1.9078, Validation Loss: 2.2705\n",
      "Iteration 643700, Train Loss: 2.5674, Validation Loss: 2.1939\n",
      "Iteration 643800, Train Loss: 2.2269, Validation Loss: 3.2295\n",
      "Iteration 643900, Train Loss: 2.2797, Validation Loss: 2.3398\n",
      "Iteration 644000, Train Loss: 2.4179, Validation Loss: 2.1509\n",
      "Iteration 644100, Train Loss: 2.0886, Validation Loss: 2.4279\n",
      "Iteration 644200, Train Loss: 2.6378, Validation Loss: 2.0823\n",
      "Iteration 644300, Train Loss: 2.3478, Validation Loss: 2.1568\n",
      "Iteration 644400, Train Loss: 2.1847, Validation Loss: 2.1793\n",
      "Iteration 644500, Train Loss: 2.0045, Validation Loss: 2.2619\n",
      "Iteration 644600, Train Loss: 2.1385, Validation Loss: 2.4184\n",
      "Iteration 644700, Train Loss: 1.9182, Validation Loss: 2.2354\n",
      "Iteration 644800, Train Loss: 2.0113, Validation Loss: 1.8806\n",
      "Iteration 644900, Train Loss: 1.9591, Validation Loss: 2.2838\n",
      "Iteration 645000, Train Loss: 2.4344, Validation Loss: 2.7422\n",
      "Iteration 645100, Train Loss: 2.1059, Validation Loss: 1.9862\n",
      "Iteration 645200, Train Loss: 2.2326, Validation Loss: 1.9695\n",
      "Iteration 645300, Train Loss: 2.2947, Validation Loss: 2.0661\n",
      "Iteration 645400, Train Loss: 2.2374, Validation Loss: 2.1855\n",
      "Iteration 645500, Train Loss: 2.0756, Validation Loss: 2.2726\n",
      "Iteration 645600, Train Loss: 2.4053, Validation Loss: 2.2783\n",
      "Iteration 645700, Train Loss: 2.4701, Validation Loss: 2.4243\n",
      "Iteration 645800, Train Loss: 2.1337, Validation Loss: 2.3391\n",
      "Iteration 645900, Train Loss: 1.8487, Validation Loss: 2.1572\n",
      "Iteration 646000, Train Loss: 2.3515, Validation Loss: 2.8176\n",
      "Iteration 646100, Train Loss: 1.7615, Validation Loss: 2.2075\n",
      "Iteration 646200, Train Loss: 2.1310, Validation Loss: 2.4568\n",
      "Iteration 646300, Train Loss: 2.3345, Validation Loss: 2.2884\n",
      "Iteration 646400, Train Loss: 2.6513, Validation Loss: 2.5358\n",
      "Iteration 646500, Train Loss: 2.5545, Validation Loss: 2.2753\n",
      "Iteration 646600, Train Loss: 2.3962, Validation Loss: 2.2419\n",
      "Iteration 646700, Train Loss: 2.0003, Validation Loss: 2.4783\n",
      "Iteration 646800, Train Loss: 2.4136, Validation Loss: 2.2171\n",
      "Iteration 646900, Train Loss: 2.1142, Validation Loss: 2.3618\n",
      "Iteration 647000, Train Loss: 1.9982, Validation Loss: 2.1050\n",
      "Iteration 647100, Train Loss: 1.8605, Validation Loss: 1.8944\n",
      "Iteration 647200, Train Loss: 2.5434, Validation Loss: 2.3168\n",
      "Iteration 647300, Train Loss: 2.4606, Validation Loss: 2.0583\n",
      "Iteration 647400, Train Loss: 2.1151, Validation Loss: 2.3120\n",
      "Iteration 647500, Train Loss: 2.3565, Validation Loss: 2.2573\n",
      "Iteration 647600, Train Loss: 2.3121, Validation Loss: 2.0794\n",
      "Iteration 647700, Train Loss: 2.1700, Validation Loss: 2.4792\n",
      "Iteration 647800, Train Loss: 2.3875, Validation Loss: 2.1049\n",
      "Iteration 647900, Train Loss: 2.7254, Validation Loss: 2.4982\n",
      "Iteration 648000, Train Loss: 1.7641, Validation Loss: 2.1390\n",
      "Iteration 648100, Train Loss: 2.2295, Validation Loss: 2.3335\n",
      "Iteration 648200, Train Loss: 1.7952, Validation Loss: 2.6666\n",
      "Iteration 648300, Train Loss: 2.4239, Validation Loss: 2.3839\n",
      "Iteration 648400, Train Loss: 2.0104, Validation Loss: 2.7701\n",
      "Iteration 648500, Train Loss: 2.0834, Validation Loss: 2.6840\n",
      "Iteration 648600, Train Loss: 2.1956, Validation Loss: 2.2099\n",
      "Iteration 648700, Train Loss: 2.1362, Validation Loss: 2.3289\n",
      "Iteration 648800, Train Loss: 2.4205, Validation Loss: 2.6335\n",
      "Iteration 648900, Train Loss: 2.6313, Validation Loss: 2.1698\n",
      "Iteration 649000, Train Loss: 2.1957, Validation Loss: 2.7344\n",
      "Iteration 649100, Train Loss: 2.2574, Validation Loss: 1.9726\n",
      "Iteration 649200, Train Loss: 2.1305, Validation Loss: 2.3788\n",
      "Iteration 649300, Train Loss: 2.3205, Validation Loss: 2.0247\n",
      "Iteration 649400, Train Loss: 1.9551, Validation Loss: 2.2165\n",
      "Iteration 649500, Train Loss: 2.1192, Validation Loss: 2.2438\n",
      "Iteration 649600, Train Loss: 2.2688, Validation Loss: 2.1709\n",
      "Iteration 649700, Train Loss: 1.9174, Validation Loss: 2.7709\n",
      "Iteration 649800, Train Loss: 2.0694, Validation Loss: 2.0176\n",
      "Iteration 649900, Train Loss: 1.9150, Validation Loss: 2.2816\n",
      "Iteration 650000, Train Loss: 2.0031, Validation Loss: 2.5723\n",
      "Iteration 650100, Train Loss: 1.7370, Validation Loss: 2.2411\n",
      "Iteration 650200, Train Loss: 2.5064, Validation Loss: 2.2947\n",
      "Iteration 650300, Train Loss: 1.9744, Validation Loss: 2.3345\n",
      "Iteration 650400, Train Loss: 2.2784, Validation Loss: 2.5721\n",
      "Iteration 650500, Train Loss: 1.6453, Validation Loss: 2.3042\n",
      "Iteration 650600, Train Loss: 2.6109, Validation Loss: 2.7264\n",
      "Iteration 650700, Train Loss: 1.9909, Validation Loss: 2.3956\n",
      "Iteration 650800, Train Loss: 2.3492, Validation Loss: 2.4367\n",
      "Iteration 650900, Train Loss: 2.4689, Validation Loss: 2.9549\n",
      "Iteration 651000, Train Loss: 2.3035, Validation Loss: 2.8794\n",
      "Iteration 651100, Train Loss: 2.2690, Validation Loss: 1.8287\n",
      "Iteration 651200, Train Loss: 2.0348, Validation Loss: 2.1436\n",
      "Iteration 651300, Train Loss: 2.2189, Validation Loss: 1.9748\n",
      "Iteration 651400, Train Loss: 2.1491, Validation Loss: 2.2864\n",
      "Iteration 651500, Train Loss: 1.9230, Validation Loss: 2.4475\n",
      "Iteration 651600, Train Loss: 2.3120, Validation Loss: 2.3917\n",
      "Iteration 651700, Train Loss: 2.3405, Validation Loss: 2.3232\n",
      "Iteration 651800, Train Loss: 1.9280, Validation Loss: 1.9359\n",
      "Iteration 651900, Train Loss: 2.3542, Validation Loss: 2.4505\n",
      "Iteration 652000, Train Loss: 2.4609, Validation Loss: 2.1135\n",
      "Iteration 652100, Train Loss: 2.0687, Validation Loss: 2.0500\n",
      "Iteration 652200, Train Loss: 2.1566, Validation Loss: 2.6820\n",
      "Iteration 652300, Train Loss: 2.9673, Validation Loss: 2.3598\n",
      "Iteration 652400, Train Loss: 2.0929, Validation Loss: 2.3601\n",
      "Iteration 652500, Train Loss: 2.1448, Validation Loss: 2.5974\n",
      "Iteration 652600, Train Loss: 2.4921, Validation Loss: 2.8029\n",
      "Iteration 652700, Train Loss: 2.2424, Validation Loss: 2.3109\n",
      "Iteration 652800, Train Loss: 1.9054, Validation Loss: 2.1967\n",
      "Iteration 652900, Train Loss: 2.4870, Validation Loss: 2.5555\n",
      "Iteration 653000, Train Loss: 2.2320, Validation Loss: 2.2018\n",
      "Iteration 653100, Train Loss: 2.2097, Validation Loss: 2.6050\n",
      "Iteration 653200, Train Loss: 2.1224, Validation Loss: 2.2512\n",
      "Iteration 653300, Train Loss: 2.2524, Validation Loss: 2.2106\n",
      "Iteration 653400, Train Loss: 2.5212, Validation Loss: 2.5523\n",
      "Iteration 653500, Train Loss: 2.6595, Validation Loss: 2.2750\n",
      "Iteration 653600, Train Loss: 2.1629, Validation Loss: 2.1315\n",
      "Iteration 653700, Train Loss: 2.5474, Validation Loss: 2.5833\n",
      "Iteration 653800, Train Loss: 2.3570, Validation Loss: 2.0932\n",
      "Iteration 653900, Train Loss: 2.1285, Validation Loss: 1.9457\n",
      "Iteration 654000, Train Loss: 1.8799, Validation Loss: 2.4079\n",
      "Iteration 654100, Train Loss: 2.4032, Validation Loss: 2.8986\n",
      "Iteration 654200, Train Loss: 2.2636, Validation Loss: 2.6237\n",
      "Iteration 654300, Train Loss: 2.0395, Validation Loss: 2.1315\n",
      "Iteration 654400, Train Loss: 2.2707, Validation Loss: 2.7770\n",
      "Iteration 654500, Train Loss: 2.3118, Validation Loss: 2.4414\n",
      "Iteration 654600, Train Loss: 2.3883, Validation Loss: 2.6879\n",
      "Iteration 654700, Train Loss: 2.2210, Validation Loss: 1.7579\n",
      "Iteration 654800, Train Loss: 2.1506, Validation Loss: 2.0861\n",
      "Iteration 654900, Train Loss: 2.4289, Validation Loss: 2.3729\n",
      "Iteration 655000, Train Loss: 2.1995, Validation Loss: 2.3789\n",
      "Iteration 655100, Train Loss: 2.4307, Validation Loss: 2.5167\n",
      "Iteration 655200, Train Loss: 2.4573, Validation Loss: 2.5688\n",
      "Iteration 655300, Train Loss: 2.4593, Validation Loss: 2.2247\n",
      "Iteration 655400, Train Loss: 2.3339, Validation Loss: 2.1204\n",
      "Iteration 655500, Train Loss: 2.2364, Validation Loss: 2.0715\n",
      "Iteration 655600, Train Loss: 2.2269, Validation Loss: 2.2455\n",
      "Iteration 655700, Train Loss: 2.3342, Validation Loss: 2.2381\n",
      "Iteration 655800, Train Loss: 2.0839, Validation Loss: 2.5679\n",
      "Iteration 655900, Train Loss: 2.4197, Validation Loss: 2.2774\n",
      "Iteration 656000, Train Loss: 2.9048, Validation Loss: 2.3101\n",
      "Iteration 656100, Train Loss: 2.0547, Validation Loss: 2.2177\n",
      "Iteration 656200, Train Loss: 2.0122, Validation Loss: 2.2633\n",
      "Iteration 656300, Train Loss: 2.1345, Validation Loss: 2.4124\n",
      "Iteration 656400, Train Loss: 2.0858, Validation Loss: 2.3416\n",
      "Iteration 656500, Train Loss: 1.8801, Validation Loss: 2.0251\n",
      "Iteration 656600, Train Loss: 2.5083, Validation Loss: 2.2918\n",
      "Iteration 656700, Train Loss: 2.2401, Validation Loss: 2.3619\n",
      "Iteration 656800, Train Loss: 2.1582, Validation Loss: 1.9561\n",
      "Iteration 656900, Train Loss: 1.9974, Validation Loss: 2.4339\n",
      "Iteration 657000, Train Loss: 3.0175, Validation Loss: 2.2782\n",
      "Iteration 657100, Train Loss: 2.0192, Validation Loss: 2.2534\n",
      "Iteration 657200, Train Loss: 1.9702, Validation Loss: 2.0259\n",
      "Iteration 657300, Train Loss: 2.5234, Validation Loss: 2.4709\n",
      "Iteration 657400, Train Loss: 1.9530, Validation Loss: 2.4555\n",
      "Iteration 657500, Train Loss: 2.1015, Validation Loss: 2.5351\n",
      "Iteration 657600, Train Loss: 2.1533, Validation Loss: 2.2705\n",
      "Iteration 657700, Train Loss: 2.0230, Validation Loss: 2.5924\n",
      "Iteration 657800, Train Loss: 2.1573, Validation Loss: 1.6778\n",
      "Iteration 657900, Train Loss: 2.0404, Validation Loss: 2.7326\n",
      "Iteration 658000, Train Loss: 2.2635, Validation Loss: 2.1591\n",
      "Iteration 658100, Train Loss: 2.3860, Validation Loss: 1.9623\n",
      "Iteration 658200, Train Loss: 2.5613, Validation Loss: 2.0935\n",
      "Iteration 658300, Train Loss: 2.0758, Validation Loss: 2.1757\n",
      "Iteration 658400, Train Loss: 1.7267, Validation Loss: 2.3733\n",
      "Iteration 658500, Train Loss: 1.7420, Validation Loss: 2.4976\n",
      "Iteration 658600, Train Loss: 2.0790, Validation Loss: 2.4021\n",
      "Iteration 658700, Train Loss: 1.9495, Validation Loss: 1.8598\n",
      "Iteration 658800, Train Loss: 1.9839, Validation Loss: 1.8421\n",
      "Iteration 658900, Train Loss: 2.3344, Validation Loss: 3.1928\n",
      "Iteration 659000, Train Loss: 1.9666, Validation Loss: 2.1616\n",
      "Iteration 659100, Train Loss: 2.2203, Validation Loss: 2.2773\n",
      "Iteration 659200, Train Loss: 2.6915, Validation Loss: 2.1859\n",
      "Iteration 659300, Train Loss: 2.1691, Validation Loss: 1.8384\n",
      "Iteration 659400, Train Loss: 2.1454, Validation Loss: 2.1334\n",
      "Iteration 659500, Train Loss: 2.4444, Validation Loss: 2.4159\n",
      "Iteration 659600, Train Loss: 2.4527, Validation Loss: 2.4804\n",
      "Iteration 659700, Train Loss: 2.5083, Validation Loss: 2.1523\n",
      "Iteration 659800, Train Loss: 1.7793, Validation Loss: 2.0414\n",
      "Iteration 659900, Train Loss: 1.8653, Validation Loss: 2.3756\n",
      "Iteration 660000, Train Loss: 2.1329, Validation Loss: 2.5777\n",
      "Iteration 660100, Train Loss: 1.8953, Validation Loss: 2.4961\n",
      "Iteration 660200, Train Loss: 1.8662, Validation Loss: 2.3511\n",
      "Iteration 660300, Train Loss: 2.1187, Validation Loss: 2.1552\n",
      "Iteration 660400, Train Loss: 2.2894, Validation Loss: 2.2172\n",
      "Iteration 660500, Train Loss: 2.4750, Validation Loss: 2.3878\n",
      "Iteration 660600, Train Loss: 2.5539, Validation Loss: 2.1085\n",
      "Iteration 660700, Train Loss: 2.4654, Validation Loss: 2.1689\n",
      "Iteration 660800, Train Loss: 2.1793, Validation Loss: 2.2830\n",
      "Iteration 660900, Train Loss: 2.1896, Validation Loss: 1.9068\n",
      "Iteration 661000, Train Loss: 2.2679, Validation Loss: 2.3843\n",
      "Iteration 661100, Train Loss: 1.7820, Validation Loss: 2.6615\n",
      "Iteration 661200, Train Loss: 1.8471, Validation Loss: 2.3316\n",
      "Iteration 661300, Train Loss: 2.0587, Validation Loss: 2.3933\n",
      "Iteration 661400, Train Loss: 2.3480, Validation Loss: 2.4207\n",
      "Iteration 661500, Train Loss: 2.1437, Validation Loss: 1.9683\n",
      "Iteration 661600, Train Loss: 1.7151, Validation Loss: 1.8885\n",
      "Iteration 661700, Train Loss: 1.9668, Validation Loss: 2.3277\n",
      "Iteration 661800, Train Loss: 2.1710, Validation Loss: 2.0128\n",
      "Iteration 661900, Train Loss: 2.1434, Validation Loss: 2.2857\n",
      "Iteration 662000, Train Loss: 2.2698, Validation Loss: 1.9115\n",
      "Iteration 662100, Train Loss: 2.1749, Validation Loss: 2.2586\n",
      "Iteration 662200, Train Loss: 1.8993, Validation Loss: 2.3190\n",
      "Iteration 662300, Train Loss: 2.2290, Validation Loss: 1.9742\n",
      "Iteration 662400, Train Loss: 2.1516, Validation Loss: 2.5349\n",
      "Iteration 662500, Train Loss: 2.2085, Validation Loss: 2.0547\n",
      "Iteration 662600, Train Loss: 2.0267, Validation Loss: 2.0163\n",
      "Iteration 662700, Train Loss: 2.3313, Validation Loss: 2.0146\n",
      "Iteration 662800, Train Loss: 2.0151, Validation Loss: 2.3771\n",
      "Iteration 662900, Train Loss: 2.8534, Validation Loss: 2.4415\n",
      "Iteration 663000, Train Loss: 2.2971, Validation Loss: 2.0721\n",
      "Iteration 663100, Train Loss: 2.2990, Validation Loss: 2.3150\n",
      "Iteration 663200, Train Loss: 2.5877, Validation Loss: 2.3327\n",
      "Iteration 663300, Train Loss: 1.8098, Validation Loss: 2.0935\n",
      "Iteration 663400, Train Loss: 2.1021, Validation Loss: 2.1951\n",
      "Iteration 663500, Train Loss: 1.9871, Validation Loss: 2.8536\n",
      "Iteration 663600, Train Loss: 2.2145, Validation Loss: 2.3636\n",
      "Iteration 663700, Train Loss: 2.2604, Validation Loss: 1.9180\n",
      "Iteration 663800, Train Loss: 3.3919, Validation Loss: 2.1424\n",
      "Iteration 663900, Train Loss: 1.9928, Validation Loss: 2.0787\n",
      "Iteration 664000, Train Loss: 2.1784, Validation Loss: 2.3397\n",
      "Iteration 664100, Train Loss: 2.2854, Validation Loss: 2.3012\n",
      "Iteration 664200, Train Loss: 2.2352, Validation Loss: 2.6171\n",
      "Iteration 664300, Train Loss: 2.0871, Validation Loss: 2.3531\n",
      "Iteration 664400, Train Loss: 2.1169, Validation Loss: 2.1678\n",
      "Iteration 664500, Train Loss: 2.2145, Validation Loss: 2.2337\n",
      "Iteration 664600, Train Loss: 2.2582, Validation Loss: 2.7770\n",
      "Iteration 664700, Train Loss: 2.4189, Validation Loss: 1.9016\n",
      "Iteration 664800, Train Loss: 2.1800, Validation Loss: 2.4117\n",
      "Iteration 664900, Train Loss: 2.3761, Validation Loss: 2.0602\n",
      "Iteration 665000, Train Loss: 2.4712, Validation Loss: 2.6257\n",
      "Iteration 665100, Train Loss: 2.7814, Validation Loss: 2.2662\n",
      "Iteration 665200, Train Loss: 2.2847, Validation Loss: 2.4352\n",
      "Iteration 665300, Train Loss: 2.5559, Validation Loss: 2.2801\n",
      "Iteration 665400, Train Loss: 2.4906, Validation Loss: 2.3900\n",
      "Iteration 665500, Train Loss: 3.0493, Validation Loss: 2.0898\n",
      "Iteration 665600, Train Loss: 2.4273, Validation Loss: 2.3853\n",
      "Iteration 665700, Train Loss: 2.1436, Validation Loss: 2.3614\n",
      "Iteration 665800, Train Loss: 2.8842, Validation Loss: 2.1441\n",
      "Iteration 665900, Train Loss: 2.4650, Validation Loss: 2.6669\n",
      "Iteration 666000, Train Loss: 1.8609, Validation Loss: 2.5642\n",
      "Iteration 666100, Train Loss: 2.2978, Validation Loss: 2.1503\n",
      "Iteration 666200, Train Loss: 2.0874, Validation Loss: 2.1661\n",
      "Iteration 666300, Train Loss: 2.0443, Validation Loss: 2.4680\n",
      "Iteration 666400, Train Loss: 2.0973, Validation Loss: 1.9851\n",
      "Iteration 666500, Train Loss: 2.4060, Validation Loss: 2.5499\n",
      "Iteration 666600, Train Loss: 2.3286, Validation Loss: 2.5112\n",
      "Iteration 666700, Train Loss: 2.2536, Validation Loss: 2.3023\n",
      "Iteration 666800, Train Loss: 2.6053, Validation Loss: 2.2166\n",
      "Iteration 666900, Train Loss: 2.4136, Validation Loss: 2.4963\n",
      "Iteration 667000, Train Loss: 2.7111, Validation Loss: 2.3923\n",
      "Iteration 667100, Train Loss: 2.3176, Validation Loss: 2.0453\n",
      "Iteration 667200, Train Loss: 2.4584, Validation Loss: 2.0974\n",
      "Iteration 667300, Train Loss: 2.2960, Validation Loss: 2.5665\n",
      "Iteration 667400, Train Loss: 2.2630, Validation Loss: 2.1491\n",
      "Iteration 667500, Train Loss: 2.4989, Validation Loss: 2.4693\n",
      "Iteration 667600, Train Loss: 1.8865, Validation Loss: 2.4537\n",
      "Iteration 667700, Train Loss: 2.0073, Validation Loss: 2.2223\n",
      "Iteration 667800, Train Loss: 2.3963, Validation Loss: 2.3210\n",
      "Iteration 667900, Train Loss: 2.0983, Validation Loss: 2.3812\n",
      "Iteration 668000, Train Loss: 2.2584, Validation Loss: 2.3578\n",
      "Iteration 668100, Train Loss: 2.1990, Validation Loss: 2.4589\n",
      "Iteration 668200, Train Loss: 2.1851, Validation Loss: 2.2950\n",
      "Iteration 668300, Train Loss: 2.2075, Validation Loss: 2.2304\n",
      "Iteration 668400, Train Loss: 2.6578, Validation Loss: 2.5681\n",
      "Iteration 668500, Train Loss: 1.7995, Validation Loss: 2.5765\n",
      "Iteration 668600, Train Loss: 2.3423, Validation Loss: 1.9410\n",
      "Iteration 668700, Train Loss: 2.2672, Validation Loss: 2.8005\n",
      "Iteration 668800, Train Loss: 1.7533, Validation Loss: 2.0093\n",
      "Iteration 668900, Train Loss: 2.3326, Validation Loss: 2.3122\n",
      "Iteration 669000, Train Loss: 2.3164, Validation Loss: 2.0454\n",
      "Iteration 669100, Train Loss: 2.0950, Validation Loss: 2.3198\n",
      "Iteration 669200, Train Loss: 2.3536, Validation Loss: 2.2099\n",
      "Iteration 669300, Train Loss: 2.6090, Validation Loss: 2.5006\n",
      "Iteration 669400, Train Loss: 2.3995, Validation Loss: 2.2691\n",
      "Iteration 669500, Train Loss: 2.1339, Validation Loss: 2.2544\n",
      "Iteration 669600, Train Loss: 2.4805, Validation Loss: 2.2824\n",
      "Iteration 669700, Train Loss: 2.3546, Validation Loss: 2.3786\n",
      "Iteration 669800, Train Loss: 2.3014, Validation Loss: 1.9533\n",
      "Iteration 669900, Train Loss: 2.4581, Validation Loss: 1.9892\n",
      "Iteration 670000, Train Loss: 2.2690, Validation Loss: 2.3984\n",
      "Iteration 670100, Train Loss: 2.5204, Validation Loss: 2.3625\n",
      "Iteration 670200, Train Loss: 2.9602, Validation Loss: 2.2284\n",
      "Iteration 670300, Train Loss: 2.2259, Validation Loss: 1.9903\n",
      "Iteration 670400, Train Loss: 2.3698, Validation Loss: 2.3949\n",
      "Iteration 670500, Train Loss: 1.8543, Validation Loss: 2.3460\n",
      "Iteration 670600, Train Loss: 2.5767, Validation Loss: 2.6762\n",
      "Iteration 670700, Train Loss: 2.2328, Validation Loss: 2.1583\n",
      "Iteration 670800, Train Loss: 2.4776, Validation Loss: 2.2304\n",
      "Iteration 670900, Train Loss: 2.0404, Validation Loss: 2.1321\n",
      "Iteration 671000, Train Loss: 2.2791, Validation Loss: 2.5086\n",
      "Iteration 671100, Train Loss: 2.7661, Validation Loss: 2.4984\n",
      "Iteration 671200, Train Loss: 2.3437, Validation Loss: 2.4156\n",
      "Iteration 671300, Train Loss: 2.0107, Validation Loss: 2.0874\n",
      "Iteration 671400, Train Loss: 2.3772, Validation Loss: 2.2863\n",
      "Iteration 671500, Train Loss: 1.8353, Validation Loss: 2.3226\n",
      "Iteration 671600, Train Loss: 2.3245, Validation Loss: 2.1341\n",
      "Iteration 671700, Train Loss: 1.7576, Validation Loss: 2.4786\n",
      "Iteration 671800, Train Loss: 2.5766, Validation Loss: 2.0815\n",
      "Iteration 671900, Train Loss: 2.2580, Validation Loss: 2.1915\n",
      "Iteration 672000, Train Loss: 2.3150, Validation Loss: 2.5269\n",
      "Iteration 672100, Train Loss: 2.1643, Validation Loss: 2.9720\n",
      "Iteration 672200, Train Loss: 2.2436, Validation Loss: 2.3444\n",
      "Iteration 672300, Train Loss: 1.9850, Validation Loss: 2.1877\n",
      "Iteration 672400, Train Loss: 2.2911, Validation Loss: 2.6812\n",
      "Iteration 672500, Train Loss: 2.1457, Validation Loss: 2.2125\n",
      "Iteration 672600, Train Loss: 2.1457, Validation Loss: 2.4696\n",
      "Iteration 672700, Train Loss: 2.3335, Validation Loss: 2.1595\n",
      "Iteration 672800, Train Loss: 2.3268, Validation Loss: 2.4979\n",
      "Iteration 672900, Train Loss: 2.7352, Validation Loss: 2.2067\n",
      "Iteration 673000, Train Loss: 2.4588, Validation Loss: 2.3194\n",
      "Iteration 673100, Train Loss: 2.1858, Validation Loss: 2.5975\n",
      "Iteration 673200, Train Loss: 2.7541, Validation Loss: 2.1192\n",
      "Iteration 673300, Train Loss: 2.1225, Validation Loss: 2.0869\n",
      "Iteration 673400, Train Loss: 2.0455, Validation Loss: 2.0482\n",
      "Iteration 673500, Train Loss: 2.5150, Validation Loss: 2.2862\n",
      "Iteration 673600, Train Loss: 2.4284, Validation Loss: 2.2482\n",
      "Iteration 673700, Train Loss: 2.5508, Validation Loss: 2.4022\n",
      "Iteration 673800, Train Loss: 2.4140, Validation Loss: 2.1675\n",
      "Iteration 673900, Train Loss: 2.0460, Validation Loss: 1.8500\n",
      "Iteration 674000, Train Loss: 1.9405, Validation Loss: 1.6841\n",
      "Iteration 674100, Train Loss: 2.3524, Validation Loss: 1.9141\n",
      "Iteration 674200, Train Loss: 1.9073, Validation Loss: 2.1566\n",
      "Iteration 674300, Train Loss: 2.2086, Validation Loss: 1.9631\n",
      "Iteration 674400, Train Loss: 2.1768, Validation Loss: 2.3860\n",
      "Iteration 674500, Train Loss: 2.4071, Validation Loss: 2.1204\n",
      "Iteration 674600, Train Loss: 2.1548, Validation Loss: 2.2948\n",
      "Iteration 674700, Train Loss: 2.5288, Validation Loss: 2.1461\n",
      "Iteration 674800, Train Loss: 2.3394, Validation Loss: 1.8740\n",
      "Iteration 674900, Train Loss: 2.1937, Validation Loss: 2.1971\n",
      "Iteration 675000, Train Loss: 2.2797, Validation Loss: 2.0192\n",
      "Iteration 675100, Train Loss: 2.6173, Validation Loss: 2.9292\n",
      "Iteration 675200, Train Loss: 2.0537, Validation Loss: 2.4030\n",
      "Iteration 675300, Train Loss: 2.0702, Validation Loss: 2.6126\n",
      "Iteration 675400, Train Loss: 1.9920, Validation Loss: 2.3099\n",
      "Iteration 675500, Train Loss: 2.0388, Validation Loss: 2.3504\n",
      "Iteration 675600, Train Loss: 2.8753, Validation Loss: 2.3337\n",
      "Iteration 675700, Train Loss: 2.6930, Validation Loss: 2.6068\n",
      "Iteration 675800, Train Loss: 1.9112, Validation Loss: 2.1315\n",
      "Iteration 675900, Train Loss: 1.6752, Validation Loss: 2.3925\n",
      "Iteration 676000, Train Loss: 2.4529, Validation Loss: 2.5505\n",
      "Iteration 676100, Train Loss: 2.4845, Validation Loss: 2.4083\n",
      "Iteration 676200, Train Loss: 2.2614, Validation Loss: 2.1804\n",
      "Iteration 676300, Train Loss: 2.1533, Validation Loss: 2.3011\n",
      "Iteration 676400, Train Loss: 2.4185, Validation Loss: 2.1710\n",
      "Iteration 676500, Train Loss: 2.3907, Validation Loss: 1.9495\n",
      "Iteration 676600, Train Loss: 2.2483, Validation Loss: 2.3950\n",
      "Iteration 676700, Train Loss: 1.7967, Validation Loss: 2.2039\n",
      "Iteration 676800, Train Loss: 2.4082, Validation Loss: 2.3922\n",
      "Iteration 676900, Train Loss: 1.9497, Validation Loss: 2.7377\n",
      "Iteration 677000, Train Loss: 2.4647, Validation Loss: 1.9883\n",
      "Iteration 677100, Train Loss: 1.5534, Validation Loss: 2.0124\n",
      "Iteration 677200, Train Loss: 3.0522, Validation Loss: 2.0285\n",
      "Iteration 677300, Train Loss: 1.6862, Validation Loss: 2.2724\n",
      "Iteration 677400, Train Loss: 2.1608, Validation Loss: 2.4301\n",
      "Iteration 677500, Train Loss: 1.9032, Validation Loss: 2.2954\n",
      "Iteration 677600, Train Loss: 2.2944, Validation Loss: 2.3901\n",
      "Iteration 677700, Train Loss: 2.0608, Validation Loss: 1.7075\n",
      "Iteration 677800, Train Loss: 2.1365, Validation Loss: 2.3283\n",
      "Iteration 677900, Train Loss: 2.1905, Validation Loss: 2.2030\n",
      "Iteration 678000, Train Loss: 2.5534, Validation Loss: 2.5381\n",
      "Iteration 678100, Train Loss: 2.2837, Validation Loss: 2.5515\n",
      "Iteration 678200, Train Loss: 1.8766, Validation Loss: 1.9344\n",
      "Iteration 678300, Train Loss: 1.9940, Validation Loss: 2.1205\n",
      "Iteration 678400, Train Loss: 1.9291, Validation Loss: 2.1177\n",
      "Iteration 678500, Train Loss: 1.9224, Validation Loss: 2.1263\n",
      "Iteration 678600, Train Loss: 2.4199, Validation Loss: 2.5146\n",
      "Iteration 678700, Train Loss: 2.8668, Validation Loss: 2.7355\n",
      "Iteration 678800, Train Loss: 2.5459, Validation Loss: 2.0060\n",
      "Iteration 678900, Train Loss: 2.3240, Validation Loss: 2.2705\n",
      "Iteration 679000, Train Loss: 2.0262, Validation Loss: 2.2042\n",
      "Iteration 679100, Train Loss: 2.5144, Validation Loss: 2.2617\n",
      "Iteration 679200, Train Loss: 2.5152, Validation Loss: 2.0680\n",
      "Iteration 679300, Train Loss: 2.2743, Validation Loss: 2.4111\n",
      "Iteration 679400, Train Loss: 2.2964, Validation Loss: 2.1514\n",
      "Iteration 679500, Train Loss: 1.9690, Validation Loss: 2.6469\n",
      "Iteration 679600, Train Loss: 2.6026, Validation Loss: 2.6264\n",
      "Iteration 679700, Train Loss: 2.2929, Validation Loss: 2.1124\n",
      "Iteration 679800, Train Loss: 2.1626, Validation Loss: 3.0545\n",
      "Iteration 679900, Train Loss: 1.8973, Validation Loss: 2.2962\n",
      "Iteration 680000, Train Loss: 2.3766, Validation Loss: 2.1683\n",
      "Iteration 680100, Train Loss: 2.2234, Validation Loss: 2.1655\n",
      "Iteration 680200, Train Loss: 2.1375, Validation Loss: 2.8963\n",
      "Iteration 680300, Train Loss: 2.1582, Validation Loss: 2.3079\n",
      "Iteration 680400, Train Loss: 2.3625, Validation Loss: 2.2495\n",
      "Iteration 680500, Train Loss: 2.4926, Validation Loss: 2.2430\n",
      "Iteration 680600, Train Loss: 2.0178, Validation Loss: 2.4334\n",
      "Iteration 680700, Train Loss: 2.0610, Validation Loss: 2.3198\n",
      "Iteration 680800, Train Loss: 1.9827, Validation Loss: 2.3189\n",
      "Iteration 680900, Train Loss: 2.3472, Validation Loss: 2.1511\n",
      "Iteration 681000, Train Loss: 2.3378, Validation Loss: 2.7093\n",
      "Iteration 681100, Train Loss: 1.8880, Validation Loss: 2.0398\n",
      "Iteration 681200, Train Loss: 2.3871, Validation Loss: 2.0802\n",
      "Iteration 681300, Train Loss: 2.4626, Validation Loss: 2.2972\n",
      "Iteration 681400, Train Loss: 2.5414, Validation Loss: 2.6704\n",
      "Iteration 681500, Train Loss: 2.4859, Validation Loss: 2.2078\n",
      "Iteration 681600, Train Loss: 2.3606, Validation Loss: 2.3142\n",
      "Iteration 681700, Train Loss: 2.6249, Validation Loss: 2.4646\n",
      "Iteration 681800, Train Loss: 2.5291, Validation Loss: 2.6823\n",
      "Iteration 681900, Train Loss: 2.3366, Validation Loss: 2.3931\n",
      "Iteration 682000, Train Loss: 2.3405, Validation Loss: 2.1985\n",
      "Iteration 682100, Train Loss: 2.1294, Validation Loss: 2.4438\n",
      "Iteration 682200, Train Loss: 2.1701, Validation Loss: 2.5580\n",
      "Iteration 682300, Train Loss: 2.1297, Validation Loss: 2.4204\n",
      "Iteration 682400, Train Loss: 2.7342, Validation Loss: 2.1284\n",
      "Iteration 682500, Train Loss: 2.0230, Validation Loss: 2.3849\n",
      "Iteration 682600, Train Loss: 1.9222, Validation Loss: 2.5779\n",
      "Iteration 682700, Train Loss: 2.2703, Validation Loss: 2.3836\n",
      "Iteration 682800, Train Loss: 2.1817, Validation Loss: 2.7308\n",
      "Iteration 682900, Train Loss: 2.0634, Validation Loss: 2.5764\n",
      "Iteration 683000, Train Loss: 2.0202, Validation Loss: 2.3118\n",
      "Iteration 683100, Train Loss: 2.2529, Validation Loss: 2.1881\n",
      "Iteration 683200, Train Loss: 2.4074, Validation Loss: 2.3967\n",
      "Iteration 683300, Train Loss: 2.1891, Validation Loss: 2.4653\n",
      "Iteration 683400, Train Loss: 2.8671, Validation Loss: 2.5193\n",
      "Iteration 683500, Train Loss: 2.2900, Validation Loss: 1.8942\n",
      "Iteration 683600, Train Loss: 2.1244, Validation Loss: 1.9754\n",
      "Iteration 683700, Train Loss: 2.3621, Validation Loss: 1.9844\n",
      "Iteration 683800, Train Loss: 2.0124, Validation Loss: 2.6840\n",
      "Iteration 683900, Train Loss: 2.4543, Validation Loss: 2.6022\n",
      "Iteration 684000, Train Loss: 2.4341, Validation Loss: 2.4930\n",
      "Iteration 684100, Train Loss: 2.1759, Validation Loss: 2.4291\n",
      "Iteration 684200, Train Loss: 2.1737, Validation Loss: 2.3604\n",
      "Iteration 684300, Train Loss: 2.2178, Validation Loss: 2.1446\n",
      "Iteration 684400, Train Loss: 1.9922, Validation Loss: 2.6214\n",
      "Iteration 684500, Train Loss: 2.3633, Validation Loss: 2.1128\n",
      "Iteration 684600, Train Loss: 2.3126, Validation Loss: 1.8755\n",
      "Iteration 684700, Train Loss: 2.3500, Validation Loss: 2.4957\n",
      "Iteration 684800, Train Loss: 2.2453, Validation Loss: 2.0170\n",
      "Iteration 684900, Train Loss: 2.9033, Validation Loss: 2.8213\n",
      "Iteration 685000, Train Loss: 2.2714, Validation Loss: 2.4343\n",
      "Iteration 685100, Train Loss: 1.9858, Validation Loss: 2.1710\n",
      "Iteration 685200, Train Loss: 2.7093, Validation Loss: 2.3577\n",
      "Iteration 685300, Train Loss: 2.4816, Validation Loss: 1.8555\n",
      "Iteration 685400, Train Loss: 2.3101, Validation Loss: 2.1457\n",
      "Iteration 685500, Train Loss: 1.7327, Validation Loss: 2.4930\n",
      "Iteration 685600, Train Loss: 2.2014, Validation Loss: 2.6294\n",
      "Iteration 685700, Train Loss: 2.6546, Validation Loss: 2.2780\n",
      "Iteration 685800, Train Loss: 2.5251, Validation Loss: 1.9733\n",
      "Iteration 685900, Train Loss: 2.3690, Validation Loss: 2.2009\n",
      "Iteration 686000, Train Loss: 2.1710, Validation Loss: 1.9677\n",
      "Iteration 686100, Train Loss: 2.5160, Validation Loss: 2.4161\n",
      "Iteration 686200, Train Loss: 2.3768, Validation Loss: 2.1356\n",
      "Iteration 686300, Train Loss: 2.1506, Validation Loss: 2.1998\n",
      "Iteration 686400, Train Loss: 2.1592, Validation Loss: 2.1146\n",
      "Iteration 686500, Train Loss: 2.6024, Validation Loss: 1.9714\n",
      "Iteration 686600, Train Loss: 1.9789, Validation Loss: 2.2358\n",
      "Iteration 686700, Train Loss: 2.3903, Validation Loss: 2.2878\n",
      "Iteration 686800, Train Loss: 2.3118, Validation Loss: 2.1407\n",
      "Iteration 686900, Train Loss: 2.0233, Validation Loss: 2.0994\n",
      "Iteration 687000, Train Loss: 1.9797, Validation Loss: 3.1326\n",
      "Iteration 687100, Train Loss: 2.3027, Validation Loss: 2.5554\n",
      "Iteration 687200, Train Loss: 2.8024, Validation Loss: 2.4048\n",
      "Iteration 687300, Train Loss: 2.5359, Validation Loss: 2.1348\n",
      "Iteration 687400, Train Loss: 2.4665, Validation Loss: 2.4547\n",
      "Iteration 687500, Train Loss: 2.1134, Validation Loss: 2.5469\n",
      "Iteration 687600, Train Loss: 2.4983, Validation Loss: 2.9326\n",
      "Iteration 687700, Train Loss: 1.9358, Validation Loss: 1.8291\n",
      "Iteration 687800, Train Loss: 2.1075, Validation Loss: 2.0696\n",
      "Iteration 687900, Train Loss: 2.6178, Validation Loss: 2.1154\n",
      "Iteration 688000, Train Loss: 1.9351, Validation Loss: 2.3376\n",
      "Iteration 688100, Train Loss: 2.5034, Validation Loss: 2.0591\n",
      "Iteration 688200, Train Loss: 2.1925, Validation Loss: 2.2095\n",
      "Iteration 688300, Train Loss: 2.3111, Validation Loss: 2.1673\n",
      "Iteration 688400, Train Loss: 2.0017, Validation Loss: 2.1824\n",
      "Iteration 688500, Train Loss: 2.5829, Validation Loss: 1.8910\n",
      "Iteration 688600, Train Loss: 2.7830, Validation Loss: 2.1153\n",
      "Iteration 688700, Train Loss: 2.3078, Validation Loss: 1.7598\n",
      "Iteration 688800, Train Loss: 2.7383, Validation Loss: 2.4697\n",
      "Iteration 688900, Train Loss: 2.2720, Validation Loss: 2.6499\n",
      "Iteration 689000, Train Loss: 2.2583, Validation Loss: 2.1662\n",
      "Iteration 689100, Train Loss: 2.1822, Validation Loss: 2.5654\n",
      "Iteration 689200, Train Loss: 1.8888, Validation Loss: 2.3517\n",
      "Iteration 689300, Train Loss: 1.8423, Validation Loss: 2.3409\n",
      "Iteration 689400, Train Loss: 2.2834, Validation Loss: 2.3721\n",
      "Iteration 689500, Train Loss: 2.4089, Validation Loss: 1.8265\n",
      "Iteration 689600, Train Loss: 2.1518, Validation Loss: 1.9474\n",
      "Iteration 689700, Train Loss: 2.1428, Validation Loss: 2.2995\n",
      "Iteration 689800, Train Loss: 2.2703, Validation Loss: 2.4183\n",
      "Iteration 689900, Train Loss: 2.1337, Validation Loss: 2.2432\n",
      "Iteration 690000, Train Loss: 2.1187, Validation Loss: 2.4495\n",
      "Iteration 690100, Train Loss: 1.5200, Validation Loss: 2.1533\n",
      "Iteration 690200, Train Loss: 2.2627, Validation Loss: 2.5412\n",
      "Iteration 690300, Train Loss: 2.1393, Validation Loss: 2.2254\n",
      "Iteration 690400, Train Loss: 1.9993, Validation Loss: 2.4430\n",
      "Iteration 690500, Train Loss: 1.8730, Validation Loss: 2.2909\n",
      "Iteration 690600, Train Loss: 2.2818, Validation Loss: 1.7907\n",
      "Iteration 690700, Train Loss: 2.2544, Validation Loss: 2.4700\n",
      "Iteration 690800, Train Loss: 2.1272, Validation Loss: 2.1938\n",
      "Iteration 690900, Train Loss: 2.4849, Validation Loss: 2.3544\n",
      "Iteration 691000, Train Loss: 2.2310, Validation Loss: 2.2536\n",
      "Iteration 691100, Train Loss: 1.7703, Validation Loss: 2.8793\n",
      "Iteration 691200, Train Loss: 2.1885, Validation Loss: 2.1403\n",
      "Iteration 691300, Train Loss: 2.4597, Validation Loss: 2.3386\n",
      "Iteration 691400, Train Loss: 2.4162, Validation Loss: 2.3655\n",
      "Iteration 691500, Train Loss: 2.1451, Validation Loss: 2.2548\n",
      "Iteration 691600, Train Loss: 2.3477, Validation Loss: 2.0634\n",
      "Iteration 691700, Train Loss: 2.6408, Validation Loss: 2.4267\n",
      "Iteration 691800, Train Loss: 1.9735, Validation Loss: 2.6915\n",
      "Iteration 691900, Train Loss: 2.0411, Validation Loss: 1.7605\n",
      "Iteration 692000, Train Loss: 1.7680, Validation Loss: 2.1403\n",
      "Iteration 692100, Train Loss: 2.2244, Validation Loss: 2.1993\n",
      "Iteration 692200, Train Loss: 2.3345, Validation Loss: 2.6022\n",
      "Iteration 692300, Train Loss: 2.0107, Validation Loss: 2.3665\n",
      "Iteration 692400, Train Loss: 1.7685, Validation Loss: 2.2833\n",
      "Iteration 692500, Train Loss: 2.3522, Validation Loss: 2.4482\n",
      "Iteration 692600, Train Loss: 2.4677, Validation Loss: 2.4902\n",
      "Iteration 692700, Train Loss: 2.0571, Validation Loss: 2.3538\n",
      "Iteration 692800, Train Loss: 2.1397, Validation Loss: 2.1783\n",
      "Iteration 692900, Train Loss: 2.7696, Validation Loss: 2.3736\n",
      "Iteration 693000, Train Loss: 2.2014, Validation Loss: 2.7774\n",
      "Iteration 693100, Train Loss: 2.5364, Validation Loss: 2.1371\n",
      "Iteration 693200, Train Loss: 2.4159, Validation Loss: 2.2893\n",
      "Iteration 693300, Train Loss: 2.4311, Validation Loss: 2.2665\n",
      "Iteration 693400, Train Loss: 2.6062, Validation Loss: 2.0358\n",
      "Iteration 693500, Train Loss: 1.8971, Validation Loss: 1.9125\n",
      "Iteration 693600, Train Loss: 2.4483, Validation Loss: 2.1044\n",
      "Iteration 693700, Train Loss: 2.3182, Validation Loss: 2.4030\n",
      "Iteration 693800, Train Loss: 2.1805, Validation Loss: 2.4038\n",
      "Iteration 693900, Train Loss: 2.4493, Validation Loss: 2.3106\n",
      "Iteration 694000, Train Loss: 2.0819, Validation Loss: 1.9588\n",
      "Iteration 694100, Train Loss: 1.8465, Validation Loss: 2.2375\n",
      "Iteration 694200, Train Loss: 2.1592, Validation Loss: 2.1097\n",
      "Iteration 694300, Train Loss: 2.2670, Validation Loss: 2.4231\n",
      "Iteration 694400, Train Loss: 2.2129, Validation Loss: 2.2780\n",
      "Iteration 694500, Train Loss: 1.5770, Validation Loss: 2.3039\n",
      "Iteration 694600, Train Loss: 1.9859, Validation Loss: 2.0072\n",
      "Iteration 694700, Train Loss: 2.4006, Validation Loss: 2.5543\n",
      "Iteration 694800, Train Loss: 2.1316, Validation Loss: 2.7635\n",
      "Iteration 694900, Train Loss: 2.5422, Validation Loss: 2.3938\n",
      "Iteration 695000, Train Loss: 2.1143, Validation Loss: 2.4796\n",
      "Iteration 695100, Train Loss: 2.0073, Validation Loss: 2.0538\n",
      "Iteration 695200, Train Loss: 2.3018, Validation Loss: 2.0024\n",
      "Iteration 695300, Train Loss: 2.3151, Validation Loss: 2.0481\n",
      "Iteration 695400, Train Loss: 2.4097, Validation Loss: 2.2445\n",
      "Iteration 695500, Train Loss: 2.2448, Validation Loss: 2.4500\n",
      "Iteration 695600, Train Loss: 1.9697, Validation Loss: 2.0015\n",
      "Iteration 695700, Train Loss: 2.1446, Validation Loss: 2.5604\n",
      "Iteration 695800, Train Loss: 2.1977, Validation Loss: 2.3107\n",
      "Iteration 695900, Train Loss: 2.4058, Validation Loss: 2.0515\n",
      "Iteration 696000, Train Loss: 2.5060, Validation Loss: 2.4818\n",
      "Iteration 696100, Train Loss: 2.5821, Validation Loss: 2.5421\n",
      "Iteration 696200, Train Loss: 2.5639, Validation Loss: 1.9812\n",
      "Iteration 696300, Train Loss: 2.5406, Validation Loss: 2.1989\n",
      "Iteration 696400, Train Loss: 2.3111, Validation Loss: 2.1307\n",
      "Iteration 696500, Train Loss: 2.6300, Validation Loss: 2.6494\n",
      "Iteration 696600, Train Loss: 2.2885, Validation Loss: 1.8607\n",
      "Iteration 696700, Train Loss: 2.9699, Validation Loss: 2.6504\n",
      "Iteration 696800, Train Loss: 1.9303, Validation Loss: 2.0233\n",
      "Iteration 696900, Train Loss: 2.1877, Validation Loss: 2.0796\n",
      "Iteration 697000, Train Loss: 2.5592, Validation Loss: 1.8773\n",
      "Iteration 697100, Train Loss: 2.2592, Validation Loss: 1.9021\n",
      "Iteration 697200, Train Loss: 2.3498, Validation Loss: 2.2621\n",
      "Iteration 697300, Train Loss: 2.2938, Validation Loss: 2.2825\n",
      "Iteration 697400, Train Loss: 2.0408, Validation Loss: 2.1357\n",
      "Iteration 697500, Train Loss: 2.2723, Validation Loss: 2.5188\n",
      "Iteration 697600, Train Loss: 1.7867, Validation Loss: 2.2562\n",
      "Iteration 697700, Train Loss: 2.6234, Validation Loss: 2.5016\n",
      "Iteration 697800, Train Loss: 2.3713, Validation Loss: 2.3899\n",
      "Iteration 697900, Train Loss: 1.8302, Validation Loss: 1.9849\n",
      "Iteration 698000, Train Loss: 2.1559, Validation Loss: 1.9764\n",
      "Iteration 698100, Train Loss: 2.2499, Validation Loss: 2.4487\n",
      "Iteration 698200, Train Loss: 2.6406, Validation Loss: 2.1613\n",
      "Iteration 698300, Train Loss: 1.9247, Validation Loss: 2.2457\n",
      "Iteration 698400, Train Loss: 1.9604, Validation Loss: 1.8117\n",
      "Iteration 698500, Train Loss: 2.3689, Validation Loss: 2.5057\n",
      "Iteration 698600, Train Loss: 2.5697, Validation Loss: 2.3383\n",
      "Iteration 698700, Train Loss: 2.0286, Validation Loss: 2.4102\n",
      "Iteration 698800, Train Loss: 2.2613, Validation Loss: 2.4053\n",
      "Iteration 698900, Train Loss: 2.3205, Validation Loss: 2.3744\n",
      "Iteration 699000, Train Loss: 2.4921, Validation Loss: 2.7703\n",
      "Iteration 699100, Train Loss: 1.9738, Validation Loss: 2.5215\n",
      "Iteration 699200, Train Loss: 2.3484, Validation Loss: 2.4550\n",
      "Iteration 699300, Train Loss: 2.3358, Validation Loss: 2.5052\n",
      "Iteration 699400, Train Loss: 2.2028, Validation Loss: 2.3830\n",
      "Iteration 699500, Train Loss: 2.0286, Validation Loss: 2.5409\n",
      "Iteration 699600, Train Loss: 2.2436, Validation Loss: 2.1472\n",
      "Iteration 699700, Train Loss: 2.1707, Validation Loss: 2.1985\n",
      "Iteration 699800, Train Loss: 1.8530, Validation Loss: 2.0067\n",
      "Iteration 699900, Train Loss: 1.9009, Validation Loss: 1.9934\n",
      "Iteration 700000, Train Loss: 2.1064, Validation Loss: 2.1577\n",
      "Iteration 700100, Train Loss: 1.9568, Validation Loss: 2.6364\n",
      "Iteration 700200, Train Loss: 1.8555, Validation Loss: 2.1909\n",
      "Iteration 700300, Train Loss: 1.8038, Validation Loss: 2.3531\n",
      "Iteration 700400, Train Loss: 2.7184, Validation Loss: 2.3106\n",
      "Iteration 700500, Train Loss: 2.5605, Validation Loss: 2.1240\n",
      "Iteration 700600, Train Loss: 2.3437, Validation Loss: 2.3634\n",
      "Iteration 700700, Train Loss: 2.6291, Validation Loss: 2.4538\n",
      "Iteration 700800, Train Loss: 2.3568, Validation Loss: 2.1987\n",
      "Iteration 700900, Train Loss: 2.3799, Validation Loss: 2.6918\n",
      "Iteration 701000, Train Loss: 1.5387, Validation Loss: 2.3503\n",
      "Iteration 701100, Train Loss: 2.3389, Validation Loss: 2.3268\n",
      "Iteration 701200, Train Loss: 1.9244, Validation Loss: 2.6820\n",
      "Iteration 701300, Train Loss: 2.6577, Validation Loss: 2.4778\n",
      "Iteration 701400, Train Loss: 2.3925, Validation Loss: 2.2222\n",
      "Iteration 701500, Train Loss: 2.6467, Validation Loss: 2.4943\n",
      "Iteration 701600, Train Loss: 1.9365, Validation Loss: 2.2784\n",
      "Iteration 701700, Train Loss: 2.2827, Validation Loss: 2.3702\n",
      "Iteration 701800, Train Loss: 2.6406, Validation Loss: 1.9627\n",
      "Iteration 701900, Train Loss: 2.2154, Validation Loss: 2.6854\n",
      "Iteration 702000, Train Loss: 2.0437, Validation Loss: 2.4719\n",
      "Iteration 702100, Train Loss: 2.3895, Validation Loss: 2.7999\n",
      "Iteration 702200, Train Loss: 1.8037, Validation Loss: 2.6153\n",
      "Iteration 702300, Train Loss: 1.9770, Validation Loss: 2.1325\n",
      "Iteration 702400, Train Loss: 2.2102, Validation Loss: 2.6325\n",
      "Iteration 702500, Train Loss: 1.9606, Validation Loss: 2.2906\n",
      "Iteration 702600, Train Loss: 2.2219, Validation Loss: 2.3171\n",
      "Iteration 702700, Train Loss: 2.0219, Validation Loss: 1.8361\n",
      "Iteration 702800, Train Loss: 2.2711, Validation Loss: 2.4545\n",
      "Iteration 702900, Train Loss: 2.3250, Validation Loss: 2.2222\n",
      "Iteration 703000, Train Loss: 2.3517, Validation Loss: 2.7265\n",
      "Iteration 703100, Train Loss: 1.9368, Validation Loss: 2.8182\n",
      "Iteration 703200, Train Loss: 1.8710, Validation Loss: 2.2384\n",
      "Iteration 703300, Train Loss: 2.7219, Validation Loss: 2.0526\n",
      "Iteration 703400, Train Loss: 2.1193, Validation Loss: 2.2384\n",
      "Iteration 703500, Train Loss: 2.4921, Validation Loss: 1.8088\n",
      "Iteration 703600, Train Loss: 2.1884, Validation Loss: 2.2767\n",
      "Iteration 703700, Train Loss: 2.4530, Validation Loss: 2.4823\n",
      "Iteration 703800, Train Loss: 2.3618, Validation Loss: 2.1056\n",
      "Iteration 703900, Train Loss: 2.5150, Validation Loss: 2.1813\n",
      "Iteration 704000, Train Loss: 2.0097, Validation Loss: 2.2977\n",
      "Iteration 704100, Train Loss: 2.4291, Validation Loss: 2.2979\n",
      "Iteration 704200, Train Loss: 2.4931, Validation Loss: 2.5790\n",
      "Iteration 704300, Train Loss: 2.2303, Validation Loss: 1.9869\n",
      "Iteration 704400, Train Loss: 2.2561, Validation Loss: 3.0077\n",
      "Iteration 704500, Train Loss: 1.9606, Validation Loss: 2.2084\n",
      "Iteration 704600, Train Loss: 2.1719, Validation Loss: 2.4990\n",
      "Iteration 704700, Train Loss: 2.4369, Validation Loss: 2.2050\n",
      "Iteration 704800, Train Loss: 2.6108, Validation Loss: 2.0265\n",
      "Iteration 704900, Train Loss: 2.6057, Validation Loss: 2.4448\n",
      "Iteration 705000, Train Loss: 2.3152, Validation Loss: 2.4673\n",
      "Iteration 705100, Train Loss: 2.4219, Validation Loss: 2.4840\n",
      "Iteration 705200, Train Loss: 2.3453, Validation Loss: 2.0142\n",
      "Iteration 705300, Train Loss: 2.2590, Validation Loss: 2.3586\n",
      "Iteration 705400, Train Loss: 2.2857, Validation Loss: 2.1446\n",
      "Iteration 705500, Train Loss: 2.5621, Validation Loss: 2.6571\n",
      "Iteration 705600, Train Loss: 1.9303, Validation Loss: 2.0568\n",
      "Iteration 705700, Train Loss: 2.2822, Validation Loss: 2.0810\n",
      "Iteration 705800, Train Loss: 2.1817, Validation Loss: 2.2227\n",
      "Iteration 705900, Train Loss: 2.0732, Validation Loss: 2.1814\n",
      "Iteration 706000, Train Loss: 2.1642, Validation Loss: 2.3332\n",
      "Iteration 706100, Train Loss: 2.2558, Validation Loss: 2.3589\n",
      "Iteration 706200, Train Loss: 2.1919, Validation Loss: 2.3588\n",
      "Iteration 706300, Train Loss: 2.4892, Validation Loss: 2.2833\n",
      "Iteration 706400, Train Loss: 2.1400, Validation Loss: 2.1330\n",
      "Iteration 706500, Train Loss: 1.7250, Validation Loss: 1.8040\n",
      "Iteration 706600, Train Loss: 2.2421, Validation Loss: 2.5375\n",
      "Iteration 706700, Train Loss: 2.3611, Validation Loss: 2.3187\n",
      "Iteration 706800, Train Loss: 2.0436, Validation Loss: 2.7535\n",
      "Iteration 706900, Train Loss: 2.1424, Validation Loss: 2.5225\n",
      "Iteration 707000, Train Loss: 2.3682, Validation Loss: 2.3839\n",
      "Iteration 707100, Train Loss: 2.4911, Validation Loss: 2.1875\n",
      "Iteration 707200, Train Loss: 2.1517, Validation Loss: 2.1171\n",
      "Iteration 707300, Train Loss: 1.9104, Validation Loss: 2.0536\n",
      "Iteration 707400, Train Loss: 1.7509, Validation Loss: 2.3155\n",
      "Iteration 707500, Train Loss: 1.8643, Validation Loss: 1.8615\n",
      "Iteration 707600, Train Loss: 2.3583, Validation Loss: 2.0279\n",
      "Iteration 707700, Train Loss: 1.9821, Validation Loss: 2.1261\n",
      "Iteration 707800, Train Loss: 2.3040, Validation Loss: 2.0048\n",
      "Iteration 707900, Train Loss: 2.0013, Validation Loss: 2.0393\n",
      "Iteration 708000, Train Loss: 1.9535, Validation Loss: 2.8387\n",
      "Iteration 708100, Train Loss: 2.2512, Validation Loss: 2.5110\n",
      "Iteration 708200, Train Loss: 2.4092, Validation Loss: 2.4614\n",
      "Iteration 708300, Train Loss: 2.5622, Validation Loss: 2.1469\n",
      "Iteration 708400, Train Loss: 2.2388, Validation Loss: 2.2912\n",
      "Iteration 708500, Train Loss: 2.3856, Validation Loss: 1.9266\n",
      "Iteration 708600, Train Loss: 2.4119, Validation Loss: 2.8508\n",
      "Iteration 708700, Train Loss: 2.0918, Validation Loss: 2.2198\n",
      "Iteration 708800, Train Loss: 2.1897, Validation Loss: 2.6703\n",
      "Iteration 708900, Train Loss: 1.9751, Validation Loss: 2.1614\n",
      "Iteration 709000, Train Loss: 2.6381, Validation Loss: 2.2250\n",
      "Iteration 709100, Train Loss: 2.3743, Validation Loss: 2.3971\n",
      "Iteration 709200, Train Loss: 2.2121, Validation Loss: 2.2757\n",
      "Iteration 709300, Train Loss: 2.3667, Validation Loss: 2.6811\n",
      "Iteration 709400, Train Loss: 2.1554, Validation Loss: 2.1326\n",
      "Iteration 709500, Train Loss: 1.9423, Validation Loss: 2.2333\n",
      "Iteration 709600, Train Loss: 2.2400, Validation Loss: 2.3010\n",
      "Iteration 709700, Train Loss: 1.9195, Validation Loss: 2.3410\n",
      "Iteration 709800, Train Loss: 2.4103, Validation Loss: 2.0869\n",
      "Iteration 709900, Train Loss: 2.0988, Validation Loss: 2.2764\n",
      "Iteration 710000, Train Loss: 2.5500, Validation Loss: 2.1339\n",
      "Iteration 710100, Train Loss: 2.3631, Validation Loss: 2.2830\n",
      "Iteration 710200, Train Loss: 1.6666, Validation Loss: 2.4324\n",
      "Iteration 710300, Train Loss: 2.0744, Validation Loss: 2.7212\n",
      "Iteration 710400, Train Loss: 2.1810, Validation Loss: 2.0809\n",
      "Iteration 710500, Train Loss: 2.6501, Validation Loss: 2.6717\n",
      "Iteration 710600, Train Loss: 2.2644, Validation Loss: 2.1055\n",
      "Iteration 710700, Train Loss: 2.1809, Validation Loss: 1.8694\n",
      "Iteration 710800, Train Loss: 2.4542, Validation Loss: 2.3161\n",
      "Iteration 710900, Train Loss: 2.2482, Validation Loss: 2.4775\n",
      "Iteration 711000, Train Loss: 2.1715, Validation Loss: 2.5088\n",
      "Iteration 711100, Train Loss: 2.4880, Validation Loss: 2.3124\n",
      "Iteration 711200, Train Loss: 2.4000, Validation Loss: 2.2915\n",
      "Iteration 711300, Train Loss: 2.0796, Validation Loss: 2.6203\n",
      "Iteration 711400, Train Loss: 2.0658, Validation Loss: 2.5406\n",
      "Iteration 711500, Train Loss: 2.3416, Validation Loss: 2.1881\n",
      "Iteration 711600, Train Loss: 2.1393, Validation Loss: 2.3449\n",
      "Iteration 711700, Train Loss: 2.6237, Validation Loss: 2.2099\n",
      "Iteration 711800, Train Loss: 2.1537, Validation Loss: 2.1652\n",
      "Iteration 711900, Train Loss: 2.1287, Validation Loss: 2.1553\n",
      "Iteration 712000, Train Loss: 2.2553, Validation Loss: 2.6249\n",
      "Iteration 712100, Train Loss: 2.2382, Validation Loss: 2.3332\n",
      "Iteration 712200, Train Loss: 2.5061, Validation Loss: 2.3541\n",
      "Iteration 712300, Train Loss: 1.9144, Validation Loss: 2.0731\n",
      "Iteration 712400, Train Loss: 2.2077, Validation Loss: 2.2300\n",
      "Iteration 712500, Train Loss: 2.6567, Validation Loss: 2.6105\n",
      "Iteration 712600, Train Loss: 1.9774, Validation Loss: 1.9553\n",
      "Iteration 712700, Train Loss: 2.4576, Validation Loss: 2.1074\n",
      "Iteration 712800, Train Loss: 2.2915, Validation Loss: 2.3423\n",
      "Iteration 712900, Train Loss: 2.6589, Validation Loss: 2.4501\n",
      "Iteration 713000, Train Loss: 2.4315, Validation Loss: 2.0333\n",
      "Iteration 713100, Train Loss: 2.3518, Validation Loss: 2.2283\n",
      "Iteration 713200, Train Loss: 2.2221, Validation Loss: 2.1983\n",
      "Iteration 713300, Train Loss: 1.9917, Validation Loss: 2.2012\n",
      "Iteration 713400, Train Loss: 1.9222, Validation Loss: 2.0293\n",
      "Iteration 713500, Train Loss: 1.9821, Validation Loss: 2.5754\n",
      "Iteration 713600, Train Loss: 2.7040, Validation Loss: 2.0210\n",
      "Iteration 713700, Train Loss: 2.4037, Validation Loss: 2.1759\n",
      "Iteration 713800, Train Loss: 2.3074, Validation Loss: 2.1745\n",
      "Iteration 713900, Train Loss: 2.2573, Validation Loss: 2.4864\n",
      "Iteration 714000, Train Loss: 2.4228, Validation Loss: 2.2306\n",
      "Iteration 714100, Train Loss: 2.4273, Validation Loss: 2.3806\n",
      "Iteration 714200, Train Loss: 2.2977, Validation Loss: 2.2123\n",
      "Iteration 714300, Train Loss: 2.1586, Validation Loss: 1.8567\n",
      "Iteration 714400, Train Loss: 2.3618, Validation Loss: 1.9564\n",
      "Iteration 714500, Train Loss: 2.3035, Validation Loss: 1.8937\n",
      "Iteration 714600, Train Loss: 2.5533, Validation Loss: 2.2545\n",
      "Iteration 714700, Train Loss: 1.8395, Validation Loss: 2.5144\n",
      "Iteration 714800, Train Loss: 1.9308, Validation Loss: 2.0354\n",
      "Iteration 714900, Train Loss: 2.3292, Validation Loss: 2.2518\n",
      "Iteration 715000, Train Loss: 2.3611, Validation Loss: 2.0278\n",
      "Iteration 715100, Train Loss: 2.1506, Validation Loss: 2.2820\n",
      "Iteration 715200, Train Loss: 2.3249, Validation Loss: 2.8487\n",
      "Iteration 715300, Train Loss: 2.7743, Validation Loss: 1.8909\n",
      "Iteration 715400, Train Loss: 2.6414, Validation Loss: 1.9935\n",
      "Iteration 715500, Train Loss: 2.2418, Validation Loss: 2.2480\n",
      "Iteration 715600, Train Loss: 2.2483, Validation Loss: 2.1692\n",
      "Iteration 715700, Train Loss: 2.6397, Validation Loss: 2.1756\n",
      "Iteration 715800, Train Loss: 2.2340, Validation Loss: 2.1140\n",
      "Iteration 715900, Train Loss: 2.0790, Validation Loss: 1.9980\n",
      "Iteration 716000, Train Loss: 2.3542, Validation Loss: 1.7299\n",
      "Iteration 716100, Train Loss: 2.2137, Validation Loss: 2.2686\n",
      "Iteration 716200, Train Loss: 2.5404, Validation Loss: 2.2655\n",
      "Iteration 716300, Train Loss: 2.4681, Validation Loss: 1.9814\n",
      "Iteration 716400, Train Loss: 2.3989, Validation Loss: 2.1964\n",
      "Iteration 716500, Train Loss: 2.4782, Validation Loss: 2.0730\n",
      "Iteration 716600, Train Loss: 2.1539, Validation Loss: 2.3219\n",
      "Iteration 716700, Train Loss: 2.1399, Validation Loss: 2.4763\n",
      "Iteration 716800, Train Loss: 2.5124, Validation Loss: 2.0346\n",
      "Iteration 716900, Train Loss: 2.3478, Validation Loss: 2.4984\n",
      "Iteration 717000, Train Loss: 2.0280, Validation Loss: 2.6647\n",
      "Iteration 717100, Train Loss: 2.0819, Validation Loss: 2.3716\n",
      "Iteration 717200, Train Loss: 2.3563, Validation Loss: 2.1262\n",
      "Iteration 717300, Train Loss: 2.0481, Validation Loss: 2.0522\n",
      "Iteration 717400, Train Loss: 2.0348, Validation Loss: 2.0630\n",
      "Iteration 717500, Train Loss: 1.8468, Validation Loss: 2.9357\n",
      "Iteration 717600, Train Loss: 2.3541, Validation Loss: 2.2454\n",
      "Iteration 717700, Train Loss: 2.0947, Validation Loss: 2.2796\n",
      "Iteration 717800, Train Loss: 2.4734, Validation Loss: 1.9411\n",
      "Iteration 717900, Train Loss: 2.2019, Validation Loss: 2.5153\n",
      "Iteration 718000, Train Loss: 2.3250, Validation Loss: 2.1311\n",
      "Iteration 718100, Train Loss: 1.9866, Validation Loss: 2.2444\n",
      "Iteration 718200, Train Loss: 2.7556, Validation Loss: 2.2624\n",
      "Iteration 718300, Train Loss: 2.3745, Validation Loss: 2.2529\n",
      "Iteration 718400, Train Loss: 1.7549, Validation Loss: 2.3391\n",
      "Iteration 718500, Train Loss: 2.4035, Validation Loss: 2.5070\n",
      "Iteration 718600, Train Loss: 2.7761, Validation Loss: 2.6103\n",
      "Iteration 718700, Train Loss: 2.2318, Validation Loss: 2.1986\n",
      "Iteration 718800, Train Loss: 2.2206, Validation Loss: 2.4340\n",
      "Iteration 718900, Train Loss: 2.3263, Validation Loss: 2.1116\n",
      "Iteration 719000, Train Loss: 2.0591, Validation Loss: 2.7719\n",
      "Iteration 719100, Train Loss: 1.8661, Validation Loss: 2.4681\n",
      "Iteration 719200, Train Loss: 2.2066, Validation Loss: 1.9655\n",
      "Iteration 719300, Train Loss: 2.3992, Validation Loss: 2.3814\n",
      "Iteration 719400, Train Loss: 2.5165, Validation Loss: 2.1364\n",
      "Iteration 719500, Train Loss: 2.4074, Validation Loss: 2.1519\n",
      "Iteration 719600, Train Loss: 2.2064, Validation Loss: 2.2527\n",
      "Iteration 719700, Train Loss: 1.9687, Validation Loss: 2.3516\n",
      "Iteration 719800, Train Loss: 2.1174, Validation Loss: 2.3847\n",
      "Iteration 719900, Train Loss: 2.5141, Validation Loss: 2.1984\n",
      "Iteration 720000, Train Loss: 2.4206, Validation Loss: 2.3986\n",
      "Iteration 720100, Train Loss: 2.4700, Validation Loss: 2.2642\n",
      "Iteration 720200, Train Loss: 2.6218, Validation Loss: 2.1698\n",
      "Iteration 720300, Train Loss: 2.1668, Validation Loss: 2.0057\n",
      "Iteration 720400, Train Loss: 2.2532, Validation Loss: 2.6722\n",
      "Iteration 720500, Train Loss: 2.0289, Validation Loss: 2.4176\n",
      "Iteration 720600, Train Loss: 2.2074, Validation Loss: 2.2554\n",
      "Iteration 720700, Train Loss: 2.3141, Validation Loss: 2.7416\n",
      "Iteration 720800, Train Loss: 2.0581, Validation Loss: 2.2231\n",
      "Iteration 720900, Train Loss: 2.4866, Validation Loss: 2.7881\n",
      "Iteration 721000, Train Loss: 2.0871, Validation Loss: 2.5387\n",
      "Iteration 721100, Train Loss: 2.0413, Validation Loss: 2.0743\n",
      "Iteration 721200, Train Loss: 2.2354, Validation Loss: 2.0681\n",
      "Iteration 721300, Train Loss: 2.1941, Validation Loss: 1.9121\n",
      "Iteration 721400, Train Loss: 2.0682, Validation Loss: 2.2315\n",
      "Iteration 721500, Train Loss: 2.2948, Validation Loss: 3.0248\n",
      "Iteration 721600, Train Loss: 2.2034, Validation Loss: 2.1967\n",
      "Iteration 721700, Train Loss: 2.2247, Validation Loss: 2.0382\n",
      "Iteration 721800, Train Loss: 1.9456, Validation Loss: 2.2521\n",
      "Iteration 721900, Train Loss: 2.0725, Validation Loss: 2.6036\n",
      "Iteration 722000, Train Loss: 1.9601, Validation Loss: 2.0534\n",
      "Iteration 722100, Train Loss: 2.4439, Validation Loss: 2.4697\n",
      "Iteration 722200, Train Loss: 2.3314, Validation Loss: 2.3103\n",
      "Iteration 722300, Train Loss: 2.1602, Validation Loss: 1.8428\n",
      "Iteration 722400, Train Loss: 2.1029, Validation Loss: 2.2648\n",
      "Iteration 722500, Train Loss: 1.7673, Validation Loss: 2.2179\n",
      "Iteration 722600, Train Loss: 2.4722, Validation Loss: 2.4340\n",
      "Iteration 722700, Train Loss: 2.3218, Validation Loss: 2.2842\n",
      "Iteration 722800, Train Loss: 2.2453, Validation Loss: 2.0428\n",
      "Iteration 722900, Train Loss: 2.0699, Validation Loss: 2.0137\n",
      "Iteration 723000, Train Loss: 2.6084, Validation Loss: 2.2184\n",
      "Iteration 723100, Train Loss: 2.2642, Validation Loss: 2.6853\n",
      "Iteration 723200, Train Loss: 2.4974, Validation Loss: 2.1798\n",
      "Iteration 723300, Train Loss: 1.9395, Validation Loss: 2.3024\n",
      "Iteration 723400, Train Loss: 2.4792, Validation Loss: 2.0638\n",
      "Iteration 723500, Train Loss: 2.2657, Validation Loss: 2.4241\n",
      "Iteration 723600, Train Loss: 2.3852, Validation Loss: 2.1509\n",
      "Iteration 723700, Train Loss: 2.2403, Validation Loss: 2.1727\n",
      "Iteration 723800, Train Loss: 2.1066, Validation Loss: 2.4990\n",
      "Iteration 723900, Train Loss: 2.0993, Validation Loss: 2.2816\n",
      "Iteration 724000, Train Loss: 2.4497, Validation Loss: 2.6258\n",
      "Iteration 724100, Train Loss: 2.2794, Validation Loss: 2.2309\n",
      "Iteration 724200, Train Loss: 2.2647, Validation Loss: 2.0292\n",
      "Iteration 724300, Train Loss: 2.6166, Validation Loss: 2.5427\n",
      "Iteration 724400, Train Loss: 2.1773, Validation Loss: 2.1873\n",
      "Iteration 724500, Train Loss: 2.1549, Validation Loss: 1.9352\n",
      "Iteration 724600, Train Loss: 2.1110, Validation Loss: 2.3993\n",
      "Iteration 724700, Train Loss: 2.2500, Validation Loss: 2.3676\n",
      "Iteration 724800, Train Loss: 2.4978, Validation Loss: 2.2256\n",
      "Iteration 724900, Train Loss: 1.9877, Validation Loss: 1.6821\n",
      "Iteration 725000, Train Loss: 2.7439, Validation Loss: 2.4548\n",
      "Iteration 725100, Train Loss: 1.6330, Validation Loss: 2.3685\n",
      "Iteration 725200, Train Loss: 2.4251, Validation Loss: 1.9678\n",
      "Iteration 725300, Train Loss: 2.2572, Validation Loss: 1.8323\n",
      "Iteration 725400, Train Loss: 2.1317, Validation Loss: 1.4521\n",
      "Iteration 725500, Train Loss: 2.2018, Validation Loss: 2.4235\n",
      "Iteration 725600, Train Loss: 1.7687, Validation Loss: 2.4091\n",
      "Iteration 725700, Train Loss: 1.9125, Validation Loss: 1.8503\n",
      "Iteration 725800, Train Loss: 2.1992, Validation Loss: 1.8672\n",
      "Iteration 725900, Train Loss: 2.5118, Validation Loss: 2.6891\n",
      "Iteration 726000, Train Loss: 2.2227, Validation Loss: 2.6281\n",
      "Iteration 726100, Train Loss: 1.7925, Validation Loss: 2.3998\n",
      "Iteration 726200, Train Loss: 2.2494, Validation Loss: 2.4062\n",
      "Iteration 726300, Train Loss: 2.1626, Validation Loss: 2.3369\n",
      "Iteration 726400, Train Loss: 2.1693, Validation Loss: 2.6147\n",
      "Iteration 726500, Train Loss: 2.6490, Validation Loss: 1.7685\n",
      "Iteration 726600, Train Loss: 1.9802, Validation Loss: 2.7100\n",
      "Iteration 726700, Train Loss: 2.0834, Validation Loss: 2.5056\n",
      "Iteration 726800, Train Loss: 2.2334, Validation Loss: 2.3869\n",
      "Iteration 726900, Train Loss: 2.2943, Validation Loss: 1.9553\n",
      "Iteration 727000, Train Loss: 2.4496, Validation Loss: 1.4420\n",
      "Iteration 727100, Train Loss: 2.4042, Validation Loss: 2.4103\n",
      "Iteration 727200, Train Loss: 2.5627, Validation Loss: 2.4125\n",
      "Iteration 727300, Train Loss: 2.1519, Validation Loss: 2.3437\n",
      "Iteration 727400, Train Loss: 2.4897, Validation Loss: 2.9143\n",
      "Iteration 727500, Train Loss: 2.0445, Validation Loss: 2.2337\n",
      "Iteration 727600, Train Loss: 2.2753, Validation Loss: 1.9504\n",
      "Iteration 727700, Train Loss: 2.5219, Validation Loss: 2.2211\n",
      "Iteration 727800, Train Loss: 2.9234, Validation Loss: 2.2562\n",
      "Iteration 727900, Train Loss: 2.1040, Validation Loss: 2.1056\n",
      "Iteration 728000, Train Loss: 2.2925, Validation Loss: 2.6479\n",
      "Iteration 728100, Train Loss: 1.9560, Validation Loss: 2.4879\n",
      "Iteration 728200, Train Loss: 2.1954, Validation Loss: 2.3117\n",
      "Iteration 728300, Train Loss: 2.0109, Validation Loss: 2.0007\n",
      "Iteration 728400, Train Loss: 2.3016, Validation Loss: 2.4900\n",
      "Iteration 728500, Train Loss: 1.9826, Validation Loss: 1.9042\n",
      "Iteration 728600, Train Loss: 2.6414, Validation Loss: 1.8900\n",
      "Iteration 728700, Train Loss: 2.3405, Validation Loss: 2.2817\n",
      "Iteration 728800, Train Loss: 2.0726, Validation Loss: 2.2465\n",
      "Iteration 728900, Train Loss: 1.8967, Validation Loss: 2.9602\n",
      "Iteration 729000, Train Loss: 2.5688, Validation Loss: 2.3775\n",
      "Iteration 729100, Train Loss: 2.2387, Validation Loss: 2.5783\n",
      "Iteration 729200, Train Loss: 1.9591, Validation Loss: 2.0261\n",
      "Iteration 729300, Train Loss: 2.2939, Validation Loss: 2.0817\n",
      "Iteration 729400, Train Loss: 2.2356, Validation Loss: 2.2006\n",
      "Iteration 729500, Train Loss: 2.0140, Validation Loss: 2.7712\n",
      "Iteration 729600, Train Loss: 2.4546, Validation Loss: 2.3783\n",
      "Iteration 729700, Train Loss: 2.5954, Validation Loss: 2.0137\n",
      "Iteration 729800, Train Loss: 2.2791, Validation Loss: 2.5557\n",
      "Iteration 729900, Train Loss: 2.1637, Validation Loss: 2.5633\n",
      "Iteration 730000, Train Loss: 2.1623, Validation Loss: 2.2286\n",
      "Iteration 730100, Train Loss: 2.3911, Validation Loss: 2.2758\n",
      "Iteration 730200, Train Loss: 2.2501, Validation Loss: 2.4047\n",
      "Iteration 730300, Train Loss: 1.9283, Validation Loss: 2.4912\n",
      "Iteration 730400, Train Loss: 2.4817, Validation Loss: 2.4013\n",
      "Iteration 730500, Train Loss: 2.2862, Validation Loss: 1.9832\n",
      "Iteration 730600, Train Loss: 2.3204, Validation Loss: 2.4704\n",
      "Iteration 730700, Train Loss: 2.3721, Validation Loss: 2.0500\n",
      "Iteration 730800, Train Loss: 1.9675, Validation Loss: 2.8580\n",
      "Iteration 730900, Train Loss: 2.3086, Validation Loss: 2.6481\n",
      "Iteration 731000, Train Loss: 2.4345, Validation Loss: 2.0721\n",
      "Iteration 731100, Train Loss: 1.9606, Validation Loss: 2.0827\n",
      "Iteration 731200, Train Loss: 2.0024, Validation Loss: 2.0404\n",
      "Iteration 731300, Train Loss: 2.5708, Validation Loss: 2.5838\n",
      "Iteration 731400, Train Loss: 2.3716, Validation Loss: 1.9719\n",
      "Iteration 731500, Train Loss: 1.6916, Validation Loss: 2.3402\n",
      "Iteration 731600, Train Loss: 2.4268, Validation Loss: 2.3380\n",
      "Iteration 731700, Train Loss: 2.3191, Validation Loss: 2.5282\n",
      "Iteration 731800, Train Loss: 1.8425, Validation Loss: 2.0899\n",
      "Iteration 731900, Train Loss: 1.8738, Validation Loss: 2.0401\n",
      "Iteration 732000, Train Loss: 2.0055, Validation Loss: 2.3820\n",
      "Iteration 732100, Train Loss: 2.1087, Validation Loss: 2.4100\n",
      "Iteration 732200, Train Loss: 1.9955, Validation Loss: 2.3112\n",
      "Iteration 732300, Train Loss: 2.0762, Validation Loss: 1.9933\n",
      "Iteration 732400, Train Loss: 2.2912, Validation Loss: 2.6524\n",
      "Iteration 732500, Train Loss: 2.1001, Validation Loss: 2.2438\n",
      "Iteration 732600, Train Loss: 2.5898, Validation Loss: 2.5701\n",
      "Iteration 732700, Train Loss: 1.7205, Validation Loss: 2.0044\n",
      "Iteration 732800, Train Loss: 1.9816, Validation Loss: 2.5051\n",
      "Iteration 732900, Train Loss: 2.5003, Validation Loss: 1.8801\n",
      "Iteration 733000, Train Loss: 2.2070, Validation Loss: 2.1592\n",
      "Iteration 733100, Train Loss: 2.3378, Validation Loss: 2.3556\n",
      "Iteration 733200, Train Loss: 2.1391, Validation Loss: 2.2829\n",
      "Iteration 733300, Train Loss: 2.0722, Validation Loss: 2.4651\n",
      "Iteration 733400, Train Loss: 2.8526, Validation Loss: 1.9287\n",
      "Iteration 733500, Train Loss: 2.6128, Validation Loss: 2.5154\n",
      "Iteration 733600, Train Loss: 2.0180, Validation Loss: 2.4104\n",
      "Iteration 733700, Train Loss: 2.0931, Validation Loss: 2.4286\n",
      "Iteration 733800, Train Loss: 2.6824, Validation Loss: 2.1180\n",
      "Iteration 733900, Train Loss: 2.4725, Validation Loss: 2.6087\n",
      "Iteration 734000, Train Loss: 2.2914, Validation Loss: 1.9942\n",
      "Iteration 734100, Train Loss: 1.9826, Validation Loss: 2.2924\n",
      "Iteration 734200, Train Loss: 2.4012, Validation Loss: 2.2789\n",
      "Iteration 734300, Train Loss: 1.9556, Validation Loss: 2.2859\n",
      "Iteration 734400, Train Loss: 2.5450, Validation Loss: 2.4154\n",
      "Iteration 734500, Train Loss: 2.5882, Validation Loss: 2.5661\n",
      "Iteration 734600, Train Loss: 2.3850, Validation Loss: 2.9526\n",
      "Iteration 734700, Train Loss: 2.5795, Validation Loss: 2.8523\n",
      "Iteration 734800, Train Loss: 2.3340, Validation Loss: 2.7232\n",
      "Iteration 734900, Train Loss: 2.2461, Validation Loss: 2.4961\n",
      "Iteration 735000, Train Loss: 2.0808, Validation Loss: 2.0468\n",
      "Iteration 735100, Train Loss: 2.1696, Validation Loss: 2.2405\n",
      "Iteration 735200, Train Loss: 2.2934, Validation Loss: 2.1229\n",
      "Iteration 735300, Train Loss: 2.2714, Validation Loss: 2.5054\n",
      "Iteration 735400, Train Loss: 2.5356, Validation Loss: 2.3003\n",
      "Iteration 735500, Train Loss: 2.1841, Validation Loss: 2.2647\n",
      "Iteration 735600, Train Loss: 2.0288, Validation Loss: 2.1171\n",
      "Iteration 735700, Train Loss: 2.1348, Validation Loss: 2.2671\n",
      "Iteration 735800, Train Loss: 2.3925, Validation Loss: 1.6999\n",
      "Iteration 735900, Train Loss: 1.6923, Validation Loss: 2.4023\n",
      "Iteration 736000, Train Loss: 2.1501, Validation Loss: 2.3371\n",
      "Iteration 736100, Train Loss: 2.1473, Validation Loss: 2.5364\n",
      "Iteration 736200, Train Loss: 2.6586, Validation Loss: 2.0340\n",
      "Iteration 736300, Train Loss: 2.4785, Validation Loss: 2.4801\n",
      "Iteration 736400, Train Loss: 2.4292, Validation Loss: 2.1832\n",
      "Iteration 736500, Train Loss: 2.2591, Validation Loss: 2.2562\n",
      "Iteration 736600, Train Loss: 2.2150, Validation Loss: 2.0149\n",
      "Iteration 736700, Train Loss: 1.9143, Validation Loss: 2.3258\n",
      "Iteration 736800, Train Loss: 2.5439, Validation Loss: 2.2978\n",
      "Iteration 736900, Train Loss: 2.3270, Validation Loss: 2.3256\n",
      "Iteration 737000, Train Loss: 2.1201, Validation Loss: 2.5852\n",
      "Iteration 737100, Train Loss: 2.4001, Validation Loss: 2.0854\n",
      "Iteration 737200, Train Loss: 2.1090, Validation Loss: 2.5732\n",
      "Iteration 737300, Train Loss: 2.4419, Validation Loss: 1.9392\n",
      "Iteration 737400, Train Loss: 2.2849, Validation Loss: 2.2791\n",
      "Iteration 737500, Train Loss: 2.1895, Validation Loss: 2.3868\n",
      "Iteration 737600, Train Loss: 2.5142, Validation Loss: 2.2945\n",
      "Iteration 737700, Train Loss: 2.2742, Validation Loss: 2.8179\n",
      "Iteration 737800, Train Loss: 2.2912, Validation Loss: 2.2463\n",
      "Iteration 737900, Train Loss: 2.1847, Validation Loss: 2.4265\n",
      "Iteration 738000, Train Loss: 2.0870, Validation Loss: 2.2757\n",
      "Iteration 738100, Train Loss: 2.3365, Validation Loss: 2.0708\n",
      "Iteration 738200, Train Loss: 2.3096, Validation Loss: 2.1008\n",
      "Iteration 738300, Train Loss: 2.8951, Validation Loss: 2.4854\n",
      "Iteration 738400, Train Loss: 2.3080, Validation Loss: 2.4558\n",
      "Iteration 738500, Train Loss: 2.2290, Validation Loss: 2.3537\n",
      "Iteration 738600, Train Loss: 2.2717, Validation Loss: 2.2601\n",
      "Iteration 738700, Train Loss: 2.3569, Validation Loss: 1.9614\n",
      "Iteration 738800, Train Loss: 2.2854, Validation Loss: 2.7812\n",
      "Iteration 738900, Train Loss: 2.3400, Validation Loss: 2.2010\n",
      "Iteration 739000, Train Loss: 2.7388, Validation Loss: 2.3081\n",
      "Iteration 739100, Train Loss: 2.5530, Validation Loss: 1.7126\n",
      "Iteration 739200, Train Loss: 2.1188, Validation Loss: 2.0285\n",
      "Iteration 739300, Train Loss: 2.6637, Validation Loss: 2.4446\n",
      "Iteration 739400, Train Loss: 2.3111, Validation Loss: 2.3291\n",
      "Iteration 739500, Train Loss: 1.8701, Validation Loss: 2.0675\n",
      "Iteration 739600, Train Loss: 2.4243, Validation Loss: 2.6647\n",
      "Iteration 739700, Train Loss: 2.3363, Validation Loss: 2.2450\n",
      "Iteration 739800, Train Loss: 2.5019, Validation Loss: 1.9992\n",
      "Iteration 739900, Train Loss: 2.6441, Validation Loss: 2.1324\n",
      "Iteration 740000, Train Loss: 2.2064, Validation Loss: 2.0971\n",
      "Iteration 740100, Train Loss: 2.4136, Validation Loss: 1.7578\n",
      "Iteration 740200, Train Loss: 2.4707, Validation Loss: 2.3406\n",
      "Iteration 740300, Train Loss: 2.1757, Validation Loss: 2.0824\n",
      "Iteration 740400, Train Loss: 2.1731, Validation Loss: 2.0947\n",
      "Iteration 740500, Train Loss: 2.3147, Validation Loss: 2.3170\n",
      "Iteration 740600, Train Loss: 2.0413, Validation Loss: 2.3030\n",
      "Iteration 740700, Train Loss: 2.5190, Validation Loss: 2.1451\n",
      "Iteration 740800, Train Loss: 2.3683, Validation Loss: 2.3161\n",
      "Iteration 740900, Train Loss: 2.5380, Validation Loss: 2.6485\n",
      "Iteration 741000, Train Loss: 2.4390, Validation Loss: 2.0106\n",
      "Iteration 741100, Train Loss: 2.4493, Validation Loss: 2.4763\n",
      "Iteration 741200, Train Loss: 2.4266, Validation Loss: 2.1975\n",
      "Iteration 741300, Train Loss: 2.2716, Validation Loss: 2.5403\n",
      "Iteration 741400, Train Loss: 2.6860, Validation Loss: 2.1524\n",
      "Iteration 741500, Train Loss: 2.6100, Validation Loss: 2.4908\n",
      "Iteration 741600, Train Loss: 2.2649, Validation Loss: 2.1309\n",
      "Iteration 741700, Train Loss: 2.1265, Validation Loss: 1.8137\n",
      "Iteration 741800, Train Loss: 2.5902, Validation Loss: 2.2702\n",
      "Iteration 741900, Train Loss: 2.3929, Validation Loss: 1.9479\n",
      "Iteration 742000, Train Loss: 2.0235, Validation Loss: 2.4082\n",
      "Iteration 742100, Train Loss: 2.3701, Validation Loss: 1.9902\n",
      "Iteration 742200, Train Loss: 2.1284, Validation Loss: 1.9686\n",
      "Iteration 742300, Train Loss: 2.2021, Validation Loss: 2.0909\n",
      "Iteration 742400, Train Loss: 2.3339, Validation Loss: 2.2676\n",
      "Iteration 742500, Train Loss: 2.1644, Validation Loss: 2.1018\n",
      "Iteration 742600, Train Loss: 2.5388, Validation Loss: 2.3248\n",
      "Iteration 742700, Train Loss: 2.6825, Validation Loss: 1.8645\n",
      "Iteration 742800, Train Loss: 2.3850, Validation Loss: 2.4078\n",
      "Iteration 742900, Train Loss: 2.3036, Validation Loss: 2.2449\n",
      "Iteration 743000, Train Loss: 2.3021, Validation Loss: 2.3622\n",
      "Iteration 743100, Train Loss: 2.2767, Validation Loss: 2.2557\n",
      "Iteration 743200, Train Loss: 2.3368, Validation Loss: 2.2117\n",
      "Iteration 743300, Train Loss: 2.3276, Validation Loss: 2.1628\n",
      "Iteration 743400, Train Loss: 2.5088, Validation Loss: 1.7747\n",
      "Iteration 743500, Train Loss: 2.0441, Validation Loss: 2.1469\n",
      "Iteration 743600, Train Loss: 2.0971, Validation Loss: 2.5307\n",
      "Iteration 743700, Train Loss: 2.2627, Validation Loss: 2.5013\n",
      "Iteration 743800, Train Loss: 2.1519, Validation Loss: 2.3055\n",
      "Iteration 743900, Train Loss: 2.4756, Validation Loss: 2.4479\n",
      "Iteration 744000, Train Loss: 2.0975, Validation Loss: 2.3359\n",
      "Iteration 744100, Train Loss: 2.1499, Validation Loss: 2.2066\n",
      "Iteration 744200, Train Loss: 1.7606, Validation Loss: 2.3781\n",
      "Iteration 744300, Train Loss: 2.1218, Validation Loss: 2.6890\n",
      "Iteration 744400, Train Loss: 2.5484, Validation Loss: 2.1234\n",
      "Iteration 744500, Train Loss: 2.0766, Validation Loss: 2.6978\n",
      "Iteration 744600, Train Loss: 1.8422, Validation Loss: 1.8747\n",
      "Iteration 744700, Train Loss: 2.0468, Validation Loss: 2.2864\n",
      "Iteration 744800, Train Loss: 2.1595, Validation Loss: 2.0835\n",
      "Iteration 744900, Train Loss: 2.2344, Validation Loss: 2.4085\n",
      "Iteration 745000, Train Loss: 2.6081, Validation Loss: 2.0711\n",
      "Iteration 745100, Train Loss: 2.1446, Validation Loss: 2.6143\n",
      "Iteration 745200, Train Loss: 2.3585, Validation Loss: 2.4446\n",
      "Iteration 745300, Train Loss: 2.0916, Validation Loss: 2.6462\n",
      "Iteration 745400, Train Loss: 2.3494, Validation Loss: 2.1674\n",
      "Iteration 745500, Train Loss: 1.7270, Validation Loss: 1.9854\n",
      "Iteration 745600, Train Loss: 1.9993, Validation Loss: 2.1545\n",
      "Iteration 745700, Train Loss: 1.9311, Validation Loss: 2.1777\n",
      "Iteration 745800, Train Loss: 2.7269, Validation Loss: 1.9066\n",
      "Iteration 745900, Train Loss: 1.8634, Validation Loss: 2.3888\n",
      "Iteration 746000, Train Loss: 1.9110, Validation Loss: 1.9577\n",
      "Iteration 746100, Train Loss: 2.1922, Validation Loss: 2.1426\n",
      "Iteration 746200, Train Loss: 2.0744, Validation Loss: 2.1196\n",
      "Iteration 746300, Train Loss: 2.5885, Validation Loss: 2.0786\n",
      "Iteration 746400, Train Loss: 2.6411, Validation Loss: 2.3329\n",
      "Iteration 746500, Train Loss: 2.0787, Validation Loss: 2.1090\n",
      "Iteration 746600, Train Loss: 1.8371, Validation Loss: 2.1731\n",
      "Iteration 746700, Train Loss: 1.9540, Validation Loss: 2.0734\n",
      "Iteration 746800, Train Loss: 2.4879, Validation Loss: 2.1896\n",
      "Iteration 746900, Train Loss: 2.5984, Validation Loss: 2.3698\n",
      "Iteration 747000, Train Loss: 2.4518, Validation Loss: 2.1447\n",
      "Iteration 747100, Train Loss: 2.3960, Validation Loss: 2.1932\n",
      "Iteration 747200, Train Loss: 2.1226, Validation Loss: 2.5626\n",
      "Iteration 747300, Train Loss: 2.3069, Validation Loss: 2.6771\n",
      "Iteration 747400, Train Loss: 1.8699, Validation Loss: 2.1589\n",
      "Iteration 747500, Train Loss: 2.6071, Validation Loss: 2.2008\n",
      "Iteration 747600, Train Loss: 2.4216, Validation Loss: 2.2952\n",
      "Iteration 747700, Train Loss: 2.3484, Validation Loss: 2.5952\n",
      "Iteration 747800, Train Loss: 2.3017, Validation Loss: 1.9325\n",
      "Iteration 747900, Train Loss: 2.4276, Validation Loss: 2.3288\n",
      "Iteration 748000, Train Loss: 2.3523, Validation Loss: 1.9835\n",
      "Iteration 748100, Train Loss: 2.1702, Validation Loss: 2.2652\n",
      "Iteration 748200, Train Loss: 2.0688, Validation Loss: 1.9572\n",
      "Iteration 748300, Train Loss: 2.7643, Validation Loss: 2.4994\n",
      "Iteration 748400, Train Loss: 1.9982, Validation Loss: 2.3862\n",
      "Iteration 748500, Train Loss: 2.1089, Validation Loss: 2.3934\n",
      "Iteration 748600, Train Loss: 1.5350, Validation Loss: 1.8036\n",
      "Iteration 748700, Train Loss: 1.9522, Validation Loss: 2.0343\n",
      "Iteration 748800, Train Loss: 2.1295, Validation Loss: 2.1649\n",
      "Iteration 748900, Train Loss: 2.3672, Validation Loss: 2.6557\n",
      "Iteration 749000, Train Loss: 2.4034, Validation Loss: 2.0371\n",
      "Iteration 749100, Train Loss: 2.1211, Validation Loss: 2.1023\n",
      "Iteration 749200, Train Loss: 2.6917, Validation Loss: 2.2518\n",
      "Iteration 749300, Train Loss: 2.2820, Validation Loss: 2.5424\n",
      "Iteration 749400, Train Loss: 1.7120, Validation Loss: 2.4311\n",
      "Iteration 749500, Train Loss: 2.1883, Validation Loss: 2.1849\n",
      "Iteration 749600, Train Loss: 2.4490, Validation Loss: 1.9780\n",
      "Iteration 749700, Train Loss: 2.1160, Validation Loss: 2.3822\n",
      "Iteration 749800, Train Loss: 2.0622, Validation Loss: 2.4723\n",
      "Iteration 749900, Train Loss: 1.9407, Validation Loss: 2.9281\n",
      "Iteration 750000, Train Loss: 2.3777, Validation Loss: 2.2024\n",
      "Iteration 750100, Train Loss: 2.2412, Validation Loss: 2.1320\n",
      "Iteration 750200, Train Loss: 2.3970, Validation Loss: 2.0323\n",
      "Iteration 750300, Train Loss: 2.6001, Validation Loss: 2.4830\n",
      "Iteration 750400, Train Loss: 1.8919, Validation Loss: 2.0690\n",
      "Iteration 750500, Train Loss: 2.0890, Validation Loss: 2.0694\n",
      "Iteration 750600, Train Loss: 2.2436, Validation Loss: 2.0711\n",
      "Iteration 750700, Train Loss: 2.1109, Validation Loss: 2.3627\n",
      "Iteration 750800, Train Loss: 1.9215, Validation Loss: 2.1841\n",
      "Iteration 750900, Train Loss: 2.0300, Validation Loss: 2.4925\n",
      "Iteration 751000, Train Loss: 2.3545, Validation Loss: 2.1885\n",
      "Iteration 751100, Train Loss: 2.3744, Validation Loss: 2.0819\n",
      "Iteration 751200, Train Loss: 2.1256, Validation Loss: 2.4128\n",
      "Iteration 751300, Train Loss: 2.7193, Validation Loss: 2.2983\n",
      "Iteration 751400, Train Loss: 2.0446, Validation Loss: 2.0340\n",
      "Iteration 751500, Train Loss: 2.2730, Validation Loss: 2.1916\n",
      "Iteration 751600, Train Loss: 2.4458, Validation Loss: 2.0645\n",
      "Iteration 751700, Train Loss: 2.5738, Validation Loss: 2.2753\n",
      "Iteration 751800, Train Loss: 2.0008, Validation Loss: 2.3138\n",
      "Iteration 751900, Train Loss: 2.1979, Validation Loss: 2.6569\n",
      "Iteration 752000, Train Loss: 2.8111, Validation Loss: 2.1132\n",
      "Iteration 752100, Train Loss: 2.2699, Validation Loss: 1.9301\n",
      "Iteration 752200, Train Loss: 2.2846, Validation Loss: 2.1086\n",
      "Iteration 752300, Train Loss: 2.4455, Validation Loss: 2.1531\n",
      "Iteration 752400, Train Loss: 2.4377, Validation Loss: 2.6377\n",
      "Iteration 752500, Train Loss: 2.1736, Validation Loss: 1.8535\n",
      "Iteration 752600, Train Loss: 2.2740, Validation Loss: 2.4971\n",
      "Iteration 752700, Train Loss: 2.1367, Validation Loss: 2.3325\n",
      "Iteration 752800, Train Loss: 1.9144, Validation Loss: 2.5001\n",
      "Iteration 752900, Train Loss: 2.0301, Validation Loss: 2.4450\n",
      "Iteration 753000, Train Loss: 2.5295, Validation Loss: 2.2242\n",
      "Iteration 753100, Train Loss: 2.2054, Validation Loss: 2.2721\n",
      "Iteration 753200, Train Loss: 2.2031, Validation Loss: 2.5014\n",
      "Iteration 753300, Train Loss: 2.0537, Validation Loss: 2.2842\n",
      "Iteration 753400, Train Loss: 2.3743, Validation Loss: 1.9908\n",
      "Iteration 753500, Train Loss: 2.3436, Validation Loss: 2.1839\n",
      "Iteration 753600, Train Loss: 2.3265, Validation Loss: 2.7958\n",
      "Iteration 753700, Train Loss: 1.9727, Validation Loss: 1.8514\n",
      "Iteration 753800, Train Loss: 2.3953, Validation Loss: 2.2028\n",
      "Iteration 753900, Train Loss: 2.1258, Validation Loss: 2.3201\n",
      "Iteration 754000, Train Loss: 2.2794, Validation Loss: 2.4137\n",
      "Iteration 754100, Train Loss: 2.2843, Validation Loss: 2.5536\n",
      "Iteration 754200, Train Loss: 2.2584, Validation Loss: 2.4605\n",
      "Iteration 754300, Train Loss: 2.4605, Validation Loss: 2.6718\n",
      "Iteration 754400, Train Loss: 2.6835, Validation Loss: 2.1221\n",
      "Iteration 754500, Train Loss: 2.0253, Validation Loss: 2.1936\n",
      "Iteration 754600, Train Loss: 2.3748, Validation Loss: 2.4136\n",
      "Iteration 754700, Train Loss: 2.2881, Validation Loss: 2.1821\n",
      "Iteration 754800, Train Loss: 2.2722, Validation Loss: 1.8027\n",
      "Iteration 754900, Train Loss: 2.3619, Validation Loss: 2.3874\n",
      "Iteration 755000, Train Loss: 2.6864, Validation Loss: 2.3610\n",
      "Iteration 755100, Train Loss: 2.4270, Validation Loss: 3.0226\n",
      "Iteration 755200, Train Loss: 2.5326, Validation Loss: 1.9322\n",
      "Iteration 755300, Train Loss: 2.5708, Validation Loss: 2.7512\n",
      "Iteration 755400, Train Loss: 2.5408, Validation Loss: 2.2053\n",
      "Iteration 755500, Train Loss: 2.0213, Validation Loss: 2.5527\n",
      "Iteration 755600, Train Loss: 2.1999, Validation Loss: 2.1575\n",
      "Iteration 755700, Train Loss: 2.6083, Validation Loss: 2.6352\n",
      "Iteration 755800, Train Loss: 2.1407, Validation Loss: 2.4012\n",
      "Iteration 755900, Train Loss: 2.2758, Validation Loss: 2.0151\n",
      "Iteration 756000, Train Loss: 2.1080, Validation Loss: 2.0615\n",
      "Iteration 756100, Train Loss: 2.7816, Validation Loss: 2.3088\n",
      "Iteration 756200, Train Loss: 2.4097, Validation Loss: 1.9850\n",
      "Iteration 756300, Train Loss: 2.3075, Validation Loss: 2.3251\n",
      "Iteration 756400, Train Loss: 2.1522, Validation Loss: 2.6083\n",
      "Iteration 756500, Train Loss: 2.2672, Validation Loss: 2.1161\n",
      "Iteration 756600, Train Loss: 2.3492, Validation Loss: 2.3824\n",
      "Iteration 756700, Train Loss: 1.9029, Validation Loss: 2.2030\n",
      "Iteration 756800, Train Loss: 2.8044, Validation Loss: 2.5196\n",
      "Iteration 756900, Train Loss: 2.2220, Validation Loss: 2.1910\n",
      "Iteration 757000, Train Loss: 2.0557, Validation Loss: 2.1057\n",
      "Iteration 757100, Train Loss: 2.5083, Validation Loss: 2.5995\n",
      "Iteration 757200, Train Loss: 2.4770, Validation Loss: 2.1720\n",
      "Iteration 757300, Train Loss: 2.3618, Validation Loss: 2.2385\n",
      "Iteration 757400, Train Loss: 2.0789, Validation Loss: 2.0573\n",
      "Iteration 757500, Train Loss: 1.9004, Validation Loss: 2.4117\n",
      "Iteration 757600, Train Loss: 2.0885, Validation Loss: 2.3382\n",
      "Iteration 757700, Train Loss: 2.5855, Validation Loss: 2.8328\n",
      "Iteration 757800, Train Loss: 2.1274, Validation Loss: 2.6177\n",
      "Iteration 757900, Train Loss: 2.1946, Validation Loss: 2.4992\n",
      "Iteration 758000, Train Loss: 2.1251, Validation Loss: 2.0743\n",
      "Iteration 758100, Train Loss: 2.0351, Validation Loss: 2.1721\n",
      "Iteration 758200, Train Loss: 2.0596, Validation Loss: 1.9344\n",
      "Iteration 758300, Train Loss: 2.2490, Validation Loss: 1.8978\n",
      "Iteration 758400, Train Loss: 2.2042, Validation Loss: 1.9334\n",
      "Iteration 758500, Train Loss: 1.8313, Validation Loss: 2.9024\n",
      "Iteration 758600, Train Loss: 1.9200, Validation Loss: 2.0337\n",
      "Iteration 758700, Train Loss: 2.6961, Validation Loss: 2.7045\n",
      "Iteration 758800, Train Loss: 2.3079, Validation Loss: 2.5454\n",
      "Iteration 758900, Train Loss: 1.5762, Validation Loss: 2.1326\n",
      "Iteration 759000, Train Loss: 2.3873, Validation Loss: 2.3972\n",
      "Iteration 759100, Train Loss: 1.7729, Validation Loss: 2.7270\n",
      "Iteration 759200, Train Loss: 2.4844, Validation Loss: 2.4926\n",
      "Iteration 759300, Train Loss: 1.9699, Validation Loss: 2.2814\n",
      "Iteration 759400, Train Loss: 2.2771, Validation Loss: 2.4440\n",
      "Iteration 759500, Train Loss: 2.4489, Validation Loss: 2.2805\n",
      "Iteration 759600, Train Loss: 1.9617, Validation Loss: 2.4689\n",
      "Iteration 759700, Train Loss: 2.0747, Validation Loss: 2.1790\n",
      "Iteration 759800, Train Loss: 2.2216, Validation Loss: 2.2445\n",
      "Iteration 759900, Train Loss: 1.7656, Validation Loss: 2.0876\n",
      "Iteration 760000, Train Loss: 2.2490, Validation Loss: 2.3054\n",
      "Iteration 760100, Train Loss: 1.8837, Validation Loss: 2.3328\n",
      "Iteration 760200, Train Loss: 2.2323, Validation Loss: 2.2199\n",
      "Iteration 760300, Train Loss: 2.4562, Validation Loss: 2.0192\n",
      "Iteration 760400, Train Loss: 2.0387, Validation Loss: 2.3030\n",
      "Iteration 760500, Train Loss: 2.1423, Validation Loss: 2.6350\n",
      "Iteration 760600, Train Loss: 2.2019, Validation Loss: 2.7760\n",
      "Iteration 760700, Train Loss: 1.7981, Validation Loss: 1.9960\n",
      "Iteration 760800, Train Loss: 1.9287, Validation Loss: 2.2520\n",
      "Iteration 760900, Train Loss: 2.5033, Validation Loss: 2.5681\n",
      "Iteration 761000, Train Loss: 2.3090, Validation Loss: 2.4254\n",
      "Iteration 761100, Train Loss: 2.1743, Validation Loss: 2.2309\n",
      "Iteration 761200, Train Loss: 2.3802, Validation Loss: 2.3821\n",
      "Iteration 761300, Train Loss: 2.1499, Validation Loss: 2.5971\n",
      "Iteration 761400, Train Loss: 2.2432, Validation Loss: 2.7125\n",
      "Iteration 761500, Train Loss: 2.0784, Validation Loss: 2.5686\n",
      "Iteration 761600, Train Loss: 2.3524, Validation Loss: 2.5118\n",
      "Iteration 761700, Train Loss: 2.2321, Validation Loss: 2.2599\n",
      "Iteration 761800, Train Loss: 1.9862, Validation Loss: 1.9945\n",
      "Iteration 761900, Train Loss: 2.3304, Validation Loss: 2.0190\n",
      "Iteration 762000, Train Loss: 2.4716, Validation Loss: 2.2821\n",
      "Iteration 762100, Train Loss: 2.5252, Validation Loss: 2.4300\n",
      "Iteration 762200, Train Loss: 2.1155, Validation Loss: 2.2868\n",
      "Iteration 762300, Train Loss: 2.2783, Validation Loss: 2.4337\n",
      "Iteration 762400, Train Loss: 2.3878, Validation Loss: 2.5269\n",
      "Iteration 762500, Train Loss: 2.4051, Validation Loss: 2.4091\n",
      "Iteration 762600, Train Loss: 2.0872, Validation Loss: 2.2495\n",
      "Iteration 762700, Train Loss: 2.8468, Validation Loss: 2.3901\n",
      "Iteration 762800, Train Loss: 2.0726, Validation Loss: 2.3474\n",
      "Iteration 762900, Train Loss: 2.7632, Validation Loss: 2.3895\n",
      "Iteration 763000, Train Loss: 1.9943, Validation Loss: 2.6407\n",
      "Iteration 763100, Train Loss: 2.0448, Validation Loss: 2.0336\n",
      "Iteration 763200, Train Loss: 2.5431, Validation Loss: 2.3298\n",
      "Iteration 763300, Train Loss: 2.3657, Validation Loss: 2.3963\n",
      "Iteration 763400, Train Loss: 1.9713, Validation Loss: 2.1356\n",
      "Iteration 763500, Train Loss: 2.4805, Validation Loss: 2.3247\n",
      "Iteration 763600, Train Loss: 2.2358, Validation Loss: 2.6842\n",
      "Iteration 763700, Train Loss: 2.0761, Validation Loss: 2.5349\n",
      "Iteration 763800, Train Loss: 2.1556, Validation Loss: 2.3260\n",
      "Iteration 763900, Train Loss: 2.1122, Validation Loss: 2.2253\n",
      "Iteration 764000, Train Loss: 2.1775, Validation Loss: 2.4924\n",
      "Iteration 764100, Train Loss: 2.1499, Validation Loss: 2.6723\n",
      "Iteration 764200, Train Loss: 2.3046, Validation Loss: 2.5497\n",
      "Iteration 764300, Train Loss: 2.3781, Validation Loss: 2.3649\n",
      "Iteration 764400, Train Loss: 2.0227, Validation Loss: 2.2646\n",
      "Iteration 764500, Train Loss: 2.2407, Validation Loss: 2.5081\n",
      "Iteration 764600, Train Loss: 1.9867, Validation Loss: 2.5072\n",
      "Iteration 764700, Train Loss: 2.4078, Validation Loss: 2.3236\n",
      "Iteration 764800, Train Loss: 1.8946, Validation Loss: 2.3515\n",
      "Iteration 764900, Train Loss: 2.5908, Validation Loss: 2.2213\n",
      "Iteration 765000, Train Loss: 2.1148, Validation Loss: 2.5209\n",
      "Iteration 765100, Train Loss: 2.0576, Validation Loss: 1.9491\n",
      "Iteration 765200, Train Loss: 2.2755, Validation Loss: 2.0304\n",
      "Iteration 765300, Train Loss: 1.9389, Validation Loss: 2.3878\n",
      "Iteration 765400, Train Loss: 2.1373, Validation Loss: 1.8932\n",
      "Iteration 765500, Train Loss: 2.3241, Validation Loss: 2.3144\n",
      "Iteration 765600, Train Loss: 2.2015, Validation Loss: 2.5811\n",
      "Iteration 765700, Train Loss: 2.5095, Validation Loss: 2.2217\n",
      "Iteration 765800, Train Loss: 2.4722, Validation Loss: 2.3053\n",
      "Iteration 765900, Train Loss: 2.2103, Validation Loss: 1.9784\n",
      "Iteration 766000, Train Loss: 1.8686, Validation Loss: 2.1257\n",
      "Iteration 766100, Train Loss: 2.0664, Validation Loss: 2.0634\n",
      "Iteration 766200, Train Loss: 2.2402, Validation Loss: 2.4054\n",
      "Iteration 766300, Train Loss: 2.4576, Validation Loss: 3.3434\n",
      "Iteration 766400, Train Loss: 2.2253, Validation Loss: 2.1085\n",
      "Iteration 766500, Train Loss: 2.7589, Validation Loss: 2.2182\n",
      "Iteration 766600, Train Loss: 2.3690, Validation Loss: 2.4032\n",
      "Iteration 766700, Train Loss: 2.2022, Validation Loss: 2.4544\n",
      "Iteration 766800, Train Loss: 2.8213, Validation Loss: 2.4954\n",
      "Iteration 766900, Train Loss: 1.9093, Validation Loss: 2.1398\n",
      "Iteration 767000, Train Loss: 2.0894, Validation Loss: 2.0666\n",
      "Iteration 767100, Train Loss: 1.8967, Validation Loss: 2.1000\n",
      "Iteration 767200, Train Loss: 1.9718, Validation Loss: 2.3280\n",
      "Iteration 767300, Train Loss: 2.3648, Validation Loss: 2.2831\n",
      "Iteration 767400, Train Loss: 1.8662, Validation Loss: 2.0265\n",
      "Iteration 767500, Train Loss: 2.0863, Validation Loss: 2.3543\n",
      "Iteration 767600, Train Loss: 1.9056, Validation Loss: 2.2758\n",
      "Iteration 767700, Train Loss: 1.9176, Validation Loss: 2.3196\n",
      "Iteration 767800, Train Loss: 2.1381, Validation Loss: 2.0386\n",
      "Iteration 767900, Train Loss: 2.5174, Validation Loss: 2.6683\n",
      "Iteration 768000, Train Loss: 2.0412, Validation Loss: 2.3100\n",
      "Iteration 768100, Train Loss: 2.0202, Validation Loss: 2.4080\n",
      "Iteration 768200, Train Loss: 2.4477, Validation Loss: 1.9105\n",
      "Iteration 768300, Train Loss: 2.2009, Validation Loss: 2.1165\n",
      "Iteration 768400, Train Loss: 2.2311, Validation Loss: 1.8304\n",
      "Iteration 768500, Train Loss: 2.2120, Validation Loss: 1.9788\n",
      "Iteration 768600, Train Loss: 2.1635, Validation Loss: 2.7953\n",
      "Iteration 768700, Train Loss: 2.2100, Validation Loss: 2.1161\n",
      "Iteration 768800, Train Loss: 1.9940, Validation Loss: 2.2376\n",
      "Iteration 768900, Train Loss: 2.3006, Validation Loss: 2.1705\n",
      "Iteration 769000, Train Loss: 2.4764, Validation Loss: 2.4873\n",
      "Iteration 769100, Train Loss: 1.6747, Validation Loss: 1.9727\n",
      "Iteration 769200, Train Loss: 2.2280, Validation Loss: 1.9378\n",
      "Iteration 769300, Train Loss: 1.9361, Validation Loss: 2.2900\n",
      "Iteration 769400, Train Loss: 2.2198, Validation Loss: 2.0794\n",
      "Iteration 769500, Train Loss: 2.3587, Validation Loss: 2.4142\n",
      "Iteration 769600, Train Loss: 2.2232, Validation Loss: 1.9339\n",
      "Iteration 769700, Train Loss: 2.0321, Validation Loss: 2.5295\n",
      "Iteration 769800, Train Loss: 2.0807, Validation Loss: 2.7344\n",
      "Iteration 769900, Train Loss: 2.2674, Validation Loss: 2.4438\n",
      "Iteration 770000, Train Loss: 2.0465, Validation Loss: 1.9632\n",
      "Iteration 770100, Train Loss: 2.3629, Validation Loss: 2.2873\n",
      "Iteration 770200, Train Loss: 2.6700, Validation Loss: 2.2438\n",
      "Iteration 770300, Train Loss: 2.2879, Validation Loss: 2.6373\n",
      "Iteration 770400, Train Loss: 2.2427, Validation Loss: 2.2201\n",
      "Iteration 770500, Train Loss: 2.3437, Validation Loss: 2.2829\n",
      "Iteration 770600, Train Loss: 2.1582, Validation Loss: 2.1781\n",
      "Iteration 770700, Train Loss: 1.9176, Validation Loss: 2.1939\n",
      "Iteration 770800, Train Loss: 2.0284, Validation Loss: 2.3667\n",
      "Iteration 770900, Train Loss: 1.9936, Validation Loss: 2.0009\n",
      "Iteration 771000, Train Loss: 2.2261, Validation Loss: 2.3228\n",
      "Iteration 771100, Train Loss: 2.0902, Validation Loss: 2.3540\n",
      "Iteration 771200, Train Loss: 2.1422, Validation Loss: 2.9204\n",
      "Iteration 771300, Train Loss: 1.9703, Validation Loss: 2.8660\n",
      "Iteration 771400, Train Loss: 2.1543, Validation Loss: 2.1955\n",
      "Iteration 771500, Train Loss: 1.8504, Validation Loss: 2.5812\n",
      "Iteration 771600, Train Loss: 2.8453, Validation Loss: 2.3741\n",
      "Iteration 771700, Train Loss: 2.1147, Validation Loss: 2.5028\n",
      "Iteration 771800, Train Loss: 2.3560, Validation Loss: 2.6093\n",
      "Iteration 771900, Train Loss: 2.3957, Validation Loss: 2.2958\n",
      "Iteration 772000, Train Loss: 2.3082, Validation Loss: 2.2826\n",
      "Iteration 772100, Train Loss: 2.5499, Validation Loss: 2.3301\n",
      "Iteration 772200, Train Loss: 2.0988, Validation Loss: 2.4088\n",
      "Iteration 772300, Train Loss: 2.4653, Validation Loss: 2.2321\n",
      "Iteration 772400, Train Loss: 2.2495, Validation Loss: 2.1395\n",
      "Iteration 772500, Train Loss: 2.2602, Validation Loss: 2.1317\n",
      "Iteration 772600, Train Loss: 2.2394, Validation Loss: 2.7129\n",
      "Iteration 772700, Train Loss: 2.2641, Validation Loss: 2.7415\n",
      "Iteration 772800, Train Loss: 2.3256, Validation Loss: 1.8243\n",
      "Iteration 772900, Train Loss: 2.6132, Validation Loss: 2.1945\n",
      "Iteration 773000, Train Loss: 2.1100, Validation Loss: 2.4611\n",
      "Iteration 773100, Train Loss: 2.3988, Validation Loss: 2.0676\n",
      "Iteration 773200, Train Loss: 1.8879, Validation Loss: 2.2280\n",
      "Iteration 773300, Train Loss: 2.2533, Validation Loss: 2.4506\n",
      "Iteration 773400, Train Loss: 1.9127, Validation Loss: 2.0592\n",
      "Iteration 773500, Train Loss: 2.0536, Validation Loss: 2.4012\n",
      "Iteration 773600, Train Loss: 2.1310, Validation Loss: 1.8895\n",
      "Iteration 773700, Train Loss: 2.0211, Validation Loss: 2.2661\n",
      "Iteration 773800, Train Loss: 2.2738, Validation Loss: 2.4794\n",
      "Iteration 773900, Train Loss: 2.1524, Validation Loss: 2.2323\n",
      "Iteration 774000, Train Loss: 2.7449, Validation Loss: 1.7381\n",
      "Iteration 774100, Train Loss: 1.9216, Validation Loss: 2.0781\n",
      "Iteration 774200, Train Loss: 2.5781, Validation Loss: 2.6751\n",
      "Iteration 774300, Train Loss: 2.1478, Validation Loss: 1.9881\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 82\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Initialize hyperparameters\n",
    "n_embd = 32\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "vocab_size = len(vocab)\n",
    "dropout = 0.3\n",
    "learning_rate = 0.00003\n",
    "num_iterations = 1000000\n",
    "log_interval = 100\n",
    "\n",
    "# Define the model components\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "ffn_c1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "ffn_c2 = nn.Linear(4 * n_embd, n_embd)\n",
    "ln1 = nn.LayerNorm(n_embd)\n",
    "ln2 = nn.LayerNorm(n_embd)\n",
    "drop = nn.Dropout(dropout)\n",
    "lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "# Combine components into a forward pass\n",
    "def model_forward(x):\n",
    "    x_emb = token_embedding_table(x)\n",
    "\n",
    "    # Self-attention\n",
    "    k = key(x_emb)\n",
    "    q = query(x_emb)\n",
    "    v = value(x_emb)\n",
    "\n",
    "    wei = q @ k.transpose(-2, -1) * (n_embd**-0.5)\n",
    "    tril = torch.tril(torch.ones(block_size, block_size, device=x.device))\n",
    "    wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "    out = wei @ v\n",
    "    x = x_emb + out\n",
    "    x = ln1(x)\n",
    "\n",
    "    # Feed-forward network\n",
    "    x = x + drop(ffn_c2(F.relu(ffn_c1(x))))\n",
    "    x = ln2(x)\n",
    "\n",
    "    # Final projection to vocab size\n",
    "    logits = lm_head(x)\n",
    "    return logits\n",
    "\n",
    "# Define optimizer and loss criterion\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(token_embedding_table.parameters()) +\n",
    "    list(key.parameters()) +\n",
    "    list(query.parameters()) +\n",
    "    list(value.parameters()) +\n",
    "    list(ffn_c1.parameters()) +\n",
    "    list(ffn_c2.parameters()) +\n",
    "    list(ln1.parameters()) +\n",
    "    list(ln2.parameters()) +\n",
    "    list(lm_head.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for iteration in range(num_iterations):\n",
    "    # Get a batch of training data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model_forward(xb)\n",
    "    # Reshape logits and targets for loss computation\n",
    "    B, T, C = logits.shape\n",
    "    loss = criterion(logits.view(B * T, C), yb.view(B * T))\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    if iteration % log_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            # Evaluate on validation set\n",
    "            val_xb, val_yb = get_batch('val')\n",
    "            val_logits = model_forward(val_xb)\n",
    "            val_loss = criterion(val_logits.view(B * T, C), val_yb.view(B * T))\n",
    "        \n",
    "        print(f\"Iteration {iteration}, Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Get a batch of training data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model_forward(xb)\n",
    "    # Reshape logits and targets for loss computation\n",
    "    B, T, C = logits.shape\n",
    "    loss = criterion(logits.view(B * T, C), yb.view(B * T))\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    if iteration % log_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            # Evaluate on validation set\n",
    "            val_xb, val_yb = get_batch('val')\n",
    "            val_logits = model_forward(val_xb)\n",
    "            val_loss = criterion(val_logits.view(B * T, C), val_yb.view(B * T))\n",
    "        \n",
    "        print(f\"Iteration {iteration}, Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
